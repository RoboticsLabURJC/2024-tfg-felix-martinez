---
title: "Investigación de estrategias de submuestreo, Primera versión de PoinNet-Tnet funcional"
categories:
  - Weblog
tags:
  - Normalización
  - PointNet
  - Segmentación
  - Tensorflow
  - Keras
  - Python
  - Submuestreo
---

Esta semana he investigado diversas técnicas utilizadas para inferir información a partir de la nube de puntos en los modelos implementados en los repositorios Open3D-ML y el repositorio oficial de PointNet de la Universidad de Stanford. También he realizado mejoras en el pipeline de PointNet, terminando la primera versión funcional de la arquitectura con la TNet de entrada y la TNet de alineación de características añadidas.

## Técnicas de Submuestreo

Los modelos que están implementados en Open3D-ML como KPConv y RandLA-Net, utilizan técnicas de submuestreo por zonas, consiguiendo no redimensionar la nube de puntos ni a la salida en proceso de entrenamiento, ni en nuevas inferencias a este.

En [PointNet](https://github.com/charlesq34/pointnet), la estrategia es cortar las nubes de puntos en un número de puntos predefinido en los parámetros de lanzamiento del modelo. En la siguiente sección de código perteneciente al archivo `pointnet/sem_seg/train.py` del repositorio, se muestra como corta las nubes de puntos previamente cargadas de archivos `.h5` y realiza un _shuffle_ de los índices de las nubes de puntos para que el proceso de entrenamiento sea invariante al orden original de estas.

```python
def train_one_epoch(sess, ops, train_writer):
    """ ops: dict mapping from string to tf ops """
    is_training = True
    
    log_string('----')
    current_data, current_label, _ = provider.shuffle_data(train_data[:,0:NUM_POINT,:], train_label) 
    
    file_size = current_data.shape[0]
    num_batches = file_size // BATCH_SIZE

                    . . .
```

Al tratarse de un modelo más sencillo, maneja la información redimensionando la nube de puntos a la salida.

### Cambios de Submuestreo Propuestos

Dada la naturaleza de las nubes LiDAR que se manejará en el entrenamiento de este modelo (dimenisones variables, densidades variables, configuraciones variables según dataset), se propone redimensionar las nubes de puntos en el pipeline a un tamaño fijo, haciendo una permutación aleatoria de los índices de los puntos seleccionando los '_n_' primeros. A ser preciso un valor de 16.384 puntos para simplificar las tareas en GPU. Todos los dataset tienen una resulución mínima superior a este umbral. En inferencia de nubes de otras fuentes, se podria densificar para cumplir con el _shape_ de entrada.

Para recuperar la resolución original de la imagen he pensado en ejecutar el algoritmo __KNN__ para la asignación semantica de los puntos restantes sin segmentar. El algoritmo __KNN__ podría introducir ruido en las fornteras entre objetos, pero sería factible. Existen otros algoritmos más complejos que se podrían investigar para la recuperación de resolución.





