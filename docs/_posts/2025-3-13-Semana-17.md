---
title: "DataLoader de subnubes aleatorias en entrenamiento, Separacción sistemática en subnubes de todo el Dataset, Entrenamiento de PointNet"
categories:
  - Weblog
tags:
  - PointNet
  - Segmentación
  - Tensorflow
  - Keras
  - Python
  - Entrenamiento
  - GOOSE
---

Esta semana me he dedicado a intentar mejorar el rendimiento del entrenamiento aplicando dos enfoques distintos. El primero, desarrollar un DataLoader en Tensorflow Keras para cargar dinámicamente la información de interés, el cual empeoró el rendimiento del entrenamiento. También se realizó un pipeline para dividir todas las nubes originales en subnubes de tamaño fijo sin sacrificar prácticamente la densidad original de la nube. Este proceso es sistemático. El resultado del entrenamiento con esta división sistemática de las nubes originales no mejoró tampoco el entrenamiento realizado con el submuestreo aleatorio de las nubes.

## DataLoader (Subnubes aleatorias)

Para cada nube original (previamente recortada radialmente al espacio cercano al sensor, 25m) se selecciona un punto aleatorio (centroide) y se calculan mediante KDTree los _N-1_ puntos más cercanos. Se carga esta subnube extraida al batch de datos. Si el batch se constituye de 16 subnubes, se realiza este proceso en 16 nubes originales del conjunto de entrenamiento. Cada batch se eligen las 16 siguientes nubes originales.

El DataLoader diferencia si es para entrenamiento o validación. En validación, divide cada nube en subnubes del mismo tamaño pero sin sacrificar información.

```python
class PointCloudGenerator(Sequence):
    def __init__(self, bin_files, label_files, num_points, mode='train', batch_size=32):
        self.bin_files = bin_files
        self.label_files = label_files
        self.num_points = num_points
        self.batch_size = batch_size
        self.mode = mode  # 'train' o 'val'

    def __len__(self):
        """Número total de batches por epoch"""
        return len(self.bin_files) // self.batch_size

    def __getitem__(self, idx):
        """Genera un batch de datos dinámicamente"""
        batch_bin_files = self.bin_files[idx * self.batch_size : (idx + 1) * self.batch_size]
        batch_label_files = self.label_files[idx * self.batch_size : (idx + 1) * self.batch_size]

        x_batch, y_batch = [], []

        for bin_file, label_file in zip(batch_bin_files, batch_label_files):
            points, indices = load_bin_file(bin_file)
            labels = load_label_file(label_file, indices)

            # Filtrar los puntos dentro del rango de (-25, 25)
            mask = (points[:, 0] >= -25) & (points[:, 0] <= 25) & \
                   (points[:, 1] >= -25) & (points[:, 1] <= 25) & \
                   (points[:, 2] >= -25) & (points[:, 2] <= 25)
            points, labels = points[mask], labels[mask]

            if len(points) < self.num_points:
                continue  

            if self.mode == 'train':
                # Construir el KDTree dinámicamente solo para esta nube
                tree = cKDTree(points)
                sub_points, sub_labels = self._get_train_sample(points, labels, tree)
                x_batch.append(sub_points)
                y_batch.append(sub_labels)
            else:
                sub_points_list, sub_labels_list = self._get_val_samples(points, labels)
                x_batch.extend(sub_points_list)
                y_batch.extend(sub_labels_list)

        return np.array(x_batch, dtype=np.float32), np.array(y_batch, dtype=np.uint8)

    def _get_train_sample(self, points, labels, tree):
        """Obtiene una subnube de entrenamiento con N vecinos más cercanos"""
        center_idx = np.random.randint(len(points))  # Selecciona un punto aleatorio
        _, neighbor_indices = tree.query(points[center_idx], k=self.num_points)  # Obtiene los N vecinos más cercanos
        
        return points[neighbor_indices], labels[neighbor_indices]

    def _get_val_samples(self, points, labels):
        """Divide la nube preprocesada en subnubes de N puntos para validación"""
        num_full_batches = len(points) // self.num_points
        indices = np.random.choice(len(points), num_full_batches * self.num_points, replace=False)
        subclouds = points[indices].reshape(num_full_batches, self.num_points, 3)
        sublabels = labels[indices].reshape(num_full_batches, self.num_points)

        return [subclouds[i] for i in range(num_full_batches)], [sublabels[i] for i in range(num_full_batches)]
```

#### Ventana Aleatoria (Centroide aleatorio)

<figure class="align-center" style="max-width: 100%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/window_knn.png" alt="Visor 3D">
</figure>


