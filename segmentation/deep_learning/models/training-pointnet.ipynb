{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:37.429123Z",
     "iopub.status.busy": "2025-02-19T22:45:37.428898Z",
     "iopub.status.idle": "2025-02-19T22:45:42.190720Z",
     "shell.execute_reply": "2025-02-19T22:45:42.189734Z",
     "shell.execute_reply.started": "2025-02-19T22:45:37.429101Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"VersiÃ³n de cuDNN:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])\n",
    "print(\"VersiÃ³n de CUDA:\", tf.sysconfig.get_build_info()[\"cuda_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"VersiÃ³n de TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOSE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:42.192969Z",
     "iopub.status.busy": "2025-02-19T22:45:42.192274Z",
     "iopub.status.idle": "2025-02-19T22:45:42.206732Z",
     "shell.execute_reply": "2025-02-19T22:45:42.205795Z",
     "shell.execute_reply.started": "2025-02-19T22:45:42.192927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_POINTS = 16384 # 256, 512, 1024, 2048, 4096, 8192, 16384, 32768\n",
    "\n",
    "def filter_points_within_radius(points: np.ndarray, radius: float = 25.0) -> np.ndarray:\n",
    "    distances = np.linalg.norm(points, axis=1)  # Calcula la distancia euclidiana para cada punto\n",
    "    mask = distances <= radius  # Crea una mÃ¡scara booleana para los puntos dentro del radio\n",
    "    return points[mask]  # Devuelve solo los puntos dentro del radio\n",
    "\n",
    "\n",
    "# GOOSE Categories mejoradas\n",
    "category_mapping = {\n",
    "    0: [43, 38, 58, 29, 41, 42, 44, 39, 55], # Construction\n",
    "    1: [4, 45, 6, 40, 60, 61, 33, 32, 14], # Object\n",
    "    2: [7, 22, 9, 26, 11, 21], # Road\n",
    "    3: [48, 47, 1, 19, 46, 10, 25], # Sign\n",
    "    4: [23, 3, 24, 31, 2], # Terrain  \n",
    "    5: [51, 50, 5, 18], # Drivable Vegetation\n",
    "    6: [28, 27, 62, 52, 16, 30, 59, 17], # Non Drivable Vegetation\n",
    "    7: [13, 15, 12, 36, 57, 49, 20, 35, 37, 34, 63], # Vehicle\n",
    "    8: [8, 56, 0, 53, 54], # Void\n",
    "}\n",
    "\n",
    "def load_bin_file(bin_path: str, num_points: int = NUM_POINTS, radius: float = 25.0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    points = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)[:, :3]  # Cargar coordenadas (x, y, z)\n",
    "    \n",
    "    # ðŸ”¹ Filtrar solo los puntos dentro del radio de 25m\n",
    "    distances = np.linalg.norm(points, axis=1)\n",
    "    mask = distances <= radius\n",
    "    points = points[mask]  # Solo mantener los puntos dentro del radio\n",
    "\n",
    "    num_available = points.shape[0]  # Puntos despuÃ©s del filtrado\n",
    "\n",
    "    if num_available >= num_points:\n",
    "        indices = np.random.choice(num_available, num_points, replace=False)  # ðŸ”¹ SelecciÃ³n aleatoria\n",
    "        return points[indices], np.where(mask)[0][indices]  # Devuelve los Ã­ndices relativos al archivo original\n",
    "    else:\n",
    "        return points, np.where(mask)[0]  # Devuelve los Ã­ndices relativos al archivo original\n",
    "\n",
    "# Reverse mapping for fast lookup\n",
    "label_to_category = {label: cat for cat, labels in category_mapping.items() for label in labels}\n",
    "\n",
    "def map_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mapea etiquetas al sistema de categorÃ­as definido en category_mapping.\n",
    "    \"\"\"\n",
    "    return np.array([label_to_category.get(label, 8) for label in labels], dtype=np.uint8)\n",
    "\n",
    "def load_label_file(label_path: str, indices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Carga las etiquetas y extrae solo las que corresponden a los Ã­ndices seleccionados.\n",
    "\n",
    "    ParÃ¡metros:\n",
    "    - label_path (str): Ruta del archivo de etiquetas.\n",
    "    - indices (np.ndarray): Ãndices usados para seleccionar los puntos.\n",
    "\n",
    "    Retorna:\n",
    "    - np.ndarray: Etiquetas correspondientes a los puntos seleccionados.\n",
    "    \"\"\"\n",
    "    labels = np.fromfile(label_path, dtype=np.uint32) & 0xFFFF  # Cargar etiquetas completas\n",
    "    return map_labels(labels[indices])  # ðŸ”¹ Extraer solo las etiquetas correspondientes a los Ã­ndices seleccionados\n",
    "\n",
    "def load_dataset(bin_files: List[str], label_files: List[str], num_points: int = NUM_POINTS) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga las nubes de puntos y etiquetas, asegurando que cada muestra tenga exactamente num_points puntos.\n",
    "    Si una nube tiene menos de num_points despuÃ©s del filtrado, se descarta.\n",
    "\n",
    "    ParÃ¡metros:\n",
    "    - bin_files: Lista de rutas a archivos de nubes de puntos.\n",
    "    - label_files: Lista de rutas a archivos de etiquetas.\n",
    "    - num_points: NÃºmero fijo de puntos en cada nube.\n",
    "\n",
    "    Retorna:\n",
    "    - x_data: np.ndarray de forma (N, num_points, 3).\n",
    "    - y_data: np.ndarray de forma (N, num_points).\n",
    "    \"\"\"\n",
    "    x_data, y_data = [], []\n",
    "    \n",
    "    for bin_f, label_f in tqdm(zip(bin_files, label_files), total=len(bin_files), desc=\"Cargando datos\"):\n",
    "        points, indices = load_bin_file(bin_f, num_points)\n",
    "        \n",
    "        # ðŸ”¹ Solo aceptar nubes con suficientes puntos\n",
    "        if points.shape[0] < num_points:\n",
    "            continue  # ðŸ”¹ Se descarta esta muestra\n",
    "\n",
    "        labels = load_label_file(label_f, indices)\n",
    "\n",
    "        x_data.append(points)\n",
    "        y_data.append(labels)\n",
    "\n",
    "    return np.array(x_data, dtype=np.float32), np.array(y_data, dtype=np.uint8)\n",
    "\n",
    "def get_file_paths(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Obtiene lista de archivos en un directorio.\n",
    "    \"\"\"\n",
    "    return sorted([str(f) for f in Path(data_dir).glob(\"*.*\")])\n",
    "\n",
    "def load_all_data(x_train_dir: str, y_train_dir: str, x_val_dir: str, y_val_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga todos los datos de entrenamiento y validaciÃ³n con barra de progreso en una sola lÃ­nea por conjunto de datos.\n",
    "    \"\"\"\n",
    "    x_train_files = get_file_paths(x_train_dir)\n",
    "    y_train_files = get_file_paths(y_train_dir)\n",
    "    x_val_files = get_file_paths(x_val_dir)\n",
    "    y_val_files = get_file_paths(y_val_dir)\n",
    "    \n",
    "    assert len(x_train_files) == len(y_train_files), \"NÃºmero de archivos x_train y y_train no coincide.\"\n",
    "    assert len(x_val_files) == len(y_val_files), \"NÃºmero de archivos x_val y y_val no coincide.\"\n",
    "    \n",
    "    print(\"Cargando datos de entrenamiento...\")\n",
    "    x_train, y_train = load_dataset(x_train_files, y_train_files)\n",
    "    \n",
    "    print(\"Cargando datos de validaciÃ³n...\")\n",
    "    x_val, y_val = load_dataset(x_val_files, y_val_files)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:48:25.468327Z",
     "iopub.status.busy": "2025-02-19T22:48:25.468011Z",
     "iopub.status.idle": "2025-02-19T22:57:03.863818Z",
     "shell.execute_reply": "2025-02-19T22:57:03.862746Z",
     "shell.execute_reply.started": "2025-02-19T22:48:25.468304Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_path = \"/home/fmartinez/datasets/lidar/train\"\n",
    "y_train_path = \"/home/fmartinez/datasets/labels/train\"\n",
    "x_val_path = \"/home/fmartinez/datasets_val/lidar/val\"\n",
    "y_val_path = \"/home/fmartinez/datasets_val/labels/val\"\n",
    "\n",
    "x_train, y_train, x_val, y_val = load_all_data(x_train_path, y_train_path, x_val_path, y_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:02:41.161169Z",
     "iopub.status.busy": "2025-02-19T23:02:41.160826Z",
     "iopub.status.idle": "2025-02-19T23:02:43.101788Z",
     "shell.execute_reply": "2025-02-19T23:02:43.101090Z",
     "shell.execute_reply.started": "2025-02-19T23:02:41.161144Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_permutados_train = np.random.permutation(x_train.shape[0])\n",
    "indices_permutados_val = np.random.permutation(x_val.shape[0])\n",
    "\n",
    "x_train_shuffle = x_train[indices_permutados_train]\n",
    "y_train_shuffle = y_train[indices_permutados_train]\n",
    "x_val_shuffle = x_val[indices_permutados_val]\n",
    "y_val_shuffle = y_val[indices_permutados_val]\n",
    "\n",
    "x_train_shuffle.shape, y_train_shuffle.shape, x_val_shuffle.shape, y_val_shuffle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train) and len(x_val) == len(y_val)\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_train)} nubes de puntos de {y_train.shape[0]} puntos\")\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_val)} nubes de puntos de {y_val.shape[0]} puntos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:07.718183Z",
     "iopub.status.busy": "2025-02-19T23:04:07.717827Z",
     "iopub.status.idle": "2025-02-19T23:04:07.724041Z",
     "shell.execute_reply": "2025-02-19T23:04:07.723077Z",
     "shell.execute_reply.started": "2025-02-19T23:04:07.718155Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_3D(xyz, labels):\n",
    "    \"\"\"\n",
    "    Visualiza una nube de puntos en 3D con Plotly.\n",
    "    - xyz: (num_points, 3) array con coordenadas (X, Y, Z).\n",
    "    - labels: (num_points,) array con etiquetas semÃ¡nticas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Definir 9 colores predefinidos en formato RGB\n",
    "    predefined_colors = [\n",
    "        \"rgb(255, 0, 0)\",    # Rojo\n",
    "        \"rgb(0, 255, 0)\",    # Verde\n",
    "        \"rgb(0, 0, 255)\",    # Azul\n",
    "        \"rgb(255, 255, 0)\",  # Amarillo\n",
    "        \"rgb(255, 165, 0)\",  # Naranja\n",
    "        \"rgb(128, 0, 128)\",  # PÃºrpura\n",
    "        \"rgb(0, 255, 255)\",  # Cian\n",
    "        \"rgb(255, 192, 203)\",# Rosa\n",
    "        \"rgb(128, 128, 128)\" # Gris\n",
    "    ]\n",
    "\n",
    "    # Asignar colores segÃºn la etiqueta (se asume que las etiquetas van de 0 a 8)\n",
    "    point_colors = [predefined_colors[label % len(predefined_colors)] for label in labels]\n",
    "\n",
    "    # Crear la figura 3D en Plotly\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=xyz[:, 0], y=xyz[:, 1], z=xyz[:, 2],  # Coordenadas X, Y, Z\n",
    "        mode='markers',\n",
    "        marker=dict(size=1, color=point_colors, opacity=0.8)  # Color basado en etiquetas\n",
    "    ))\n",
    "\n",
    "    # Configurar etiquetas y tÃ­tulo\n",
    "    fig.update_layout(\n",
    "        title=\"Nube de Puntos con Etiquetas SemÃ¡nticas\",\n",
    "        scene=dict(xaxis_title=\"X\", yaxis_title=\"Y\", zaxis_title=\"Z\")\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:24.140632Z",
     "iopub.status.busy": "2025-02-19T23:04:24.140280Z",
     "iopub.status.idle": "2025-02-19T23:04:25.355377Z",
     "shell.execute_reply": "2025-02-19T23:04:25.353411Z",
     "shell.execute_reply.started": "2025-02-19T23:04:24.140600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Llamar a la funciÃ³n con la primera nube de puntos\n",
    "plot_3D(x_train_shuffle[1000], y_train_shuffle[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:08.622104Z",
     "iopub.status.busy": "2025-02-19T23:05:08.621750Z",
     "iopub.status.idle": "2025-02-19T23:05:11.936407Z",
     "shell.execute_reply": "2025-02-19T23:05:11.935406Z",
     "shell.execute_reply.started": "2025-02-19T23:05:08.622075Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Contar cuÃ¡ntos puntos hay por clase\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Mostrar distribuciÃ³n de clases\n",
    "print(len(y_train))\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Clase {cls}: {count} puntos\")\n",
    "\n",
    "# Crear grÃ¡fico interactivo con Plotly\n",
    "fig = px.bar(x=unique_classes, y=class_counts, labels={'x': 'Clase', 'y': 'Cantidad de puntos'},\n",
    "             title='DistribuciÃ³n de etiquetas en y_train')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:16.216947Z",
     "iopub.status.busy": "2025-02-19T23:05:16.216572Z",
     "iopub.status.idle": "2025-02-19T23:05:16.246341Z",
     "shell.execute_reply": "2025-02-19T23:05:16.245375Z",
     "shell.execute_reply.started": "2025-02-19T23:05:16.216914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular pesos inversamente proporcionales a la frecuencia de cada clase\n",
    "total_samples = len(y_train.flatten())  # Total de puntos\n",
    "class_weights = {cls: total_samples / (len(unique_classes) * count) for cls, count in zip(unique_classes, class_counts)}\n",
    "\n",
    "print(\"Pesos de las clases:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:19.384048Z",
     "iopub.status.busy": "2025-02-19T23:05:19.383654Z",
     "iopub.status.idle": "2025-02-19T23:05:19.396156Z",
     "shell.execute_reply": "2025-02-19T23:05:19.395116Z",
     "shell.execute_reply.started": "2025-02-19T23:05:19.384013Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "MAX_POINTS = 16384\n",
    "\n",
    "def tnet(inputs, num_features):\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(num_features * num_features, kernel_initializer='zeros',\n",
    "                     bias_initializer=tf.keras.initializers.Constant(tf.eye(num_features).numpy().flatten()))(x)\n",
    "    transform_matrix = layers.Reshape((num_features, num_features))(x)\n",
    "\n",
    "    def transform(inputs_and_matrix):\n",
    "        inputs, matrix = inputs_and_matrix\n",
    "        return tf.matmul(inputs, matrix)\n",
    "\n",
    "    transformed_inputs = layers.Lambda(transform)([inputs, transform_matrix])\n",
    "    transformed_inputs = layers.Lambda(lambda t: tf.ensure_shape(t, (None, MAX_POINTS, num_features)))(transformed_inputs)\n",
    "\n",
    "    return transformed_inputs\n",
    "\n",
    "def build_pointnet(num_classes, input_dim=3, max_points=MAX_POINTS):\n",
    "    inputs = tf.keras.Input(shape=(None, input_dim))\n",
    "\n",
    "    x = tnet(inputs, input_dim)\n",
    "\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tnet(x, 64)\n",
    "\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    global_features = layers.GlobalMaxPooling1D()(x)\n",
    "    global_features = layers.Lambda(lambda t: tf.expand_dims(t, axis=1))(global_features)\n",
    "    global_features = layers.Lambda(lambda t: tf.repeat(t, repeats=max_points, axis=1))(global_features)\n",
    "\n",
    "    x = layers.Lambda(lambda t: tf.ensure_shape(t, (None, max_points, 1024)))(x)\n",
    "\n",
    "    x = layers.concatenate([x, global_features], axis=-1)\n",
    "\n",
    "    x = layers.Conv1D(512, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(256, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    outputs = layers.Conv1D(num_classes, 1, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:24.002726Z",
     "iopub.status.busy": "2025-02-19T23:05:24.002339Z",
     "iopub.status.idle": "2025-02-19T23:05:24.009292Z",
     "shell.execute_reply": "2025-02-19T23:05:24.007668Z",
     "shell.execute_reply.started": "2025-02-19T23:05:24.002692Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MeanIoUWrapper(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "        self.metric.update_state(y_true, y_pred_labels)\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n",
    "\n",
    "class MeanIoUWrapper_2(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int32)  # âœ… Asegurar que y_true sea int32\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "\n",
    "        self.metric.update_state(y_true, y_pred_labels, sample_weight)  # âœ… Pasar sample_weight\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:26.333484Z",
     "iopub.status.busy": "2025-02-19T23:05:26.333162Z",
     "iopub.status.idle": "2025-02-19T23:05:27.232814Z",
     "shell.execute_reply": "2025-02-19T23:05:27.231873Z",
     "shell.execute_reply.started": "2025-02-19T23:05:26.333458Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convertir los pesos a tensores\n",
    "class_weight_tensor = tf.constant([class_weights[i] for i in range(len(unique_classes))], dtype=tf.float32)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    \"\"\"Aplica pesos de clases a la pÃ©rdida de entropÃ­a cruzada\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    sample_weights = tf.gather(class_weight_tensor, y_true)  # Asigna el peso segÃºn la clase\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return loss * sample_weights  # Escala la pÃ©rdida por el peso de la clase\n",
    "\n",
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Convertir y_true a int32 por seguridad\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "        # Aplicar softmax si aÃºn no se ha aplicado\n",
    "        y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "\n",
    "        # Extraer las probabilidades correctas usando one-hot encoding\n",
    "        y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])  # (batch, puntos, clases)\n",
    "        y_pred = tf.reduce_sum(y_pred * y_true_one_hot, axis=-1)  # (batch, puntos)\n",
    "\n",
    "        # Evitar log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        # Calcular la Focal Loss\n",
    "        ce = -tf.math.log(y_pred)\n",
    "        weight = alpha * tf.pow(1 - y_pred, gamma)\n",
    "        \n",
    "        return tf.reduce_mean(weight * ce)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:46.005650Z",
     "iopub.status.busy": "2025-02-19T23:05:46.005327Z",
     "iopub.status.idle": "2025-02-19T23:05:46.259051Z",
     "shell.execute_reply": "2025-02-19T23:05:46.257938Z",
     "shell.execute_reply.started": "2025-02-19T23:05:46.005625Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 3\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "# Definir el modelo PointNet\n",
    "model = build_pointnet(num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n",
    "mean_iou_wrapper = MeanIoUWrapper(num_classes=NUM_CLASSES)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.0003)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"accuracy\", mean_iou_wrapper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.259Z",
     "iopub.execute_input": "2025-02-19T23:10:17.372591Z",
     "iopub.status.busy": "2025-02-19T23:10:17.372234Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train_shuffle,  \n",
    "    y_train_shuffle,  \n",
    "    validation_data=(x_val_shuffle, y_val_shuffle), \n",
    "    epochs=60,\n",
    "    batch_size = 16,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"pointnet_goose_16k_3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Supongamos que estos vienen de tu modelo entrenado (Keras history)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss')\n",
    "mean_iou = history.history.get('mean_iou_wrapper')\n",
    "val_mean_iou = history.history.get('val_mean_iou_wrapper')\n",
    "accuracy = history.history.get('accuracy')\n",
    "val_accuracy = history.history.get('val_accuracy')\n",
    "\n",
    "# Crear figura con 1 fila y 3 columnas de subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"Loss Curve\", \"Mean IoU Curve\", \"Accuracy Curve\")\n",
    ")\n",
    "\n",
    "# 1) GRÃFICO DE PÃ‰RDIDA (col=1)\n",
    "epochs_loss = list(range(1, len(loss) + 1))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_loss,\n",
    "        y=loss,\n",
    "        mode='lines+markers',\n",
    "        name='Train Loss'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "if val_loss:\n",
    "    epochs_val_loss = list(range(1, len(val_loss) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_val_loss,\n",
    "            y=val_loss,\n",
    "            mode='lines+markers',\n",
    "            name='Validation Loss'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1, range=[0, 10])  # Rango ajustado\n",
    "\n",
    "# 2) GRÃFICO DE Mean IoU (col=2) -> Limitar valores a [0,1]\n",
    "if mean_iou:\n",
    "    mean_iou = np.clip(mean_iou, 0, 1)\n",
    "    epochs_mean_iou = list(range(1, len(mean_iou) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_mean_iou,\n",
    "            y=mean_iou,\n",
    "            mode='lines+markers',\n",
    "            name='Train Mean IoU'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    if val_mean_iou:\n",
    "        val_mean_iou = np.clip(val_mean_iou, 0, 1)\n",
    "        epochs_val_mean_iou = list(range(1, len(val_mean_iou) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_mean_iou,\n",
    "                y=val_mean_iou,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Mean IoU'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Mean IoU', row=1, col=2, range=[0, 1])\n",
    "\n",
    "# 3) GRÃFICO DE ACCURACY (col=3) -> Limitar valores a [0,1]\n",
    "if accuracy:\n",
    "    accuracy = np.clip(accuracy, 0, 1)\n",
    "    epochs_acc = list(range(1, len(accuracy) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_acc,\n",
    "            y=accuracy,\n",
    "            mode='lines+markers',\n",
    "            name='Train Accuracy'\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "    if val_accuracy:\n",
    "        val_accuracy = np.clip(val_accuracy, 0, 1)\n",
    "        epochs_val_acc = list(range(1, len(val_accuracy) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_acc,\n",
    "                y=val_accuracy,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Accuracy'\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=3)\n",
    "fig.update_yaxes(title_text='Accuracy', row=1, col=3, range=[0, 1])\n",
    "\n",
    "# Ajustes generales de la figura\n",
    "fig.update_layout(\n",
    "    width=1300,\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6693936,
     "sourceId": 10786805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
