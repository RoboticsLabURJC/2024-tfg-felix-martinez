{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:37.429123Z",
     "iopub.status.busy": "2025-02-19T22:45:37.428898Z",
     "iopub.status.idle": "2025-02-19T22:45:42.190720Z",
     "shell.execute_reply": "2025-02-19T22:45:42.189734Z",
     "shell.execute_reply.started": "2025-02-19T22:45:37.429101Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 14:36:11.836866: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-20 14:36:12.457283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de cuDNN: 8\n",
      "Versión de CUDA: 11.8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Versión de cuDNN:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])\n",
    "print(\"Versión de CUDA:\", tf.sysconfig.get_build_info()[\"cuda_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Versión de TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOSE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:42.192969Z",
     "iopub.status.busy": "2025-02-19T22:45:42.192274Z",
     "iopub.status.idle": "2025-02-19T22:45:42.206732Z",
     "shell.execute_reply": "2025-02-19T22:45:42.205795Z",
     "shell.execute_reply.started": "2025-02-19T22:45:42.192927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_bin_file(bin_path: str, num_points: int = 4098) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    points = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)[:, :3]\n",
    "    if points.shape[0] > num_points:\n",
    "        indices = np.random.choice(points.shape[0], num_points, replace=False)\n",
    "        return points[indices], indices\n",
    "    elif points.shape[0] < num_points:\n",
    "        pad = np.zeros((num_points - points.shape[0], 3), dtype=np.float32)\n",
    "        return np.vstack((points, pad)), np.arange(points.shape[0])  # No padding en índices\n",
    "    return points, np.arange(num_points)\n",
    "\n",
    "# GOOSE Categories mejoradas\n",
    "category_mapping = {\n",
    "    0: [43, 38, 58, 29, 41, 42, 44, 39, 55], # Construction\n",
    "    1: [4, 45, 6, 40, 60, 61, 33, 32, 14], # Object\n",
    "    2: [7, 22, 9, 26, 11, 21], # Road\n",
    "    3: [48, 47, 1, 19, 46, 10, 25], # Sign\n",
    "    4: [23, 3, 24, 31, 2], # Terrain  \n",
    "    5: [51, 50, 5, 18], # Drivable Vegetation\n",
    "    6: [28, 27, 62, 52, 16, 30, 59, 17], # Non Drivable Vegetation\n",
    "    7: [13, 15, 12, 36, 57, 49, 20, 35, 37, 34, 63], # Vehicle\n",
    "    8: [8, 56, 0, 53, 54], # Void\n",
    "}\n",
    "\n",
    "# Reverse mapping for fast lookup\n",
    "label_to_category = {label: cat for cat, labels in category_mapping.items() for label in labels}\n",
    "\n",
    "def map_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mapea etiquetas al sistema de categorías definido en category_mapping.\n",
    "    \"\"\"\n",
    "    return np.array([label_to_category.get(label, 8) for label in labels], dtype=np.uint8)\n",
    "\n",
    "def load_label_file(label_path: str, indices: np.ndarray, num_points: int = 16384) -> np.ndarray:\n",
    "    labels = np.fromfile(label_path, dtype=np.uint32) & 0xFFFF\n",
    "    if labels.shape[0] > num_points:\n",
    "        labels = labels[indices]  # Usar los mismos índices de los puntos\n",
    "    elif labels.shape[0] < num_points:\n",
    "        pad = np.full(num_points - labels.shape[0], 8, dtype=np.uint16)\n",
    "        labels = np.hstack((labels, pad))\n",
    "    return map_labels(labels)\n",
    "\n",
    "def load_dataset(bin_files: List[str], label_files: List[str], num_points: int = 16384) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    x_data, y_data = [], []\n",
    "    \n",
    "    for bin_f, label_f in tqdm(zip(bin_files, label_files), total=len(bin_files), desc=\"Cargando datos\"):\n",
    "        points, indices = load_bin_file(bin_f, num_points)\n",
    "        labels = load_label_file(label_f, indices, num_points)\n",
    "        x_data.append(points)\n",
    "        y_data.append(labels)\n",
    "    \n",
    "    return np.array(x_data, dtype=np.float32), np.array(y_data, dtype=np.uint8)\n",
    "\n",
    "def get_file_paths(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Obtiene lista de archivos en un directorio.\n",
    "    \"\"\"\n",
    "    return sorted([str(f) for f in Path(data_dir).glob(\"*.*\")])\n",
    "\n",
    "def load_all_data(x_train_dir: str, y_train_dir: str, x_val_dir: str, y_val_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga todos los datos de entrenamiento y validación con barra de progreso en una sola línea por conjunto de datos.\n",
    "    \"\"\"\n",
    "    x_train_files = get_file_paths(x_train_dir)\n",
    "    y_train_files = get_file_paths(y_train_dir)\n",
    "    x_val_files = get_file_paths(x_val_dir)\n",
    "    y_val_files = get_file_paths(y_val_dir)\n",
    "    \n",
    "    assert len(x_train_files) == len(y_train_files), \"Número de archivos x_train y y_train no coincide.\"\n",
    "    assert len(x_val_files) == len(y_val_files), \"Número de archivos x_val y y_val no coincide.\"\n",
    "    \n",
    "    print(\"Cargando datos de entrenamiento...\")\n",
    "    x_train, y_train = load_dataset(x_train_files, y_train_files)\n",
    "    \n",
    "    print(\"Cargando datos de validación...\")\n",
    "    x_val, y_val = load_dataset(x_val_files, y_val_files)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:48:25.468327Z",
     "iopub.status.busy": "2025-02-19T22:48:25.468011Z",
     "iopub.status.idle": "2025-02-19T22:57:03.863818Z",
     "shell.execute_reply": "2025-02-19T22:57:03.862746Z",
     "shell.execute_reply.started": "2025-02-19T22:48:25.468304Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_path = \"/home/fmartinez/datasets/lidar/train\"\n",
    "y_train_path = \"/home/fmartinez/datasets/labels/train\"\n",
    "x_val_path = \"/home/fmartinez/datasets_val/lidar/val\"\n",
    "y_val_path = \"/home/fmartinez/datasets_val/labels/val\"\n",
    "\n",
    "x_train, y_train, x_val, y_val = load_all_data(x_train_path, y_train_path, x_val_path, y_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:02:41.161169Z",
     "iopub.status.busy": "2025-02-19T23:02:41.160826Z",
     "iopub.status.idle": "2025-02-19T23:02:43.101788Z",
     "shell.execute_reply": "2025-02-19T23:02:43.101090Z",
     "shell.execute_reply.started": "2025-02-19T23:02:41.161144Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_permutados_train = np.random.permutation(x_train.shape[0])\n",
    "indices_permutados_val = np.random.permutation(x_val.shape[0])\n",
    "\n",
    "x_train_shuffle = x_train[indices_permutados_train]\n",
    "y_train_shuffle = y_train[indices_permutados_train]\n",
    "x_val_shuffle = x_val[indices_permutados_val]\n",
    "y_val_shuffle = y_val[indices_permutados_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train) and len(x_val) == len(y_val)\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_train)} nubes de puntos de {y_train.shape[0]} puntos\")\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_val)} nubes de puntos de {y_val.shape[0]} puntos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:07.718183Z",
     "iopub.status.busy": "2025-02-19T23:04:07.717827Z",
     "iopub.status.idle": "2025-02-19T23:04:07.724041Z",
     "shell.execute_reply": "2025-02-19T23:04:07.723077Z",
     "shell.execute_reply.started": "2025-02-19T23:04:07.718155Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_3D(xyz, labels):\n",
    "    \"\"\"\n",
    "    Visualiza una nube de puntos en 3D con Plotly.\n",
    "    - xyz: (num_points, 3) array con coordenadas (X, Y, Z).\n",
    "    - labels: (num_points,) array con etiquetas semánticas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Definir 9 colores predefinidos en formato RGB\n",
    "    predefined_colors = [\n",
    "        \"rgb(255, 0, 0)\",    # Rojo\n",
    "        \"rgb(0, 255, 0)\",    # Verde\n",
    "        \"rgb(0, 0, 255)\",    # Azul\n",
    "        \"rgb(255, 255, 0)\",  # Amarillo\n",
    "        \"rgb(255, 165, 0)\",  # Naranja\n",
    "        \"rgb(128, 0, 128)\",  # Púrpura\n",
    "        \"rgb(0, 255, 255)\",  # Cian\n",
    "        \"rgb(255, 192, 203)\",# Rosa\n",
    "        \"rgb(128, 128, 128)\" # Gris\n",
    "    ]\n",
    "\n",
    "    # Asignar colores según la etiqueta (se asume que las etiquetas van de 0 a 8)\n",
    "    point_colors = [predefined_colors[label % len(predefined_colors)] for label in labels]\n",
    "\n",
    "    # Crear la figura 3D en Plotly\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=xyz[:, 0], y=xyz[:, 1], z=xyz[:, 2],  # Coordenadas X, Y, Z\n",
    "        mode='markers',\n",
    "        marker=dict(size=1, color=point_colors, opacity=0.8)  # Color basado en etiquetas\n",
    "    ))\n",
    "\n",
    "    # Configurar etiquetas y título\n",
    "    fig.update_layout(\n",
    "        title=\"Nube de Puntos con Etiquetas Semánticas\",\n",
    "        scene=dict(xaxis_title=\"X\", yaxis_title=\"Y\", zaxis_title=\"Z\")\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:24.140632Z",
     "iopub.status.busy": "2025-02-19T23:04:24.140280Z",
     "iopub.status.idle": "2025-02-19T23:04:25.355377Z",
     "shell.execute_reply": "2025-02-19T23:04:25.353411Z",
     "shell.execute_reply.started": "2025-02-19T23:04:24.140600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Llamar a la función con la primera nube de puntos\n",
    "plot_3D(x_train_shuffle[5310], y_train_shuffle[5310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:08.622104Z",
     "iopub.status.busy": "2025-02-19T23:05:08.621750Z",
     "iopub.status.idle": "2025-02-19T23:05:11.936407Z",
     "shell.execute_reply": "2025-02-19T23:05:11.935406Z",
     "shell.execute_reply.started": "2025-02-19T23:05:08.622075Z"
    }
   },
   "outputs": [],
   "source": [
    "# Contar cuántos puntos hay por clase\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print(len(y_train))\n",
    "\n",
    "# Mostrar distribución de clases))\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Clase {cls}: {count} puntos\")\n",
    "\n",
    "# Opcional: visualizar la distribución con un gráfico de barras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(unique_classes, class_counts)\n",
    "plt.xlabel(\"Clase\")\n",
    "plt.ylabel(\"Cantidad de puntos\")\n",
    "plt.title(\"Distribución de etiquetas en y_train\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:16.216947Z",
     "iopub.status.busy": "2025-02-19T23:05:16.216572Z",
     "iopub.status.idle": "2025-02-19T23:05:16.246341Z",
     "shell.execute_reply": "2025-02-19T23:05:16.245375Z",
     "shell.execute_reply.started": "2025-02-19T23:05:16.216914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular pesos inversamente proporcionales a la frecuencia de cada clase\n",
    "total_samples = len(y_train.flatten())  # Total de puntos\n",
    "class_weights = {cls: total_samples / (len(unique_classes) * count) for cls, count in zip(unique_classes, class_counts)}\n",
    "\n",
    "print(\"Pesos de las clases:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:19.384048Z",
     "iopub.status.busy": "2025-02-19T23:05:19.383654Z",
     "iopub.status.idle": "2025-02-19T23:05:19.396156Z",
     "shell.execute_reply": "2025-02-19T23:05:19.395116Z",
     "shell.execute_reply.started": "2025-02-19T23:05:19.384013Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def tnet(inputs, num_features):\n",
    "    \"\"\"\n",
    "    Implementación de T-Net para aprender transformaciones afines.\n",
    "    \"\"\"\n",
    "    x = layers.Conv1D(64, 1, activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(1024, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Matriz de transformación (num_features x num_features)\n",
    "    x = layers.Dense(num_features * num_features, kernel_initializer='zeros',\n",
    "                     bias_initializer=tf.keras.initializers.Constant(tf.eye(num_features).numpy().flatten()))(x)\n",
    "    transform_matrix = layers.Reshape((num_features, num_features))(x)\n",
    "\n",
    "    # Aplicar la transformación correctamente\n",
    "    def transform(inputs_and_matrix):\n",
    "        inputs, matrix = inputs_and_matrix\n",
    "        return tf.matmul(inputs, matrix)\n",
    "\n",
    "    transformed_inputs = layers.Lambda(transform)([inputs, transform_matrix])\n",
    "\n",
    "    return transformed_inputs  # Retorna los datos transformados\n",
    "\n",
    "\n",
    "def build_pointnet(num_classes, input_dim=3, max_points=16384):\n",
    "    inputs = tf.keras.Input(shape=(None, input_dim))  # Entrada: N puntos con D características\n",
    "\n",
    "    # Aplicar T-Net en la entrada para corregir la orientación de los puntos\n",
    "    x = tnet(inputs, input_dim)\n",
    "\n",
    "    # MLP Layers\n",
    "    x = layers.Conv1D(64, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(64, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Aplicar T-Net en características para mejorar representaciones intermedias\n",
    "    x = tnet(x, 64)\n",
    "\n",
    "    x = layers.Conv1D(1024, 1, activation='relu')(x)  # MLP variable (64, 256, 1024, 2048)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # **Corrección en la manipulación de `global_features`**\n",
    "    global_features = layers.GlobalMaxPooling1D()(x)  # (batch, 1024)\n",
    "    global_features = layers.Lambda(lambda t: tf.expand_dims(t, axis=1))(global_features)  # (batch, 1, 1024)\n",
    "    global_features = layers.Lambda(lambda t: tf.tile(t, [1, max_points, 1]))(global_features)  # (batch, 16384, 1024)\n",
    "\n",
    "    print(f\"Shape de x antes de concatenar: {x.shape}\")  # (batch, 16384, 1024)\n",
    "    print(f\"Shape de global_features: {global_features.shape}\")  # (batch, 16384, 1024)\n",
    "\n",
    "    x = layers.concatenate([x, global_features], axis=-1)  # Ambos tienen (batch, 16384, 2048)\n",
    "\n",
    "    # MLP final para clasificación\n",
    "    x = layers.Conv1D(512, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(256, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    #x = layers.Dropout(0.3)(x)  # Regularización\n",
    "\n",
    "    outputs = layers.Conv1D(num_classes, 1, activation='softmax')(x)  # Clasificación por punto\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:24.002726Z",
     "iopub.status.busy": "2025-02-19T23:05:24.002339Z",
     "iopub.status.idle": "2025-02-19T23:05:24.009292Z",
     "shell.execute_reply": "2025-02-19T23:05:24.007668Z",
     "shell.execute_reply.started": "2025-02-19T23:05:24.002692Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MeanIoUWrapper(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "        self.metric.update_state(y_true, y_pred_labels)\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:26.333484Z",
     "iopub.status.busy": "2025-02-19T23:05:26.333162Z",
     "iopub.status.idle": "2025-02-19T23:05:27.232814Z",
     "shell.execute_reply": "2025-02-19T23:05:27.231873Z",
     "shell.execute_reply.started": "2025-02-19T23:05:26.333458Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convertir los pesos a tensores\n",
    "class_weight_tensor = tf.constant([class_weights[i] for i in range(len(unique_classes))], dtype=tf.float32)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    \"\"\"Aplica pesos de clases a la pérdida de entropía cruzada\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    sample_weights = tf.gather(class_weight_tensor, y_true)  # Asigna el peso según la clase\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return loss * sample_weights  # Escala la pérdida por el peso de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:46.005650Z",
     "iopub.status.busy": "2025-02-19T23:05:46.005327Z",
     "iopub.status.idle": "2025-02-19T23:05:46.259051Z",
     "shell.execute_reply": "2025-02-19T23:05:46.257938Z",
     "shell.execute_reply.started": "2025-02-19T23:05:46.005625Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 3\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Usando {strategy.num_replicas_in_sync} GPU(s)\")\n",
    "\n",
    "\n",
    "# Definir el modelo PointNet\n",
    "model = build_pointnet(num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n",
    "\n",
    "mean_iou_wrapper = MeanIoUWrapper(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=0.0003),  # Adam con weight decay\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"accuracy\", mean_iou_wrapper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.259Z",
     "iopub.execute_input": "2025-02-19T23:10:17.372591Z",
     "iopub.status.busy": "2025-02-19T23:10:17.372234Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train_shuffle,  \n",
    "    y_train_shuffle,  \n",
    "    validation_data=(x_val_shuffle, y_val_shuffle), \n",
    "    epochs=30,\n",
    "    batch_size = 32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Guardar modelo completo en formato .keras (recomendado desde TensorFlow 2.6+)\n",
    "model.save(\"mi_modelo.keras\")\n",
    "\n",
    "# Guardar modelo en formato .h5 (si necesitas compatibilidad con versiones más antiguas)\n",
    "model.save(\"mi_modelo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraer métricas\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss')\n",
    "mean_iou = history.history.get('mean_iou_wrapper')\n",
    "val_mean_iou = history.history.get('val_mean_iou_wrapper')\n",
    "\n",
    "# Crear figura\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Gráfico de la pérdida\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss, label='Train Loss')\n",
    "if val_loss:\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curve')\n",
    "\n",
    "# Gráfico de Mean IoU\n",
    "if mean_iou:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(mean_iou, label='Train Mean IoU')\n",
    "    if val_mean_iou:\n",
    "        plt.plot(val_mean_iou, label='Validation Mean IoU')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean IoU')\n",
    "    plt.legend()\n",
    "    plt.title('Mean IoU Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 378s 782ms/step - accuracy: 0.1974 - loss: 2.0782 - mean_iou_wrapper: 0.0620 - val_accuracy: 0.2606 - val_loss: 2.3051 - val_mean_iou_wrapper: 0.0676\n",
    "Epoch 2/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.2560 - loss: 1.8835 - mean_iou_wrapper: 0.0811 - val_accuracy: 0.2635 - val_loss: 2.0695 - val_mean_iou_wrapper: 0.0844\n",
    "Epoch 3/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.2668 - loss: 1.8064 - mean_iou_wrapper: 0.0890 - val_accuracy: 0.2187 - val_loss: 2.0295 - val_mean_iou_wrapper: 0.0717\n",
    "Epoch 4/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.3200 - loss: 1.7121 - mean_iou_wrapper: 0.1051 - val_accuracy: 0.3783 - val_loss: 1.8921 - val_mean_iou_wrapper: 0.1164\n",
    "Epoch 5/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.3409 - loss: 1.6596 - mean_iou_wrapper: 0.1135 - val_accuracy: 0.3608 - val_loss: 1.9976 - val_mean_iou_wrapper: 0.1206\n",
    "Epoch 6/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.3473 - loss: 1.6475 - mean_iou_wrapper: 0.1180 - val_accuracy: 0.2971 - val_loss: 1.8919 - val_mean_iou_wrapper: 0.0930\n",
    "Epoch 7/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.3850 - loss: 1.5954 - mean_iou_wrapper: 0.1280 - val_accuracy: 0.3707 - val_loss: 2.1627 - val_mean_iou_wrapper: 0.1101\n",
    "Epoch 8/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.3928 - loss: 1.5726 - mean_iou_wrapper: 0.1305 - val_accuracy: 0.3419 - val_loss: 1.8835 - val_mean_iou_wrapper: 0.1155\n",
    "Epoch 9/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.4121 - loss: 1.5251 - mean_iou_wrapper: 0.1386 - val_accuracy: 0.3426 - val_loss: 2.0606 - val_mean_iou_wrapper: 0.1069\n",
    "Epoch 10/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.4367 - loss: 1.4814 - mean_iou_wrapper: 0.1481 - val_accuracy: 0.4342 - val_loss: 1.9583 - val_mean_iou_wrapper: 0.1401\n",
    "Epoch 11/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.4149 - loss: 1.5287 - mean_iou_wrapper: 0.1395 - val_accuracy: 0.3544 - val_loss: 1.8760 - val_mean_iou_wrapper: 0.1184\n",
    "Epoch 12/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.4392 - loss: 1.4580 - mean_iou_wrapper: 0.1505 - val_accuracy: 0.3511 - val_loss: 1.7919 - val_mean_iou_wrapper: 0.1244\n",
    "Epoch 13/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.4695 - loss: 1.3695 - mean_iou_wrapper: 0.1660 - val_accuracy: 0.4573 - val_loss: 1.6821 - val_mean_iou_wrapper: 0.1545\n",
    "Epoch 14/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 722ms/step - accuracy: 0.4931 - loss: 1.3009 - mean_iou_wrapper: 0.1798 - val_accuracy: 0.5514 - val_loss: 1.6161 - val_mean_iou_wrapper: 0.1899\n",
    "Epoch 15/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 722ms/step - accuracy: 0.5151 - loss: 1.2271 - mean_iou_wrapper: 0.2020 - val_accuracy: 0.6254 - val_loss: 1.8242 - val_mean_iou_wrapper: 0.2169\n",
    "Epoch 16/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 722ms/step - accuracy: 0.5359 - loss: 1.1652 - mean_iou_wrapper: 0.2154 - val_accuracy: 0.5977 - val_loss: 1.6061 - val_mean_iou_wrapper: 0.2147\n",
    "Epoch 17/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 722ms/step - accuracy: 0.5537 - loss: 1.1096 - mean_iou_wrapper: 0.2278 - val_accuracy: 0.6259 - val_loss: 2.0443 - val_mean_iou_wrapper: 0.1971\n",
    "Epoch 18/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 722ms/step - accuracy: 0.5662 - loss: 1.0688 - mean_iou_wrapper: 0.2377 - val_accuracy: 0.5784 - val_loss: 1.8561 - val_mean_iou_wrapper: 0.1962\n",
    "Epoch 19/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.5646 - loss: 1.0748 - mean_iou_wrapper: 0.2309 - val_accuracy: 0.5857 - val_loss: 1.7395 - val_mean_iou_wrapper: 0.2128\n",
    "Epoch 20/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.5844 - loss: 1.0161 - mean_iou_wrapper: 0.2496 - val_accuracy: 0.4979 - val_loss: 2.0896 - val_mean_iou_wrapper: 0.1632\n",
    "Epoch 21/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.6055 - loss: 0.9643 - mean_iou_wrapper: 0.2653 - val_accuracy: 0.5984 - val_loss: 1.8471 - val_mean_iou_wrapper: 0.2134\n",
    "Epoch 22/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.6205 - loss: 0.9233 - mean_iou_wrapper: 0.2813 - val_accuracy: 0.5607 - val_loss: 1.6588 - val_mean_iou_wrapper: 0.2047\n",
    "Epoch 23/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.6275 - loss: 0.8986 - mean_iou_wrapper: 0.2861 - val_accuracy: 0.5962 - val_loss: 1.7949 - val_mean_iou_wrapper: 0.2071\n",
    "Epoch 24/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.6379 - loss: 0.8737 - mean_iou_wrapper: 0.2948 - val_accuracy: 0.1219 - val_loss: 52.6255 - val_mean_iou_wrapper: 0.0459\n",
    "Epoch 25/30\n",
    "204/483 ━━━━━━━━━━━━━━━━━━━━ 3:17 707ms/step - accuracy: 0.6255 - loss: 0.8810 - mean_iou_wrapper: 0.2887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6693936,
     "sourceId": 10786805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
