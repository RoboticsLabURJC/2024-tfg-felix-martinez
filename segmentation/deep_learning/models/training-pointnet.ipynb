{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:37.429123Z",
     "iopub.status.busy": "2025-02-19T22:45:37.428898Z",
     "iopub.status.idle": "2025-02-19T22:45:42.190720Z",
     "shell.execute_reply": "2025-02-19T22:45:42.189734Z",
     "shell.execute_reply.started": "2025-02-19T22:45:37.429101Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Versión de cuDNN:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])\n",
    "print(\"Versión de CUDA:\", tf.sysconfig.get_build_info()[\"cuda_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Versión de TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOSE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:42.192969Z",
     "iopub.status.busy": "2025-02-19T22:45:42.192274Z",
     "iopub.status.idle": "2025-02-19T22:45:42.206732Z",
     "shell.execute_reply": "2025-02-19T22:45:42.205795Z",
     "shell.execute_reply.started": "2025-02-19T22:45:42.192927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_POINTS = 16384 # 256, 512, 1024, 2048, 4096, 8192, 16384, 32768\n",
    "\n",
    "def filter_points_within_radius(points: np.ndarray, radius: float = 25.0) -> np.ndarray:\n",
    "    distances = np.linalg.norm(points, axis=1)  # Calcula la distancia euclidiana para cada punto\n",
    "    mask = distances <= radius  # Crea una máscara booleana para los puntos dentro del radio\n",
    "    return points[mask]  # Devuelve solo los puntos dentro del radio\n",
    "\n",
    "\n",
    "# GOOSE Categories mejoradas\n",
    "category_mapping = {\n",
    "    0: [43, 38, 58, 29, 41, 42, 44, 39, 55], # Construction\n",
    "    1: [4, 45, 6, 40, 60, 61, 33, 32, 14], # Object\n",
    "    2: [7, 22, 9, 26, 11, 21], # Road\n",
    "    3: [48, 47, 1, 19, 46, 10, 25], # Sign\n",
    "    4: [23, 3, 24, 31, 2], # Terrain  \n",
    "    5: [51, 50, 5, 18], # Drivable Vegetation\n",
    "    6: [28, 27, 62, 52, 16, 30, 59, 17], # Non Drivable Vegetation\n",
    "    7: [13, 15, 12, 36, 57, 49, 20, 35, 37, 34, 63], # Vehicle\n",
    "    8: [8, 56, 0, 53, 54], # Void\n",
    "}\n",
    "\n",
    "def load_bin_file(bin_path: str, num_points: int = NUM_POINTS, radius: float = 25.0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    points = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)[:, :3]  # Cargar coordenadas (x, y, z)\n",
    "    \n",
    "    # 🔹 Filtrar solo los puntos dentro del radio de 25m\n",
    "    distances = np.linalg.norm(points, axis=1)\n",
    "    mask = distances <= radius\n",
    "    points = points[mask]  # Solo mantener los puntos dentro del radio\n",
    "\n",
    "    num_available = points.shape[0]  # Puntos después del filtrado\n",
    "\n",
    "    if num_available >= num_points:\n",
    "        indices = np.random.choice(num_available, num_points, replace=False)  # 🔹 Selección aleatoria\n",
    "        return points[indices], np.where(mask)[0][indices]  # Devuelve los índices relativos al archivo original\n",
    "    else:\n",
    "        return points, np.where(mask)[0]  # Devuelve los índices relativos al archivo original\n",
    "\n",
    "# Reverse mapping for fast lookup\n",
    "label_to_category = {label: cat for cat, labels in category_mapping.items() for label in labels}\n",
    "\n",
    "def map_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mapea etiquetas al sistema de categorías definido en category_mapping.\n",
    "    \"\"\"\n",
    "    return np.array([label_to_category.get(label, 8) for label in labels], dtype=np.uint8)\n",
    "\n",
    "def load_label_file(label_path: str, indices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Carga las etiquetas y extrae solo las que corresponden a los índices seleccionados.\n",
    "\n",
    "    Parámetros:\n",
    "    - label_path (str): Ruta del archivo de etiquetas.\n",
    "    - indices (np.ndarray): Índices usados para seleccionar los puntos.\n",
    "\n",
    "    Retorna:\n",
    "    - np.ndarray: Etiquetas correspondientes a los puntos seleccionados.\n",
    "    \"\"\"\n",
    "    labels = np.fromfile(label_path, dtype=np.uint32) & 0xFFFF  # Cargar etiquetas completas\n",
    "    return map_labels(labels[indices])  # 🔹 Extraer solo las etiquetas correspondientes a los índices seleccionados\n",
    "\n",
    "def load_dataset(bin_files: List[str], label_files: List[str], num_points: int = NUM_POINTS) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga las nubes de puntos y etiquetas, asegurando que cada muestra tenga exactamente num_points puntos.\n",
    "    Si una nube tiene menos de num_points después del filtrado, se descarta.\n",
    "\n",
    "    Parámetros:\n",
    "    - bin_files: Lista de rutas a archivos de nubes de puntos.\n",
    "    - label_files: Lista de rutas a archivos de etiquetas.\n",
    "    - num_points: Número fijo de puntos en cada nube.\n",
    "\n",
    "    Retorna:\n",
    "    - x_data: np.ndarray de forma (N, num_points, 3).\n",
    "    - y_data: np.ndarray de forma (N, num_points).\n",
    "    \"\"\"\n",
    "    x_data, y_data = [], []\n",
    "    \n",
    "    for bin_f, label_f in tqdm(zip(bin_files, label_files), total=len(bin_files), desc=\"Cargando datos\"):\n",
    "        points, indices = load_bin_file(bin_f, num_points)\n",
    "        \n",
    "        # 🔹 Solo aceptar nubes con suficientes puntos\n",
    "        if points.shape[0] < num_points:\n",
    "            continue  # 🔹 Se descarta esta muestra\n",
    "\n",
    "        labels = load_label_file(label_f, indices)\n",
    "\n",
    "        x_data.append(points)\n",
    "        y_data.append(labels)\n",
    "\n",
    "    return np.array(x_data, dtype=np.float32), np.array(y_data, dtype=np.uint8)\n",
    "\n",
    "def get_file_paths(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Obtiene lista de archivos en un directorio.\n",
    "    \"\"\"\n",
    "    return sorted([str(f) for f in Path(data_dir).glob(\"*.*\")])\n",
    "\n",
    "def load_all_data(x_train_dir: str, y_train_dir: str, x_val_dir: str, y_val_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga todos los datos de entrenamiento y validación con barra de progreso en una sola línea por conjunto de datos.\n",
    "    \"\"\"\n",
    "    x_train_files = get_file_paths(x_train_dir)\n",
    "    y_train_files = get_file_paths(y_train_dir)\n",
    "    x_val_files = get_file_paths(x_val_dir)\n",
    "    y_val_files = get_file_paths(y_val_dir)\n",
    "    \n",
    "    assert len(x_train_files) == len(y_train_files), \"Número de archivos x_train y y_train no coincide.\"\n",
    "    assert len(x_val_files) == len(y_val_files), \"Número de archivos x_val y y_val no coincide.\"\n",
    "    \n",
    "    print(\"Cargando datos de entrenamiento...\")\n",
    "    x_train, y_train = load_dataset(x_train_files, y_train_files)\n",
    "    \n",
    "    print(\"Cargando datos de validación...\")\n",
    "    x_val, y_val = load_dataset(x_val_files, y_val_files)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:48:25.468327Z",
     "iopub.status.busy": "2025-02-19T22:48:25.468011Z",
     "iopub.status.idle": "2025-02-19T22:57:03.863818Z",
     "shell.execute_reply": "2025-02-19T22:57:03.862746Z",
     "shell.execute_reply.started": "2025-02-19T22:48:25.468304Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_path = \"/home/fmartinez/datasets/lidar/train\"\n",
    "y_train_path = \"/home/fmartinez/datasets/labels/train\"\n",
    "x_val_path = \"/home/fmartinez/datasets_val/lidar/val\"\n",
    "y_val_path = \"/home/fmartinez/datasets_val/labels/val\"\n",
    "\n",
    "x_train, y_train, x_val, y_val = load_all_data(x_train_path, y_train_path, x_val_path, y_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:02:41.161169Z",
     "iopub.status.busy": "2025-02-19T23:02:41.160826Z",
     "iopub.status.idle": "2025-02-19T23:02:43.101788Z",
     "shell.execute_reply": "2025-02-19T23:02:43.101090Z",
     "shell.execute_reply.started": "2025-02-19T23:02:41.161144Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_permutados_train = np.random.permutation(x_train.shape[0])\n",
    "indices_permutados_val = np.random.permutation(x_val.shape[0])\n",
    "\n",
    "x_train_shuffle = x_train[indices_permutados_train]\n",
    "y_train_shuffle = y_train[indices_permutados_train]\n",
    "x_val_shuffle = x_val[indices_permutados_val]\n",
    "y_val_shuffle = y_val[indices_permutados_val]\n",
    "\n",
    "x_train_shuffle.shape, y_train_shuffle.shape, x_val_shuffle.shape, y_val_shuffle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train) and len(x_val) == len(y_val)\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_train)} nubes de puntos de {y_train.shape[0]} puntos\")\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_val)} nubes de puntos de {y_val.shape[0]} puntos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:07.718183Z",
     "iopub.status.busy": "2025-02-19T23:04:07.717827Z",
     "iopub.status.idle": "2025-02-19T23:04:07.724041Z",
     "shell.execute_reply": "2025-02-19T23:04:07.723077Z",
     "shell.execute_reply.started": "2025-02-19T23:04:07.718155Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_3D(xyz, labels):\n",
    "    \"\"\"\n",
    "    Visualiza una nube de puntos en 3D con Plotly.\n",
    "    - xyz: (num_points, 3) array con coordenadas (X, Y, Z).\n",
    "    - labels: (num_points,) array con etiquetas semánticas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Definir 9 colores predefinidos en formato RGB\n",
    "    predefined_colors = [\n",
    "        \"rgb(255, 0, 0)\",    # Rojo\n",
    "        \"rgb(0, 255, 0)\",    # Verde\n",
    "        \"rgb(0, 0, 255)\",    # Azul\n",
    "        \"rgb(255, 255, 0)\",  # Amarillo\n",
    "        \"rgb(255, 165, 0)\",  # Naranja\n",
    "        \"rgb(128, 0, 128)\",  # Púrpura\n",
    "        \"rgb(0, 255, 255)\",  # Cian\n",
    "        \"rgb(255, 192, 203)\",# Rosa\n",
    "        \"rgb(128, 128, 128)\" # Gris\n",
    "    ]\n",
    "\n",
    "    # Asignar colores según la etiqueta (se asume que las etiquetas van de 0 a 8)\n",
    "    point_colors = [predefined_colors[label % len(predefined_colors)] for label in labels]\n",
    "\n",
    "    # Crear la figura 3D en Plotly\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=xyz[:, 0], y=xyz[:, 1], z=xyz[:, 2],  # Coordenadas X, Y, Z\n",
    "        mode='markers',\n",
    "        marker=dict(size=1, color=point_colors, opacity=0.8)  # Color basado en etiquetas\n",
    "    ))\n",
    "\n",
    "    # Configurar etiquetas y título\n",
    "    fig.update_layout(\n",
    "        title=\"Nube de Puntos con Etiquetas Semánticas\",\n",
    "        scene=dict(xaxis_title=\"X\", yaxis_title=\"Y\", zaxis_title=\"Z\")\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:24.140632Z",
     "iopub.status.busy": "2025-02-19T23:04:24.140280Z",
     "iopub.status.idle": "2025-02-19T23:04:25.355377Z",
     "shell.execute_reply": "2025-02-19T23:04:25.353411Z",
     "shell.execute_reply.started": "2025-02-19T23:04:24.140600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Llamar a la función con la primera nube de puntos\n",
    "plot_3D(x_train_shuffle[1000], y_train_shuffle[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:08.622104Z",
     "iopub.status.busy": "2025-02-19T23:05:08.621750Z",
     "iopub.status.idle": "2025-02-19T23:05:11.936407Z",
     "shell.execute_reply": "2025-02-19T23:05:11.935406Z",
     "shell.execute_reply.started": "2025-02-19T23:05:08.622075Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Contar cuántos puntos hay por clase\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Mostrar distribución de clases\n",
    "print(len(y_train))\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Clase {cls}: {count} puntos\")\n",
    "\n",
    "# Crear gráfico interactivo con Plotly\n",
    "fig = px.bar(x=unique_classes, y=class_counts, labels={'x': 'Clase', 'y': 'Cantidad de puntos'},\n",
    "             title='Distribución de etiquetas en y_train')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:16.216947Z",
     "iopub.status.busy": "2025-02-19T23:05:16.216572Z",
     "iopub.status.idle": "2025-02-19T23:05:16.246341Z",
     "shell.execute_reply": "2025-02-19T23:05:16.245375Z",
     "shell.execute_reply.started": "2025-02-19T23:05:16.216914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular pesos inversamente proporcionales a la frecuencia de cada clase\n",
    "total_samples = len(y_train.flatten())  # Total de puntos\n",
    "class_weights = {cls: total_samples / (len(unique_classes) * count) for cls, count in zip(unique_classes, class_counts)}\n",
    "\n",
    "print(\"Pesos de las clases:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:19.384048Z",
     "iopub.status.busy": "2025-02-19T23:05:19.383654Z",
     "iopub.status.idle": "2025-02-19T23:05:19.396156Z",
     "shell.execute_reply": "2025-02-19T23:05:19.395116Z",
     "shell.execute_reply.started": "2025-02-19T23:05:19.384013Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "MAX_POINTS = 16384\n",
    "\n",
    "def tnet(inputs, num_features):\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(num_features * num_features, kernel_initializer='zeros',\n",
    "                     bias_initializer=tf.keras.initializers.Constant(tf.eye(num_features).numpy().flatten()))(x)\n",
    "    transform_matrix = layers.Reshape((num_features, num_features))(x)\n",
    "\n",
    "    def transform(inputs_and_matrix):\n",
    "        inputs, matrix = inputs_and_matrix\n",
    "        return tf.matmul(inputs, matrix)\n",
    "\n",
    "    transformed_inputs = layers.Lambda(transform)([inputs, transform_matrix])\n",
    "    transformed_inputs = layers.Lambda(lambda t: tf.ensure_shape(t, (None, MAX_POINTS, num_features)))(transformed_inputs)\n",
    "\n",
    "    return transformed_inputs\n",
    "\n",
    "def build_pointnet(num_classes, input_dim=3, max_points=MAX_POINTS):\n",
    "    inputs = tf.keras.Input(shape=(None, input_dim))\n",
    "\n",
    "    x = tnet(inputs, input_dim)\n",
    "\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tnet(x, 64)\n",
    "\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    global_features = layers.GlobalMaxPooling1D()(x)\n",
    "    global_features = layers.Lambda(lambda t: tf.expand_dims(t, axis=1))(global_features)\n",
    "    global_features = layers.Lambda(lambda t: tf.repeat(t, repeats=max_points, axis=1))(global_features)\n",
    "\n",
    "    x = layers.Lambda(lambda t: tf.ensure_shape(t, (None, max_points, 1024)))(x)\n",
    "\n",
    "    x = layers.concatenate([x, global_features], axis=-1)\n",
    "\n",
    "    x = layers.Conv1D(512, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(256, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    outputs = layers.Conv1D(num_classes, 1, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:24.002726Z",
     "iopub.status.busy": "2025-02-19T23:05:24.002339Z",
     "iopub.status.idle": "2025-02-19T23:05:24.009292Z",
     "shell.execute_reply": "2025-02-19T23:05:24.007668Z",
     "shell.execute_reply.started": "2025-02-19T23:05:24.002692Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MeanIoUWrapper(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "        self.metric.update_state(y_true, y_pred_labels)\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n",
    "\n",
    "class MeanIoUWrapper_2(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int32)  # ✅ Asegurar que y_true sea int32\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "\n",
    "        self.metric.update_state(y_true, y_pred_labels, sample_weight)  # ✅ Pasar sample_weight\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:26.333484Z",
     "iopub.status.busy": "2025-02-19T23:05:26.333162Z",
     "iopub.status.idle": "2025-02-19T23:05:27.232814Z",
     "shell.execute_reply": "2025-02-19T23:05:27.231873Z",
     "shell.execute_reply.started": "2025-02-19T23:05:26.333458Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convertir los pesos a tensores\n",
    "class_weight_tensor = tf.constant([class_weights[i] for i in range(len(unique_classes))], dtype=tf.float32)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    \"\"\"Aplica pesos de clases a la pérdida de entropía cruzada\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    sample_weights = tf.gather(class_weight_tensor, y_true)  # Asigna el peso según la clase\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return loss * sample_weights  # Escala la pérdida por el peso de la clase\n",
    "\n",
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Convertir y_true a int32 por seguridad\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "        # Aplicar softmax si aún no se ha aplicado\n",
    "        y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "\n",
    "        # Extraer las probabilidades correctas usando one-hot encoding\n",
    "        y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])  # (batch, puntos, clases)\n",
    "        y_pred = tf.reduce_sum(y_pred * y_true_one_hot, axis=-1)  # (batch, puntos)\n",
    "\n",
    "        # Evitar log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        # Calcular la Focal Loss\n",
    "        ce = -tf.math.log(y_pred)\n",
    "        weight = alpha * tf.pow(1 - y_pred, gamma)\n",
    "        \n",
    "        return tf.reduce_mean(weight * ce)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:46.005650Z",
     "iopub.status.busy": "2025-02-19T23:05:46.005327Z",
     "iopub.status.idle": "2025-02-19T23:05:46.259051Z",
     "shell.execute_reply": "2025-02-19T23:05:46.257938Z",
     "shell.execute_reply.started": "2025-02-19T23:05:46.005625Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 3\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "# Definir el modelo PointNet\n",
    "model = build_pointnet(num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n",
    "mean_iou_wrapper = MeanIoUWrapper(num_classes=NUM_CLASSES)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.0003)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"accuracy\", mean_iou_wrapper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.259Z",
     "iopub.execute_input": "2025-02-19T23:10:17.372591Z",
     "iopub.status.busy": "2025-02-19T23:10:17.372234Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train_shuffle,  \n",
    "    y_train_shuffle,  \n",
    "    validation_data=(x_val_shuffle, y_val_shuffle), \n",
    "    epochs=60,\n",
    "    batch_size = 16,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"pointnet_goose_16k_3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Supongamos que estos vienen de tu modelo entrenado (Keras history)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss')\n",
    "mean_iou = history.history.get('mean_iou_wrapper')\n",
    "val_mean_iou = history.history.get('val_mean_iou_wrapper')\n",
    "accuracy = history.history.get('accuracy')\n",
    "val_accuracy = history.history.get('val_accuracy')\n",
    "\n",
    "# Crear figura con 1 fila y 3 columnas de subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"Loss Curve\", \"Mean IoU Curve\", \"Accuracy Curve\")\n",
    ")\n",
    "\n",
    "# 1) GRÁFICO DE PÉRDIDA (col=1)\n",
    "epochs_loss = list(range(1, len(loss) + 1))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_loss,\n",
    "        y=loss,\n",
    "        mode='lines+markers',\n",
    "        name='Train Loss'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "if val_loss:\n",
    "    epochs_val_loss = list(range(1, len(val_loss) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_val_loss,\n",
    "            y=val_loss,\n",
    "            mode='lines+markers',\n",
    "            name='Validation Loss'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1, range=[0, 10])  # Rango ajustado\n",
    "\n",
    "# 2) GRÁFICO DE Mean IoU (col=2) -> Limitar valores a [0,1]\n",
    "if mean_iou:\n",
    "    mean_iou = np.clip(mean_iou, 0, 1)\n",
    "    epochs_mean_iou = list(range(1, len(mean_iou) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_mean_iou,\n",
    "            y=mean_iou,\n",
    "            mode='lines+markers',\n",
    "            name='Train Mean IoU'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    if val_mean_iou:\n",
    "        val_mean_iou = np.clip(val_mean_iou, 0, 1)\n",
    "        epochs_val_mean_iou = list(range(1, len(val_mean_iou) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_mean_iou,\n",
    "                y=val_mean_iou,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Mean IoU'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Mean IoU', row=1, col=2, range=[0, 1])\n",
    "\n",
    "# 3) GRÁFICO DE ACCURACY (col=3) -> Limitar valores a [0,1]\n",
    "if accuracy:\n",
    "    accuracy = np.clip(accuracy, 0, 1)\n",
    "    epochs_acc = list(range(1, len(accuracy) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_acc,\n",
    "            y=accuracy,\n",
    "            mode='lines+markers',\n",
    "            name='Train Accuracy'\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "    if val_accuracy:\n",
    "        val_accuracy = np.clip(val_accuracy, 0, 1)\n",
    "        epochs_val_acc = list(range(1, len(val_accuracy) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_acc,\n",
    "                y=val_accuracy,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Accuracy'\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=3)\n",
    "fig.update_yaxes(title_text='Accuracy', row=1, col=3, range=[0, 1])\n",
    "\n",
    "# Ajustes generales de la figura\n",
    "fig.update_layout(\n",
    "    width=1300,\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6693936,
     "sourceId": 10786805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
