{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:37.429123Z",
     "iopub.status.busy": "2025-02-19T22:45:37.428898Z",
     "iopub.status.idle": "2025-02-19T22:45:42.190720Z",
     "shell.execute_reply": "2025-02-19T22:45:42.189734Z",
     "shell.execute_reply.started": "2025-02-19T22:45:37.429101Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Versión de cuDNN:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])\n",
    "print(\"Versión de CUDA:\", tf.sysconfig.get_build_info()[\"cuda_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Versión de TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOSE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:42.192969Z",
     "iopub.status.busy": "2025-02-19T22:45:42.192274Z",
     "iopub.status.idle": "2025-02-19T22:45:42.206732Z",
     "shell.execute_reply": "2025-02-19T22:45:42.205795Z",
     "shell.execute_reply.started": "2025-02-19T22:45:42.192927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_POINTS = 4096  # Puntos por fragmento\n",
    "\n",
    "# Diccionario de mapeo: cada categoría tiene una lista de etiquetas originales.\n",
    "category_mapping = {\n",
    "    0: [43, 38, 58, 29, 41, 42, 44, 39, 55],  # Construction\n",
    "    1: [4, 45, 6, 40, 60, 61, 33, 32, 14],      # Object\n",
    "    2: [7, 22, 9, 26, 11, 21],                   # Road\n",
    "    3: [48, 47, 1, 19, 46, 10, 25],              # Sign\n",
    "    4: [23, 3, 24, 31, 2],                       # Terrain  \n",
    "    5: [51, 50, 5, 18],                          # Drivable Vegetation\n",
    "    6: [28, 27, 62, 52, 16, 30, 59, 17],          # Non Drivable Vegetation\n",
    "    7: [13, 15, 12, 36, 57, 49, 20, 35, 37, 34, 63],  # Vehicle\n",
    "    8: [8, 56, 0, 53, 54],                        # Void\n",
    "}\n",
    "\n",
    "# Generamos un diccionario inverso: etiqueta original -> categoría\n",
    "label_to_category = {\n",
    "    label: cat for cat, labels in category_mapping.items() for label in labels\n",
    "}\n",
    "\n",
    "def map_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    return np.array([label_to_category.get(label, 8) for label in labels], dtype=np.uint8)\n",
    "\n",
    "def load_bin_and_label(bin_path: Path, label_path: Path):\n",
    "    points = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)[:, :3]\n",
    "    labels = np.fromfile(label_path, dtype=np.uint32) & 0xFFFF\n",
    "    labels = map_labels(labels)\n",
    "    return points, labels\n",
    "\n",
    "def process_file(bin_path: Path, label_path: Path):\n",
    "    points, labels = load_bin_and_label(bin_path, label_path)\n",
    "    num_points = points.shape[0]\n",
    "    n_chunks = num_points // NUM_POINTS  # Número de fragmentos completos\n",
    "    if n_chunks == 0:\n",
    "        return None, None\n",
    "    # Recortar y redimensionar\n",
    "    points = points[:n_chunks * NUM_POINTS].reshape(n_chunks, NUM_POINTS, 3)\n",
    "    labels = labels[:n_chunks * NUM_POINTS].reshape(n_chunks, NUM_POINTS)\n",
    "    return points, labels\n",
    "\n",
    "def process_path(x_path, y_path):\n",
    "    x_files = sorted(os.listdir(x_path))\n",
    "    y_files = sorted(os.listdir(y_path))\n",
    "\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for x, y in tqdm(zip(x_files, y_files), total=len(x_files), desc=\"Procesando archivos\", unit=\"archivo\"):\n",
    "        points, labels = process_file(os.path.join(x_path, x), os.path.join(y_path, y))\n",
    "        x_data.append(points)\n",
    "        y_data.append(labels)\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:48:25.468327Z",
     "iopub.status.busy": "2025-02-19T22:48:25.468011Z",
     "iopub.status.idle": "2025-02-19T22:57:03.863818Z",
     "shell.execute_reply": "2025-02-19T22:57:03.862746Z",
     "shell.execute_reply.started": "2025-02-19T22:48:25.468304Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 100%|██████████| 961/961 [05:28<00:00,  2.92archivo/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m x_val_path   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/fmartinez/datasets_val/lidar/val\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m y_val_path   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/fmartinez/datasets_val/labels/val\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m x_val, y_val \u001b[38;5;241m=\u001b[39m process_path(x_val_path, y_val_path)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_path = \"/home/fmartinez/datasets/lidar/train\"\n",
    "y_train_path = \"/home/fmartinez/datasets/labels/train\"\n",
    "x_val_path   = \"/home/fmartinez/datasets_val/lidar/val\"\n",
    "y_val_path   = \"/home/fmartinez/datasets_val/labels/val\"\n",
    "\n",
    "x_val, y_val = process_path(x_val_path, y_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx_val\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_val' is not defined"
     ]
    }
   ],
   "source": [
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:02:41.161169Z",
     "iopub.status.busy": "2025-02-19T23:02:41.160826Z",
     "iopub.status.idle": "2025-02-19T23:02:43.101788Z",
     "shell.execute_reply": "2025-02-19T23:02:43.101090Z",
     "shell.execute_reply.started": "2025-02-19T23:02:41.161144Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_permutados_train = np.random.permutation(x_train.shape[0])\n",
    "indices_permutados_val = np.random.permutation(x_val.shape[0])\n",
    "\n",
    "x_train_shuffle = x_train[indices_permutados_train]\n",
    "y_train_shuffle = y_train[indices_permutados_train]\n",
    "x_val_shuffle = x_val[indices_permutados_val]\n",
    "y_val_shuffle = y_val[indices_permutados_val]\n",
    "\n",
    "x_train_shuffle.shape, y_train_shuffle.shape, x_val_shuffle.shape, y_val_shuffle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train) and len(x_val) == len(y_val)\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_train)} nubes de puntos de {y_train.shape[0]} puntos\")\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_val)} nubes de puntos de {y_val.shape[0]} puntos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:07.718183Z",
     "iopub.status.busy": "2025-02-19T23:04:07.717827Z",
     "iopub.status.idle": "2025-02-19T23:04:07.724041Z",
     "shell.execute_reply": "2025-02-19T23:04:07.723077Z",
     "shell.execute_reply.started": "2025-02-19T23:04:07.718155Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_3D(xyz, labels):\n",
    "    \"\"\"\n",
    "    Visualiza una nube de puntos en 3D con Plotly.\n",
    "    - xyz: (num_points, 3) array con coordenadas (X, Y, Z).\n",
    "    - labels: (num_points,) array con etiquetas semánticas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Definir 9 colores predefinidos en formato RGB\n",
    "    predefined_colors = [\n",
    "        \"rgb(255, 0, 0)\",    # Rojo\n",
    "        \"rgb(0, 255, 0)\",    # Verde\n",
    "        \"rgb(0, 0, 255)\",    # Azul\n",
    "        \"rgb(255, 255, 0)\",  # Amarillo\n",
    "        \"rgb(255, 165, 0)\",  # Naranja\n",
    "        \"rgb(128, 0, 128)\",  # Púrpura\n",
    "        \"rgb(0, 255, 255)\",  # Cian\n",
    "        \"rgb(255, 192, 203)\",# Rosa\n",
    "        \"rgb(128, 128, 128)\" # Gris\n",
    "    ]\n",
    "\n",
    "    # Asignar colores según la etiqueta (se asume que las etiquetas van de 0 a 8)\n",
    "    point_colors = [predefined_colors[label % len(predefined_colors)] for label in labels]\n",
    "\n",
    "    # Crear la figura 3D en Plotly\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=xyz[:, 0], y=xyz[:, 1], z=xyz[:, 2],  # Coordenadas X, Y, Z\n",
    "        mode='markers',\n",
    "        marker=dict(size=1, color=point_colors, opacity=0.8)  # Color basado en etiquetas\n",
    "    ))\n",
    "\n",
    "    # Configurar etiquetas y título\n",
    "    fig.update_layout(\n",
    "        title=\"Nube de Puntos con Etiquetas Semánticas\",\n",
    "        scene=dict(xaxis_title=\"X\", yaxis_title=\"Y\", zaxis_title=\"Z\")\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:24.140632Z",
     "iopub.status.busy": "2025-02-19T23:04:24.140280Z",
     "iopub.status.idle": "2025-02-19T23:04:25.355377Z",
     "shell.execute_reply": "2025-02-19T23:04:25.353411Z",
     "shell.execute_reply.started": "2025-02-19T23:04:24.140600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Llamar a la función con la primera nube de puntos\n",
    "plot_3D(x_train_shuffle[1000], y_train_shuffle[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:08.622104Z",
     "iopub.status.busy": "2025-02-19T23:05:08.621750Z",
     "iopub.status.idle": "2025-02-19T23:05:11.936407Z",
     "shell.execute_reply": "2025-02-19T23:05:11.935406Z",
     "shell.execute_reply.started": "2025-02-19T23:05:08.622075Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Contar cuántos puntos hay por clase\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Mostrar distribución de clases\n",
    "print(len(y_train))\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Clase {cls}: {count} puntos\")\n",
    "\n",
    "# Crear gráfico interactivo con Plotly\n",
    "fig = px.bar(x=unique_classes, y=class_counts, labels={'x': 'Clase', 'y': 'Cantidad de puntos'},\n",
    "             title='Distribución de etiquetas en y_train')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:16.216947Z",
     "iopub.status.busy": "2025-02-19T23:05:16.216572Z",
     "iopub.status.idle": "2025-02-19T23:05:16.246341Z",
     "shell.execute_reply": "2025-02-19T23:05:16.245375Z",
     "shell.execute_reply.started": "2025-02-19T23:05:16.216914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular pesos inversamente proporcionales a la frecuencia de cada clase\n",
    "total_samples = len(y_train.flatten())  # Total de puntos\n",
    "class_weights = {cls: total_samples / (len(unique_classes) * count) for cls, count in zip(unique_classes, class_counts)}\n",
    "\n",
    "print(\"Pesos de las clases:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:19.384048Z",
     "iopub.status.busy": "2025-02-19T23:05:19.383654Z",
     "iopub.status.idle": "2025-02-19T23:05:19.396156Z",
     "shell.execute_reply": "2025-02-19T23:05:19.395116Z",
     "shell.execute_reply.started": "2025-02-19T23:05:19.384013Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "MAX_POINTS = 16384\n",
    "\n",
    "def tnet(inputs, num_features):\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(num_features * num_features, kernel_initializer='zeros',\n",
    "                     bias_initializer=tf.keras.initializers.Constant(tf.eye(num_features).numpy().flatten()))(x)\n",
    "    transform_matrix = layers.Reshape((num_features, num_features))(x)\n",
    "\n",
    "    def transform(inputs_and_matrix):\n",
    "        inputs, matrix = inputs_and_matrix\n",
    "        return tf.matmul(inputs, matrix)\n",
    "\n",
    "    transformed_inputs = layers.Lambda(transform)([inputs, transform_matrix])\n",
    "    transformed_inputs = layers.Lambda(lambda t: tf.ensure_shape(t, (None, MAX_POINTS, num_features)))(transformed_inputs)\n",
    "\n",
    "    return transformed_inputs\n",
    "\n",
    "def build_pointnet(num_classes, input_dim=3, max_points=MAX_POINTS):\n",
    "    inputs = tf.keras.Input(shape=(None, input_dim))\n",
    "\n",
    "    x = tnet(inputs, input_dim)\n",
    "\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tnet(x, 64)\n",
    "\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    global_features = layers.GlobalMaxPooling1D()(x)\n",
    "    global_features = layers.Lambda(lambda t: tf.expand_dims(t, axis=1))(global_features)\n",
    "    global_features = layers.Lambda(lambda t: tf.repeat(t, repeats=max_points, axis=1))(global_features)\n",
    "\n",
    "    x = layers.Lambda(lambda t: tf.ensure_shape(t, (None, max_points, 1024)))(x)\n",
    "\n",
    "    x = layers.concatenate([x, global_features], axis=-1)\n",
    "\n",
    "    x = layers.Conv1D(512, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(256, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    outputs = layers.Conv1D(num_classes, 1, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:24.002726Z",
     "iopub.status.busy": "2025-02-19T23:05:24.002339Z",
     "iopub.status.idle": "2025-02-19T23:05:24.009292Z",
     "shell.execute_reply": "2025-02-19T23:05:24.007668Z",
     "shell.execute_reply.started": "2025-02-19T23:05:24.002692Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MeanIoUWrapper(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "        self.metric.update_state(y_true, y_pred_labels)\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n",
    "\n",
    "class MeanIoUWrapper_2(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int32)  # ✅ Asegurar que y_true sea int32\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "\n",
    "        self.metric.update_state(y_true, y_pred_labels, sample_weight)  # ✅ Pasar sample_weight\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:26.333484Z",
     "iopub.status.busy": "2025-02-19T23:05:26.333162Z",
     "iopub.status.idle": "2025-02-19T23:05:27.232814Z",
     "shell.execute_reply": "2025-02-19T23:05:27.231873Z",
     "shell.execute_reply.started": "2025-02-19T23:05:26.333458Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convertir los pesos a tensores\n",
    "class_weight_tensor = tf.constant([class_weights[i] for i in range(len(unique_classes))], dtype=tf.float32)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    \"\"\"Aplica pesos de clases a la pérdida de entropía cruzada\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    sample_weights = tf.gather(class_weight_tensor, y_true)  # Asigna el peso según la clase\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return loss * sample_weights  # Escala la pérdida por el peso de la clase\n",
    "\n",
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Convertir y_true a int32 por seguridad\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "        # Aplicar softmax si aún no se ha aplicado\n",
    "        y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "\n",
    "        # Extraer las probabilidades correctas usando one-hot encoding\n",
    "        y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])  # (batch, puntos, clases)\n",
    "        y_pred = tf.reduce_sum(y_pred * y_true_one_hot, axis=-1)  # (batch, puntos)\n",
    "\n",
    "        # Evitar log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        # Calcular la Focal Loss\n",
    "        ce = -tf.math.log(y_pred)\n",
    "        weight = alpha * tf.pow(1 - y_pred, gamma)\n",
    "        \n",
    "        return tf.reduce_mean(weight * ce)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:46.005650Z",
     "iopub.status.busy": "2025-02-19T23:05:46.005327Z",
     "iopub.status.idle": "2025-02-19T23:05:46.259051Z",
     "shell.execute_reply": "2025-02-19T23:05:46.257938Z",
     "shell.execute_reply.started": "2025-02-19T23:05:46.005625Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 3\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "# Definir el modelo PointNet\n",
    "model = build_pointnet(num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n",
    "mean_iou_wrapper = MeanIoUWrapper(num_classes=NUM_CLASSES)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.0003)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"accuracy\", mean_iou_wrapper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.259Z",
     "iopub.execute_input": "2025-02-19T23:10:17.372591Z",
     "iopub.status.busy": "2025-02-19T23:10:17.372234Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train_shuffle,  \n",
    "    y_train_shuffle,  \n",
    "    validation_data=(x_val_shuffle, y_val_shuffle), \n",
    "    epochs=60,\n",
    "    batch_size = 16,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"pointnet_goose_16k_3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Supongamos que estos vienen de tu modelo entrenado (Keras history)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss')\n",
    "mean_iou = history.history.get('mean_iou_wrapper')\n",
    "val_mean_iou = history.history.get('val_mean_iou_wrapper')\n",
    "accuracy = history.history.get('accuracy')\n",
    "val_accuracy = history.history.get('val_accuracy')\n",
    "\n",
    "# Crear figura con 1 fila y 3 columnas de subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"Loss Curve\", \"Mean IoU Curve\", \"Accuracy Curve\")\n",
    ")\n",
    "\n",
    "# 1) GRÁFICO DE PÉRDIDA (col=1)\n",
    "epochs_loss = list(range(1, len(loss) + 1))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_loss,\n",
    "        y=loss,\n",
    "        mode='lines+markers',\n",
    "        name='Train Loss'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "if val_loss:\n",
    "    epochs_val_loss = list(range(1, len(val_loss) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_val_loss,\n",
    "            y=val_loss,\n",
    "            mode='lines+markers',\n",
    "            name='Validation Loss'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1, range=[0, 10])  # Rango ajustado\n",
    "\n",
    "# 2) GRÁFICO DE Mean IoU (col=2) -> Limitar valores a [0,1]\n",
    "if mean_iou:\n",
    "    mean_iou = np.clip(mean_iou, 0, 1)\n",
    "    epochs_mean_iou = list(range(1, len(mean_iou) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_mean_iou,\n",
    "            y=mean_iou,\n",
    "            mode='lines+markers',\n",
    "            name='Train Mean IoU'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    if val_mean_iou:\n",
    "        val_mean_iou = np.clip(val_mean_iou, 0, 1)\n",
    "        epochs_val_mean_iou = list(range(1, len(val_mean_iou) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_mean_iou,\n",
    "                y=val_mean_iou,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Mean IoU'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Mean IoU', row=1, col=2, range=[0, 1])\n",
    "\n",
    "# 3) GRÁFICO DE ACCURACY (col=3) -> Limitar valores a [0,1]\n",
    "if accuracy:\n",
    "    accuracy = np.clip(accuracy, 0, 1)\n",
    "    epochs_acc = list(range(1, len(accuracy) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_acc,\n",
    "            y=accuracy,\n",
    "            mode='lines+markers',\n",
    "            name='Train Accuracy'\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "    if val_accuracy:\n",
    "        val_accuracy = np.clip(val_accuracy, 0, 1)\n",
    "        epochs_val_acc = list(range(1, len(val_accuracy) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_acc,\n",
    "                y=val_accuracy,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Accuracy'\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=3)\n",
    "fig.update_yaxes(title_text='Accuracy', row=1, col=3, range=[0, 1])\n",
    "\n",
    "# Ajustes generales de la figura\n",
    "fig.update_layout(\n",
    "    width=1300,\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6693936,
     "sourceId": 10786805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
