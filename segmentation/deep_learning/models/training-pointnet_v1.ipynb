{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 17:48:59.910218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-12 17:49:00.542857: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/fmartinez/cuda/cudnn/lib64:/usr/local/cuda/lib64:\n",
      "2025-06-12 17:49:00.542915: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/fmartinez/cuda/cudnn/lib64:/usr/local/cuda/lib64:\n",
      "2025-06-12 17:49:00.542919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Usa solo la GPU 1\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:37.429123Z",
     "iopub.status.busy": "2025-02-19T22:45:37.428898Z",
     "iopub.status.idle": "2025-02-19T22:45:42.190720Z",
     "shell.execute_reply": "2025-02-19T22:45:42.189734Z",
     "shell.execute_reply.started": "2025-02-19T22:45:37.429101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de cuDNN: 8\n",
      "Versión de CUDA: 11.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Versión de cuDNN:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])\n",
    "print(\"Versión de CUDA:\", tf.sysconfig.get_build_info()[\"cuda_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de TensorFlow: 2.11.0\n",
      "GPUs disponibles: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Versión de TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOSE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:42.192969Z",
     "iopub.status.busy": "2025-02-19T22:45:42.192274Z",
     "iopub.status.idle": "2025-02-19T22:45:42.206732Z",
     "shell.execute_reply": "2025-02-19T22:45:42.205795Z",
     "shell.execute_reply.started": "2025-02-19T22:45:42.192927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_POINTS = 4096 # 256, 512, 1024, 2048, 4096, 8192, 16384, 32768\n",
    "\n",
    "category_mapping = {\n",
    "    0: [43, 38, 58, 29, 41, 42, 44, 39, 55], # Construction\n",
    "    1: [4, 45, 6, 40, 60, 61, 33, 32, 14], # Object\n",
    "    2: [7, 22, 9, 26, 11, 21], # Road\n",
    "    3: [48, 47, 1, 19, 46, 10, 25], # Sign\n",
    "    4: [23, 3, 24, 31, 2], # Terrain  \n",
    "    5: [51, 50, 5, 18], # Drivable Vegetation\n",
    "    6: [28, 27, 62, 52, 16, 30, 59, 17], # Non Drivable Vegetation\n",
    "    7: [13, 15, 12, 36, 57, 49, 20, 35, 37, 34, 63], # Vehicle\n",
    "    8: [8, 56, 0, 53, 54], # Void\n",
    "}\n",
    "\n",
    "def filter_points_within_radius(points: np.ndarray, radius: float = 25.0) -> np.ndarray:\n",
    "    distances = np.linalg.norm(points, axis=1) \n",
    "    mask = distances <= radius  \n",
    "    return points[mask]  \n",
    "\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def load_bin_file(bin_path: str, num_points: int = NUM_POINTS, radius: float = 25.0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Cargar la nube de puntos\n",
    "    points = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)[:, :3] \n",
    "\n",
    "    # Filtrar por distancia\n",
    "    distances = np.linalg.norm(points, axis=1)\n",
    "    mask = distances <= radius\n",
    "    points = points[mask]\n",
    "\n",
    "    num_available = points.shape[0]\n",
    "\n",
    "    if num_available >= num_points:\n",
    "        # Seleccionar un punto aleatorio\n",
    "        center_idx = np.random.randint(0, num_available)\n",
    "\n",
    "        # Calcular inicio y fin tratando de centrar la ventana\n",
    "        half_span = num_points // 2\n",
    "        start_idx = max(0, center_idx - half_span)\n",
    "        end_idx = start_idx + num_points\n",
    "\n",
    "        # Si nos pasamos del límite superior, compensar hacia atrás\n",
    "        if end_idx > num_available:\n",
    "            end_idx = num_available\n",
    "            start_idx = max(0, end_idx - num_points)\n",
    "\n",
    "        # Si nos pasamos del límite inferior, compensar hacia adelante\n",
    "        if start_idx == 0:\n",
    "            end_idx = min(num_available, num_points)\n",
    "\n",
    "        indices = np.arange(start_idx, end_idx)\n",
    "        return points[indices], np.where(mask)[0][indices]\n",
    "\n",
    "    else:\n",
    "        # Si hay menos de num_points disponibles, devolver todo lo que haya\n",
    "        return points, np.where(mask)[0]\n",
    "\n",
    "\n",
    "label_to_category = {label: cat for cat, labels in category_mapping.items() for label in labels}\n",
    "\n",
    "def map_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    return np.array([label_to_category.get(label, 8) for label in labels], dtype=np.uint8)\n",
    "\n",
    "def load_label_file(label_path: str, indices: np.ndarray) -> np.ndarray:\n",
    "    labels = np.fromfile(label_path, dtype=np.uint32) & 0xFFFF \n",
    "    return map_labels(labels[indices]) \n",
    "\n",
    "def load_dataset(bin_files: List[str], label_files: List[str], num_points: int = NUM_POINTS) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    x_data, y_data = [], []\n",
    "    \n",
    "    for bin_f, label_f in tqdm(zip(bin_files, label_files), total=len(bin_files), desc=\"Cargando datos\"):\n",
    "        points, indices = load_bin_file(bin_f, num_points)\n",
    "        \n",
    "        if points.shape[0] < num_points:\n",
    "            continue  \n",
    "\n",
    "        labels = load_label_file(label_f, indices)\n",
    "\n",
    "        x_data.append(points)\n",
    "        y_data.append(labels)\n",
    "\n",
    "    return np.array(x_data, dtype=np.float32), np.array(y_data, dtype=np.uint8)\n",
    "\n",
    "def get_file_paths(data_dir: str) -> List[str]:\n",
    "    return sorted([str(f) for f in Path(data_dir).glob(\"*.*\")])\n",
    "\n",
    "def load_all_data(x_train_dir: str, y_train_dir: str, x_val_dir: str, y_val_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    x_train_files = get_file_paths(x_train_dir)\n",
    "    y_train_files = get_file_paths(y_train_dir)\n",
    "    x_val_files = get_file_paths(x_val_dir)\n",
    "    y_val_files = get_file_paths(y_val_dir)\n",
    "    \n",
    "    assert len(x_train_files) == len(y_train_files), \"Número de archivos x_train y y_train no coincide.\"\n",
    "    assert len(x_val_files) == len(y_val_files), \"Número de archivos x_val y y_val no coincide.\"\n",
    "    \n",
    "    print(\"Cargando datos de entrenamiento...\")\n",
    "    x_train, y_train = load_dataset(x_train_files, y_train_files)\n",
    "    \n",
    "    print(\"Cargando datos de validación...\")\n",
    "    x_val, y_val = load_dataset(x_val_files, y_val_files)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "class PointCloudGenerator(Sequence):\n",
    "    def __init__(self, bin_files, label_files, num_points, mode='train', batch_size=32):\n",
    "        self.bin_files = bin_files\n",
    "        self.label_files = label_files\n",
    "        self.num_points = num_points\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode  # 'train' o 'val'\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Número total de batches por epoch.\"\"\"\n",
    "        return len(self.bin_files) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Genera un batch de datos.\"\"\"\n",
    "        batch_bin_files = self.bin_files[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_label_files = self.label_files[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "\n",
    "        x_batch, y_batch = [], []\n",
    "\n",
    "        for bin_file, label_file in zip(batch_bin_files, batch_label_files):\n",
    "            points, indices = load_bin_file(bin_file, self.num_points)\n",
    "            labels = load_label_file(label_file, indices)\n",
    "\n",
    "            if self.mode == 'train':\n",
    "                sub_points, sub_labels = self._get_train_sample(points, labels)\n",
    "                x_batch.append(sub_points)\n",
    "                y_batch.append(sub_labels)\n",
    "            else:\n",
    "                sub_points_list, sub_labels_list = self._get_val_samples(points, labels)\n",
    "                x_batch.extend(sub_points_list)  # Agrega cada subnube individualmente\n",
    "                y_batch.extend(sub_labels_list)\n",
    "\n",
    "        return np.array(x_batch, dtype=np.float32), np.array(y_batch, dtype=np.uint8)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Mezcla los datos al final de cada epoch.\"\"\"\n",
    "        temp = list(zip(self.bin_files, self.label_files))\n",
    "        random.shuffle(temp)\n",
    "        self.bin_files, self.label_files = zip(*temp)\n",
    "\n",
    "    def _get_train_sample(self, points, labels):\n",
    "        \"\"\"Obtiene una subnube de entrenamiento con N puntos.\"\"\"\n",
    "        center_idx = np.random.randint(len(points))\n",
    "        tree = cKDTree(points)\n",
    "        _, neighbor_indices = tree.query(points[center_idx], k=self.num_points)\n",
    "        return points[neighbor_indices], labels[neighbor_indices]\n",
    "\n",
    "    def _get_val_samples(self, points, labels):\n",
    "        \"\"\"Divide la nube en subnubes de N puntos para validación.\"\"\"\n",
    "        num_full_batches = len(points) // self.num_points\n",
    "        indices = np.random.choice(len(points), num_full_batches * self.num_points, replace=False)\n",
    "        subclouds = points[indices].reshape(num_full_batches, self.num_points, 3)\n",
    "        sublabels = labels[indices].reshape(num_full_batches, self.num_points)\n",
    "        \n",
    "        # Convertir de (num_subclouds, num_points, 3) a lista de subnubes individuales\n",
    "        return [subclouds[i] for i in range(num_full_batches)], [sublabels[i] for i in range(num_full_batches)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_path = \"/home/fmartinez/datasets/goose/lidar/train\"\n",
    "y_train_path = \"/home/fmartinez/datasets/goose/labels/train\"\n",
    "\n",
    "x_val_path = \"/home/fmartinez/datasets/goose/lidar/val\"\n",
    "y_val_path = \"/home/fmartinez/datasets/goose/labels/val\"\n",
    "\n",
    "x_test_path = \"/home/fmartinez/datasets_val/lidar/val\"\n",
    "y_test_path = \"/home/fmartinez/datasets_val/labels/val\"\n",
    "\n",
    "x_train_files = get_file_paths(x_train_path)\n",
    "y_train_files = get_file_paths(y_train_path)\n",
    "x_val_files = get_file_paths(x_val_path)\n",
    "y_val_files = get_file_paths(y_val_path)\n",
    "x_test_files = get_file_paths(x_test_path)\n",
    "y_test_files = get_file_paths(y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = PointCloudGenerator(x_train_files, y_train_files, num_points=16384, mode='train', batch_size=16)\n",
    "val_generator = PointCloudGenerator(x_val_files, y_val_files, num_points=16384, mode='val', batch_size=16)\n",
    "test_generator = PointCloudGenerator(x_test_files, y_test_files, num_points=16384, mode='val', batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 1.3756800728474192, 1: 11.540744634253176, 2: 14.010762980452215, 3: 18.564527493588116, 4: 1.0714398416315722, 5: 0.4806517192715152, 6: 0.20640943202058679, 7: 6.540442855724357, 8: 20.108312727050706}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:19.384048Z",
     "iopub.status.busy": "2025-02-19T23:05:19.383654Z",
     "iopub.status.idle": "2025-02-19T23:05:19.396156Z",
     "shell.execute_reply": "2025-02-19T23:05:19.395116Z",
     "shell.execute_reply.started": "2025-02-19T23:05:19.384013Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "MAX_POINTS = 16384\n",
    "\n",
    "def tnet(inputs, num_features):\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(num_features * num_features, kernel_initializer='zeros',\n",
    "                     bias_initializer=tf.keras.initializers.Constant(tf.eye(num_features).numpy().flatten()))(x)\n",
    "    transform_matrix = layers.Reshape((num_features, num_features))(x)\n",
    "\n",
    "    def transform(inputs_and_matrix):\n",
    "        inputs, matrix = inputs_and_matrix\n",
    "        return tf.matmul(inputs, matrix)\n",
    "\n",
    "    transformed_inputs = layers.Lambda(transform)([inputs, transform_matrix])\n",
    "    transformed_inputs = layers.Lambda(lambda t: tf.ensure_shape(t, (None, MAX_POINTS, num_features)))(transformed_inputs)\n",
    "\n",
    "    return transformed_inputs\n",
    "\n",
    "def build_pointnet(num_classes, input_dim=3, max_points=MAX_POINTS):\n",
    "    inputs = tf.keras.Input(shape=(None, input_dim))\n",
    "\n",
    "    x = tnet(inputs, input_dim)\n",
    "\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tnet(x, 64)\n",
    "\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    global_features = layers.GlobalMaxPooling1D()(x)\n",
    "    global_features = layers.Lambda(lambda t: tf.expand_dims(t, axis=1))(global_features)\n",
    "    global_features = layers.Lambda(lambda t: tf.repeat(t, repeats=max_points, axis=1))(global_features)\n",
    "\n",
    "    x = layers.Lambda(lambda t: tf.ensure_shape(t, (None, max_points, 1024)))(x)\n",
    "\n",
    "    x = layers.concatenate([x, global_features], axis=-1)\n",
    "\n",
    "    x = layers.Conv1D(512, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(256, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    outputs = layers.Conv1D(num_classes, 1, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:24.002726Z",
     "iopub.status.busy": "2025-02-19T23:05:24.002339Z",
     "iopub.status.idle": "2025-02-19T23:05:24.009292Z",
     "shell.execute_reply": "2025-02-19T23:05:24.007668Z",
     "shell.execute_reply.started": "2025-02-19T23:05:24.002692Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MeanIoUWrapper(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "        self.metric.update_state(y_true, y_pred_labels)\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n",
    "\n",
    "class MeanIoUWrapper_2(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int32)  # Asegurar que y_true sea int32\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "\n",
    "        self.metric.update_state(y_true, y_pred_labels, sample_weight)  # Pasar sample_weight\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:26.333484Z",
     "iopub.status.busy": "2025-02-19T23:05:26.333162Z",
     "iopub.status.idle": "2025-02-19T23:05:27.232814Z",
     "shell.execute_reply": "2025-02-19T23:05:27.231873Z",
     "shell.execute_reply.started": "2025-02-19T23:05:26.333458Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convertir los pesos a tensores\n",
    "class_weight_tensor = tf.constant([class_weights[i] for i in range(9)], dtype=tf.float32)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    sample_weights = tf.gather(class_weight_tensor, y_true)  # Asigna el peso según la clase\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return loss * sample_weights  # Escala la pérdida por el peso de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:46.005650Z",
     "iopub.status.busy": "2025-02-19T23:05:46.005327Z",
     "iopub.status.idle": "2025-02-19T23:05:46.259051Z",
     "shell.execute_reply": "2025-02-19T23:05:46.257938Z",
     "shell.execute_reply.started": "2025-02-19T23:05:46.005625Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 3\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "# Definir el modelo PointNet\n",
    "model = build_pointnet(num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n",
    "mean_iou_wrapper = MeanIoUWrapper(num_classes=NUM_CLASSES)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"accuracy\", mean_iou_wrapper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.259Z",
     "iopub.execute_input": "2025-02-19T23:10:17.372591Z",
     "iopub.status.busy": "2025-02-19T23:10:17.372234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 75/422 [====>.........................] - ETA: 3:50 - loss: 2.5194 - accuracy: 0.1591 - mean_iou_wrapper: 0.0537"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 17:54:16.648515: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: IndexError: index 11446 is out of bounds for axis 0 with size 11446\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1039, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/home/fmartinez/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 901, in wrapped_generator\n",
      "    for data in generator_fn():\n",
      "\n",
      "  File \"/home/fmartinez/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1048, in generator_fn\n",
      "    yield x[i]\n",
      "\n",
      "  File \"/tmp/ipykernel_1113031/3322937290.py\", line 31, in __getitem__\n",
      "    sub_points, sub_labels = self._get_train_sample(points, labels)\n",
      "\n",
      "  File \"/tmp/ipykernel_1113031/3322937290.py\", line 52, in _get_train_sample\n",
      "    return points[neighbor_indices], labels[neighbor_indices]\n",
      "\n",
      "IndexError: index 11446 is out of bounds for axis 0 with size 11446\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  IndexError: index 11446 is out of bounds for axis 0 with size 11446\nTraceback (most recent call last):\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1039, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 901, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1048, in generator_fn\n    yield x[i]\n\n  File \"/tmp/ipykernel_1113031/3322937290.py\", line 31, in __getitem__\n    sub_points, sub_labels = self._get_train_sample(points, labels)\n\n  File \"/tmp/ipykernel_1113031/3322937290.py\", line 52, in _get_train_sample\n    return points[neighbor_indices], labels[neighbor_indices]\n\nIndexError: index 11446 is out of bounds for axis 0 with size 11446\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[gradient_tape/model_1/lambda_7/MatMul/Shape/_6]]\n  (1) UNKNOWN:  IndexError: index 11446 is out of bounds for axis 0 with size 11446\nTraceback (most recent call last):\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1039, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 901, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1048, in generator_fn\n    yield x[i]\n\n  File \"/tmp/ipykernel_1113031/3322937290.py\", line 31, in __getitem__\n    sub_points, sub_labels = self._get_train_sample(points, labels)\n\n  File \"/tmp/ipykernel_1113031/3322937290.py\", line 52, in _get_train_sample\n    return points[neighbor_indices], labels[neighbor_indices]\n\nIndexError: index 11446 is out of bounds for axis 0 with size 11446\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_22101]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  IndexError: index 11446 is out of bounds for axis 0 with size 11446\nTraceback (most recent call last):\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1039, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 901, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1048, in generator_fn\n    yield x[i]\n\n  File \"/tmp/ipykernel_1113031/3322937290.py\", line 31, in __getitem__\n    sub_points, sub_labels = self._get_train_sample(points, labels)\n\n  File \"/tmp/ipykernel_1113031/3322937290.py\", line 52, in _get_train_sample\n    return points[neighbor_indices], labels[neighbor_indices]\n\nIndexError: index 11446 is out of bounds for axis 0 with size 11446\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[gradient_tape/model_1/lambda_7/MatMul/Shape/_6]]\n  (1) UNKNOWN:  IndexError: index 11446 is out of bounds for axis 0 with size 11446\nTraceback (most recent call last):\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1039, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 901, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/home/fmartinez/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py\", line 1048, in generator_fn\n    yield x[i]\n\n  File \"/tmp/ipykernel_1113031/3322937290.py\", line 31, in __getitem__\n    sub_points, sub_labels = self._get_train_sample(points, labels)\n\n  File \"/tmp/ipykernel_1113031/3322937290.py\", line 52, in _get_train_sample\n    return points[neighbor_indices], labels[neighbor_indices]\n\nIndexError: index 11446 is out of bounds for axis 0 with size 11446\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_22101]"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=30,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"pointnet_goose_16k_3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Supongamos que estos vienen de tu modelo entrenado (Keras history)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss')\n",
    "mean_iou = history.history.get('mean_iou_wrapper')\n",
    "val_mean_iou = history.history.get('val_mean_iou_wrapper')\n",
    "accuracy = history.history.get('accuracy')\n",
    "val_accuracy = history.history.get('val_accuracy')\n",
    "\n",
    "# Crear figura con 1 fila y 3 columnas de subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"Loss Curve\", \"Mean IoU Curve\", \"Accuracy Curve\")\n",
    ")\n",
    "\n",
    "# 1) GRÁFICO DE PÉRDIDA (col=1)\n",
    "epochs_loss = list(range(1, len(loss) + 1))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_loss,\n",
    "        y=loss,\n",
    "        mode='lines+markers',\n",
    "        name='Train Loss'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "if val_loss:\n",
    "    epochs_val_loss = list(range(1, len(val_loss) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_val_loss,\n",
    "            y=val_loss,\n",
    "            mode='lines+markers',\n",
    "            name='Validation Loss'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1, range=[0, 10])  # Rango ajustado\n",
    "\n",
    "# 2) GRÁFICO DE Mean IoU (col=2) -> Limitar valores a [0,1]\n",
    "if mean_iou:\n",
    "    mean_iou = np.clip(mean_iou, 0, 1)\n",
    "    epochs_mean_iou = list(range(1, len(mean_iou) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_mean_iou,\n",
    "            y=mean_iou,\n",
    "            mode='lines+markers',\n",
    "            name='Train Mean IoU'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    if val_mean_iou:\n",
    "        val_mean_iou = np.clip(val_mean_iou, 0, 1)\n",
    "        epochs_val_mean_iou = list(range(1, len(val_mean_iou) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_mean_iou,\n",
    "                y=val_mean_iou,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Mean IoU'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Mean IoU', row=1, col=2, range=[0, 1])\n",
    "\n",
    "# 3) GRÁFICO DE ACCURACY (col=3) -> Limitar valores a [0,1]\n",
    "if accuracy:\n",
    "    accuracy = np.clip(accuracy, 0, 1)\n",
    "    epochs_acc = list(range(1, len(accuracy) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_acc,\n",
    "            y=accuracy,\n",
    "            mode='lines+markers',\n",
    "            name='Train Accuracy'\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "    if val_accuracy:\n",
    "        val_accuracy = np.clip(val_accuracy, 0, 1)\n",
    "        epochs_val_acc = list(range(1, len(val_accuracy) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_acc,\n",
    "                y=val_accuracy,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Accuracy'\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=3)\n",
    "fig.update_yaxes(title_text='Accuracy', row=1, col=3, range=[0, 1])\n",
    "\n",
    "# Ajustes generales de la figura\n",
    "fig.update_layout(\n",
    "    width=1300,\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6693936,
     "sourceId": 10786805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
