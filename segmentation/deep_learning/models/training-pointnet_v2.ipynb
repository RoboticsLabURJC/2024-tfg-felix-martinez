{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:37.429123Z",
     "iopub.status.busy": "2025-02-19T22:45:37.428898Z",
     "iopub.status.idle": "2025-02-19T22:45:42.190720Z",
     "shell.execute_reply": "2025-02-19T22:45:42.189734Z",
     "shell.execute_reply.started": "2025-02-19T22:45:37.429101Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Versión de cuDNN:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])\n",
    "print(\"Versión de CUDA:\", tf.sysconfig.get_build_info()[\"cuda_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Versión de TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOSE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:42.192969Z",
     "iopub.status.busy": "2025-02-19T22:45:42.192274Z",
     "iopub.status.idle": "2025-02-19T22:45:42.206732Z",
     "shell.execute_reply": "2025-02-19T22:45:42.205795Z",
     "shell.execute_reply.started": "2025-02-19T22:45:42.192927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_bin_file(bin_path: str, num_points: int = 4098) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    points = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)[:, :3]\n",
    "    if points.shape[0] > num_points:\n",
    "        indices = np.random.choice(points.shape[0], num_points, replace=False)\n",
    "        return points[indices], indices\n",
    "    elif points.shape[0] < num_points:\n",
    "        pad = np.zeros((num_points - points.shape[0], 3), dtype=np.float32)\n",
    "        return np.vstack((points, pad)), np.arange(points.shape[0])  # No padding en índices\n",
    "    return points, np.arange(num_points)\n",
    "\n",
    "# GOOSE Categories mejoradas\n",
    "category_mapping = {\n",
    "    0: [43, 38, 58, 29, 41, 42, 44, 39, 55], # Construction\n",
    "    1: [4, 45, 6, 40, 60, 61, 33, 32, 14], # Object\n",
    "    2: [7, 22, 9, 26, 11, 21], # Road\n",
    "    3: [48, 47, 1, 19, 46, 10, 25], # Sign\n",
    "    4: [23, 3, 24, 31, 2], # Terrain  \n",
    "    5: [51, 50, 5, 18], # Drivable Vegetation\n",
    "    6: [28, 27, 62, 52, 16, 30, 59, 17], # Non Drivable Vegetation\n",
    "    7: [13, 15, 12, 36, 57, 49, 20, 35, 37, 34, 63], # Vehicle\n",
    "    8: [8, 56, 0, 53, 54], # Void\n",
    "}\n",
    "\n",
    "# Reverse mapping for fast lookup\n",
    "label_to_category = {label: cat for cat, labels in category_mapping.items() for label in labels}\n",
    "\n",
    "def map_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mapea etiquetas al sistema de categorías definido en category_mapping.\n",
    "    \"\"\"\n",
    "    return np.array([label_to_category.get(label, 8) for label in labels], dtype=np.uint8)\n",
    "\n",
    "def load_label_file(label_path: str, indices: np.ndarray, num_points: int = 16384) -> np.ndarray:\n",
    "    labels = np.fromfile(label_path, dtype=np.uint32) & 0xFFFF\n",
    "    if labels.shape[0] > num_points:\n",
    "        labels = labels[indices]  # Usar los mismos índices de los puntos\n",
    "    elif labels.shape[0] < num_points:\n",
    "        pad = np.full(num_points - labels.shape[0], 8, dtype=np.uint16)\n",
    "        labels = np.hstack((labels, pad))\n",
    "    return map_labels(labels)\n",
    "\n",
    "def load_dataset(bin_files: List[str], label_files: List[str], num_points: int = 16384) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    x_data, y_data = [], []\n",
    "    \n",
    "    for bin_f, label_f in tqdm(zip(bin_files, label_files), total=len(bin_files), desc=\"Cargando datos\"):\n",
    "        points, indices = load_bin_file(bin_f, num_points)\n",
    "        labels = load_label_file(label_f, indices, num_points)\n",
    "        x_data.append(points)\n",
    "        y_data.append(labels)\n",
    "    \n",
    "    return np.array(x_data, dtype=np.float32), np.array(y_data, dtype=np.uint8)\n",
    "\n",
    "def get_file_paths(data_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Obtiene lista de archivos en un directorio.\n",
    "    \"\"\"\n",
    "    return sorted([str(f) for f in Path(data_dir).glob(\"*.*\")])\n",
    "\n",
    "def load_all_data(x_train_dir: str, y_train_dir: str, x_val_dir: str, y_val_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga todos los datos de entrenamiento y validación con barra de progreso en una sola línea por conjunto de datos.\n",
    "    \"\"\"\n",
    "    x_train_files = get_file_paths(x_train_dir)\n",
    "    y_train_files = get_file_paths(y_train_dir)\n",
    "    x_val_files = get_file_paths(x_val_dir)\n",
    "    y_val_files = get_file_paths(y_val_dir)\n",
    "    \n",
    "    assert len(x_train_files) == len(y_train_files), \"Número de archivos x_train y y_train no coincide.\"\n",
    "    assert len(x_val_files) == len(y_val_files), \"Número de archivos x_val y y_val no coincide.\"\n",
    "    \n",
    "    print(\"Cargando datos de entrenamiento...\")\n",
    "    x_train, y_train = load_dataset(x_train_files, y_train_files)\n",
    "    \n",
    "    print(\"Cargando datos de validación...\")\n",
    "    x_val, y_val = load_dataset(x_val_files, y_val_files)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:48:25.468327Z",
     "iopub.status.busy": "2025-02-19T22:48:25.468011Z",
     "iopub.status.idle": "2025-02-19T22:57:03.863818Z",
     "shell.execute_reply": "2025-02-19T22:57:03.862746Z",
     "shell.execute_reply.started": "2025-02-19T22:48:25.468304Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_path = \"/home/fmartinez/datasets/lidar/train\"\n",
    "y_train_path = \"/home/fmartinez/datasets/labels/train\"\n",
    "x_val_path = \"/home/fmartinez/datasets_val/lidar/val\"\n",
    "y_val_path = \"/home/fmartinez/datasets_val/labels/val\"\n",
    "\n",
    "x_train, y_train, x_val, y_val = load_all_data(x_train_path, y_train_path, x_val_path, y_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:02:41.161169Z",
     "iopub.status.busy": "2025-02-19T23:02:41.160826Z",
     "iopub.status.idle": "2025-02-19T23:02:43.101788Z",
     "shell.execute_reply": "2025-02-19T23:02:43.101090Z",
     "shell.execute_reply.started": "2025-02-19T23:02:41.161144Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_permutados_train = np.random.permutation(x_train.shape[0])\n",
    "indices_permutados_val = np.random.permutation(x_val.shape[0])\n",
    "\n",
    "x_train_shuffle = x_train[indices_permutados_train]\n",
    "y_train_shuffle = y_train[indices_permutados_train]\n",
    "x_val_shuffle = x_val[indices_permutados_val]\n",
    "y_val_shuffle = y_val[indices_permutados_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train) and len(x_val) == len(y_val)\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_train)} nubes de puntos de {y_train.shape[0]} puntos\")\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_val)} nubes de puntos de {y_val.shape[0]} puntos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:07.718183Z",
     "iopub.status.busy": "2025-02-19T23:04:07.717827Z",
     "iopub.status.idle": "2025-02-19T23:04:07.724041Z",
     "shell.execute_reply": "2025-02-19T23:04:07.723077Z",
     "shell.execute_reply.started": "2025-02-19T23:04:07.718155Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_3D(xyz, labels):\n",
    "    \"\"\"\n",
    "    Visualiza una nube de puntos en 3D con Plotly.\n",
    "    - xyz: (num_points, 3) array con coordenadas (X, Y, Z).\n",
    "    - labels: (num_points,) array con etiquetas semánticas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Definir 9 colores predefinidos en formato RGB\n",
    "    predefined_colors = [\n",
    "        \"rgb(255, 0, 0)\",    # Rojo\n",
    "        \"rgb(0, 255, 0)\",    # Verde\n",
    "        \"rgb(0, 0, 255)\",    # Azul\n",
    "        \"rgb(255, 255, 0)\",  # Amarillo\n",
    "        \"rgb(255, 165, 0)\",  # Naranja\n",
    "        \"rgb(128, 0, 128)\",  # Púrpura\n",
    "        \"rgb(0, 255, 255)\",  # Cian\n",
    "        \"rgb(255, 192, 203)\",# Rosa\n",
    "        \"rgb(128, 128, 128)\" # Gris\n",
    "    ]\n",
    "\n",
    "    # Asignar colores según la etiqueta (se asume que las etiquetas van de 0 a 8)\n",
    "    point_colors = [predefined_colors[label % len(predefined_colors)] for label in labels]\n",
    "\n",
    "    # Crear la figura 3D en Plotly\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=xyz[:, 0], y=xyz[:, 1], z=xyz[:, 2],  # Coordenadas X, Y, Z\n",
    "        mode='markers',\n",
    "        marker=dict(size=1, color=point_colors, opacity=0.8)  # Color basado en etiquetas\n",
    "    ))\n",
    "\n",
    "    # Configurar etiquetas y título\n",
    "    fig.update_layout(\n",
    "        title=\"Nube de Puntos con Etiquetas Semánticas\",\n",
    "        scene=dict(xaxis_title=\"X\", yaxis_title=\"Y\", zaxis_title=\"Z\")\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:24.140632Z",
     "iopub.status.busy": "2025-02-19T23:04:24.140280Z",
     "iopub.status.idle": "2025-02-19T23:04:25.355377Z",
     "shell.execute_reply": "2025-02-19T23:04:25.353411Z",
     "shell.execute_reply.started": "2025-02-19T23:04:24.140600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Llamar a la función con la primera nube de puntos\n",
    "plot_3D(x_train_shuffle[5310], y_train_shuffle[5310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:08.622104Z",
     "iopub.status.busy": "2025-02-19T23:05:08.621750Z",
     "iopub.status.idle": "2025-02-19T23:05:11.936407Z",
     "shell.execute_reply": "2025-02-19T23:05:11.935406Z",
     "shell.execute_reply.started": "2025-02-19T23:05:08.622075Z"
    }
   },
   "outputs": [],
   "source": [
    "# Contar cuántos puntos hay por clase\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print(len(y_train))\n",
    "\n",
    "# Mostrar distribución de clases))\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Clase {cls}: {count} puntos\")\n",
    "\n",
    "# Opcional: visualizar la distribución con un gráfico de barras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(unique_classes, class_counts)\n",
    "plt.xlabel(\"Clase\")\n",
    "plt.ylabel(\"Cantidad de puntos\")\n",
    "plt.title(\"Distribución de etiquetas en y_train\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:16.216947Z",
     "iopub.status.busy": "2025-02-19T23:05:16.216572Z",
     "iopub.status.idle": "2025-02-19T23:05:16.246341Z",
     "shell.execute_reply": "2025-02-19T23:05:16.245375Z",
     "shell.execute_reply.started": "2025-02-19T23:05:16.216914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular pesos inversamente proporcionales a la frecuencia de cada clase\n",
    "total_samples = len(y_train.flatten())  # Total de puntos\n",
    "class_weights = {cls: total_samples / (len(unique_classes) * count) for cls, count in zip(unique_classes, class_counts)}\n",
    "\n",
    "print(\"Pesos de las clases:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:19.384048Z",
     "iopub.status.busy": "2025-02-19T23:05:19.383654Z",
     "iopub.status.idle": "2025-02-19T23:05:19.396156Z",
     "shell.execute_reply": "2025-02-19T23:05:19.395116Z",
     "shell.execute_reply.started": "2025-02-19T23:05:19.384013Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def tnet(inputs, num_features):\n",
    "    \"\"\"\n",
    "    Implementación de T-Net con regularización para mejorar estabilidad.\n",
    "    \"\"\"\n",
    "    x = layers.Conv1D(64, 1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Matriz de transformación con activación `tanh`\n",
    "    x = layers.Dense(num_features * num_features, kernel_initializer='zeros',\n",
    "                     bias_initializer=tf.keras.initializers.Constant(tf.eye(num_features).numpy().flatten()))(x)\n",
    "    x = layers.Activation('tanh')(x)  # Ayuda a la estabilidad numérica\n",
    "\n",
    "    transform_matrix = layers.Reshape((num_features, num_features))(x)\n",
    "\n",
    "    def transform(inputs_and_matrix):\n",
    "        inputs, matrix = inputs_and_matrix\n",
    "        return tf.matmul(inputs, matrix)\n",
    "\n",
    "    transformed_inputs = layers.Lambda(transform)([inputs, transform_matrix])\n",
    "    return transformed_inputs\n",
    "\n",
    "\n",
    "def build_pointnet(num_classes, input_dim=3, max_points=16384):\n",
    "    inputs = tf.keras.Input(shape=(None, input_dim))  # Entrada: N puntos con D características\n",
    "\n",
    "    # Aplicar T-Net en la entrada\n",
    "    x = tnet(inputs, input_dim)\n",
    "\n",
    "    # MLP inicial\n",
    "    x = layers.Conv1D(64, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Aplicar T-Net después de expandir a 128D\n",
    "    x = tnet(x, 128)\n",
    "\n",
    "    # Continuar con el MLP\n",
    "    x = layers.Conv1D(256, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(512, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(1024, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Extracción de características globales optimizada (sin tile)\n",
    "    global_features = layers.GlobalMaxPooling1D()(x)  # (batch, 1024)\n",
    "    global_features = layers.Dense(512, activation='relu')(global_features)  # Reducimos su dimensión\n",
    "    global_features = layers.BatchNormalization()(global_features)\n",
    "    global_features = layers.Lambda(lambda t: tf.expand_dims(t, axis=1))(global_features)  # (batch, 1, 512)\n",
    "\n",
    "    # Concatenación con las características locales sin usar tile\n",
    "    x = layers.concatenate([x, global_features], axis=-1)  # (batch, N, 1536)\n",
    "\n",
    "    # MLP final con dropout\n",
    "    x = layers.Conv1D(512, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)  # Regularización\n",
    "\n",
    "    x = layers.Conv1D(256, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)  # Regularización\n",
    "\n",
    "    # Capa de salida\n",
    "    outputs = layers.Conv1D(num_classes, 1, activation='softmax')(x)  # Clasificación por punto\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:24.002726Z",
     "iopub.status.busy": "2025-02-19T23:05:24.002339Z",
     "iopub.status.idle": "2025-02-19T23:05:24.009292Z",
     "shell.execute_reply": "2025-02-19T23:05:24.007668Z",
     "shell.execute_reply.started": "2025-02-19T23:05:24.002692Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MeanIoUWrapper(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "        self.metric.update_state(y_true, y_pred_labels)\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:26.333484Z",
     "iopub.status.busy": "2025-02-19T23:05:26.333162Z",
     "iopub.status.idle": "2025-02-19T23:05:27.232814Z",
     "shell.execute_reply": "2025-02-19T23:05:27.231873Z",
     "shell.execute_reply.started": "2025-02-19T23:05:26.333458Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convertir los pesos a tensores\n",
    "class_weight_tensor = tf.constant([class_weights[i] for i in range(len(unique_classes))], dtype=tf.float32)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    \"\"\"Aplica pesos de clases a la pérdida de entropía cruzada\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    sample_weights = tf.gather(class_weight_tensor, y_true)  # Asigna el peso según la clase\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return loss * sample_weights  # Escala la pérdida por el peso de la clase\n",
    "\n",
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, dtype=tf.float32)\n",
    "        y_pred = K.clip(y_pred, 1e-7, 1.0 - 1e-7)  # Evitar log(0)\n",
    "        ce = -y_true * K.log(y_pred)\n",
    "        weight = alpha * K.pow(1 - y_pred, gamma)\n",
    "        return K.mean(weight * ce)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:46.005650Z",
     "iopub.status.busy": "2025-02-19T23:05:46.005327Z",
     "iopub.status.idle": "2025-02-19T23:05:46.259051Z",
     "shell.execute_reply": "2025-02-19T23:05:46.257938Z",
     "shell.execute_reply.started": "2025-02-19T23:05:46.005625Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 3\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "mean_iou = MeanIoUWrapper(num_classes=NUM_CLASSES)\n",
    "\n",
    "model = build_pointnet(num_classes=NUM_CLASSES, input_dim=INPUT_DIM, max_points=16384)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=focal_loss(),\n",
    "              metrics=['accuracy', mean_iou])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.259Z",
     "iopub.execute_input": "2025-02-19T23:10:17.372591Z",
     "iopub.status.busy": "2025-02-19T23:10:17.372234Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 18:39:06.203069: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.41GiB (rounded to 1517617152)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-02-20 18:39:06.203401: W tensorflow/tsl/framework/bfc_allocator.cc:492] ***************************************_*********************_*****_____****________________________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train_shuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_shuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val_shuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_shuffle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train_shuffle,  \n",
    "    y_train_shuffle,  \n",
    "    validation_data=(x_val_shuffle, y_val_shuffle), \n",
    "    epochs=30,\n",
    "    batch_size = 8,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Guardar modelo completo en formato .keras (recomendado desde TensorFlow 2.6+)\n",
    "model.save(\"mi_modelo.keras\")\n",
    "\n",
    "# Guardar modelo en formato .h5 (si necesitas compatibilidad con versiones más antiguas)\n",
    "model.save(\"mi_modelo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Supongamos que estos vienen de tu modelo entrenado (Keras history)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss')\n",
    "mean_iou = history.history.get('mean_iou_wrapper')\n",
    "val_mean_iou = history.history.get('val_mean_iou_wrapper')\n",
    "accuracy = history.history.get('accuracy')\n",
    "val_accuracy = history.history.get('val_accuracy')\n",
    "\n",
    "# Crear figura con 1 fila y 3 columnas de subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"Loss Curve\", \"Mean IoU Curve\", \"Accuracy Curve\")\n",
    ")\n",
    "\n",
    "# 1) GRÁFICO DE PÉRDIDA (col=1)\n",
    "epochs_loss = list(range(1, len(loss) + 1))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_loss,\n",
    "        y=loss,\n",
    "        mode='lines+markers',\n",
    "        name='Train Loss'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "if val_loss:\n",
    "    epochs_val_loss = list(range(1, len(val_loss) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_val_loss,\n",
    "            y=val_loss,\n",
    "            mode='lines+markers',\n",
    "            name='Validation Loss'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1)\n",
    "\n",
    "# 2) GRÁFICO DE Mean IoU (col=2)\n",
    "if mean_iou:\n",
    "    epochs_mean_iou = list(range(1, len(mean_iou) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_mean_iou,\n",
    "            y=mean_iou,\n",
    "            mode='lines+markers',\n",
    "            name='Train Mean IoU'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    if val_mean_iou:\n",
    "        epochs_val_mean_iou = list(range(1, len(val_mean_iou) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_mean_iou,\n",
    "                y=val_mean_iou,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Mean IoU'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Mean IoU', row=1, col=2)\n",
    "\n",
    "# 3) GRÁFICO DE ACCURACY (col=3)\n",
    "if accuracy:\n",
    "    epochs_acc = list(range(1, len(accuracy) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_acc,\n",
    "            y=accuracy,\n",
    "            mode='lines+markers',\n",
    "            name='Train Accuracy'\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "    if val_accuracy:\n",
    "        epochs_val_acc = list(range(1, len(val_accuracy) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_acc,\n",
    "                y=val_accuracy,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Accuracy'\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=3)\n",
    "fig.update_yaxes(title_text='Accuracy', row=1, col=3)\n",
    "\n",
    "# Ajustes generales de la figura\n",
    "fig.update_layout(\n",
    "    width=1300,\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 378s 782ms/step - accuracy: 0.1974 - loss: 2.0782 - mean_iou_wrapper: 0.0620 - val_accuracy: 0.2606 - val_loss: 2.3051 - val_mean_iou_wrapper: 0.0676\n",
    "Epoch 2/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.2560 - loss: 1.8835 - mean_iou_wrapper: 0.0811 - val_accuracy: 0.2635 - val_loss: 2.0695 - val_mean_iou_wrapper: 0.0844\n",
    "Epoch 3/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.2668 - loss: 1.8064 - mean_iou_wrapper: 0.0890 - val_accuracy: 0.2187 - val_loss: 2.0295 - val_mean_iou_wrapper: 0.0717\n",
    "Epoch 4/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.3200 - loss: 1.7121 - mean_iou_wrapper: 0.1051 - val_accuracy: 0.3783 - val_loss: 1.8921 - val_mean_iou_wrapper: 0.1164\n",
    "Epoch 5/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.3409 - loss: 1.6596 - mean_iou_wrapper: 0.1135 - val_accuracy: 0.3608 - val_loss: 1.9976 - val_mean_iou_wrapper: 0.1206\n",
    "Epoch 6/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.3473 - loss: 1.6475 - mean_iou_wrapper: 0.1180 - val_accuracy: 0.2971 - val_loss: 1.8919 - val_mean_iou_wrapper: 0.0930\n",
    "Epoch 7/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.3850 - loss: 1.5954 - mean_iou_wrapper: 0.1280 - val_accuracy: 0.3707 - val_loss: 2.1627 - val_mean_iou_wrapper: 0.1101\n",
    "Epoch 8/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.3928 - loss: 1.5726 - mean_iou_wrapper: 0.1305 - val_accuracy: 0.3419 - val_loss: 1.8835 - val_mean_iou_wrapper: 0.1155\n",
    "Epoch 9/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.4121 - loss: 1.5251 - mean_iou_wrapper: 0.1386 - val_accuracy: 0.3426 - val_loss: 2.0606 - val_mean_iou_wrapper: 0.1069\n",
    "Epoch 10/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.4367 - loss: 1.4814 - mean_iou_wrapper: 0.1481 - val_accuracy: 0.4342 - val_loss: 1.9583 - val_mean_iou_wrapper: 0.1401\n",
    "Epoch 11/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.4149 - loss: 1.5287 - mean_iou_wrapper: 0.1395 - val_accuracy: 0.3544 - val_loss: 1.8760 - val_mean_iou_wrapper: 0.1184\n",
    "Epoch 12/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.4392 - loss: 1.4580 - mean_iou_wrapper: 0.1505 - val_accuracy: 0.3511 - val_loss: 1.7919 - val_mean_iou_wrapper: 0.1244\n",
    "Epoch 13/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 723ms/step - accuracy: 0.4695 - loss: 1.3695 - mean_iou_wrapper: 0.1660 - val_accuracy: 0.4573 - val_loss: 1.6821 - val_mean_iou_wrapper: 0.1545\n",
    "Epoch 14/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 722ms/step - accuracy: 0.4931 - loss: 1.3009 - mean_iou_wrapper: 0.1798 - val_accuracy: 0.5514 - val_loss: 1.6161 - val_mean_iou_wrapper: 0.1899\n",
    "Epoch 15/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 722ms/step - accuracy: 0.5151 - loss: 1.2271 - mean_iou_wrapper: 0.2020 - val_accuracy: 0.6254 - val_loss: 1.8242 - val_mean_iou_wrapper: 0.2169\n",
    "Epoch 16/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 722ms/step - accuracy: 0.5359 - loss: 1.1652 - mean_iou_wrapper: 0.2154 - val_accuracy: 0.5977 - val_loss: 1.6061 - val_mean_iou_wrapper: 0.2147\n",
    "Epoch 17/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 722ms/step - accuracy: 0.5537 - loss: 1.1096 - mean_iou_wrapper: 0.2278 - val_accuracy: 0.6259 - val_loss: 2.0443 - val_mean_iou_wrapper: 0.1971\n",
    "Epoch 18/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 349s 722ms/step - accuracy: 0.5662 - loss: 1.0688 - mean_iou_wrapper: 0.2377 - val_accuracy: 0.5784 - val_loss: 1.8561 - val_mean_iou_wrapper: 0.1962\n",
    "Epoch 19/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.5646 - loss: 1.0748 - mean_iou_wrapper: 0.2309 - val_accuracy: 0.5857 - val_loss: 1.7395 - val_mean_iou_wrapper: 0.2128\n",
    "Epoch 20/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.5844 - loss: 1.0161 - mean_iou_wrapper: 0.2496 - val_accuracy: 0.4979 - val_loss: 2.0896 - val_mean_iou_wrapper: 0.1632\n",
    "Epoch 21/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.6055 - loss: 0.9643 - mean_iou_wrapper: 0.2653 - val_accuracy: 0.5984 - val_loss: 1.8471 - val_mean_iou_wrapper: 0.2134\n",
    "Epoch 22/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.6205 - loss: 0.9233 - mean_iou_wrapper: 0.2813 - val_accuracy: 0.5607 - val_loss: 1.6588 - val_mean_iou_wrapper: 0.2047\n",
    "Epoch 23/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.6275 - loss: 0.8986 - mean_iou_wrapper: 0.2861 - val_accuracy: 0.5962 - val_loss: 1.7949 - val_mean_iou_wrapper: 0.2071\n",
    "Epoch 24/30\n",
    "483/483 ━━━━━━━━━━━━━━━━━━━━ 348s 721ms/step - accuracy: 0.6379 - loss: 0.8737 - mean_iou_wrapper: 0.2948 - val_accuracy: 0.1219 - val_loss: 52.6255 - val_mean_iou_wrapper: 0.0459\n",
    "Epoch 25/30\n",
    "204/483 ━━━━━━━━━━━━━━━━━━━━ 3:17 707ms/step - accuracy: 0.6255 - loss: 0.8810 - mean_iou_wrapper: 0.2887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6693936,
     "sourceId": 10786805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
