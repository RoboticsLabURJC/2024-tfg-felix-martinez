{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:37.429123Z",
     "iopub.status.busy": "2025-02-19T22:45:37.428898Z",
     "iopub.status.idle": "2025-02-19T22:45:42.190720Z",
     "shell.execute_reply": "2025-02-19T22:45:42.189734Z",
     "shell.execute_reply.started": "2025-02-19T22:45:37.429101Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Versión de cuDNN:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])\n",
    "print(\"Versión de CUDA:\", tf.sysconfig.get_build_info()[\"cuda_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Versión de TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOSE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:42.192969Z",
     "iopub.status.busy": "2025-02-19T22:45:42.192274Z",
     "iopub.status.idle": "2025-02-19T22:45:42.206732Z",
     "shell.execute_reply": "2025-02-19T22:45:42.205795Z",
     "shell.execute_reply.started": "2025-02-19T22:45:42.192927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_POINTS = 32768  # Tamaño de subnube fijo\n",
    "\n",
    "def filter_points_within_radius(points: np.ndarray, radius: float = 25.0) -> np.ndarray:\n",
    "    \"\"\"Filtra los puntos que están dentro de un radio dado.\"\"\"\n",
    "    distances = np.linalg.norm(points, axis=1) \n",
    "    mask = distances <= radius  \n",
    "    return points[mask]  \n",
    "\n",
    "# Mapeo de categorías\n",
    "category_mapping = {\n",
    "    0: [43, 38, 58, 29, 41, 42, 44, 39, 55],  # Construction\n",
    "    1: [4, 45, 6, 40, 60, 61, 33, 32, 14],  # Object\n",
    "    2: [7, 22, 9, 26, 11, 21],  # Road\n",
    "    3: [48, 47, 1, 19, 46, 10, 25],  # Sign\n",
    "    4: [23, 3, 24, 31, 2],  # Terrain  \n",
    "    5: [51, 50, 5, 18],  # Drivable Vegetation\n",
    "    6: [28, 27, 62, 52, 16, 30, 59, 17],  # Non Drivable Vegetation\n",
    "    7: [13, 15, 12, 36, 57, 49, 20, 35, 37, 34, 63],  # Vehicle\n",
    "    8: [8, 56, 0, 53, 54],  # Void\n",
    "}\n",
    "\n",
    "label_to_category = {label: cat for cat, labels in category_mapping.items() for label in labels}\n",
    "\n",
    "def map_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convierte etiquetas en categorías.\"\"\"\n",
    "    return np.array([label_to_category.get(label, 8) for label in labels], dtype=np.uint8)\n",
    "\n",
    "def load_bin_file(bin_path: str, num_points: int = NUM_POINTS, radius: float = 25.0) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Carga una nube de puntos y la divide en subnubes de tamaño fijo `num_points`.\n",
    "\n",
    "    Retorna:\n",
    "        Lista de tuplas (subnube, índices) para cada subnube generada.\n",
    "    \"\"\"\n",
    "    # Cargar la nube de puntos\n",
    "    points = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)[:, :3]\n",
    "\n",
    "    # Filtrar puntos dentro del radio dado\n",
    "    distances = np.linalg.norm(points, axis=1)\n",
    "    mask = distances <= radius\n",
    "    points = points[mask]\n",
    "\n",
    "    num_available = points.shape[0]\n",
    "\n",
    "    if num_available < num_points:\n",
    "        return []  # Si no hay suficientes puntos, se descarta la nube\n",
    "\n",
    "    # Ajustar el número de puntos a un múltiplo exacto de `num_points`\n",
    "    num_valid = num_available - (num_available % num_points)\n",
    "\n",
    "    # Seleccionar los primeros `num_valid` puntos\n",
    "    points = points[:num_valid]\n",
    "    indices = np.arange(num_valid)\n",
    "\n",
    "    # Dividir en subnubes de `num_points`\n",
    "    num_subnubes = num_valid // num_points\n",
    "    subnubes = [\n",
    "        (points[i * num_points: (i + 1) * num_points], indices[i * num_points: (i + 1) * num_points])\n",
    "        for i in range(num_subnubes)\n",
    "    ]\n",
    "\n",
    "    return subnubes\n",
    "\n",
    "def load_label_file(label_path: str, indices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Carga las etiquetas y las mapea a las categorías correspondientes.\"\"\"\n",
    "    labels = np.fromfile(label_path, dtype=np.uint32) & 0xFFFF\n",
    "    return map_labels(labels[indices])\n",
    "\n",
    "def load_dataset(bin_files: List[str], label_files: List[str], num_points: int = NUM_POINTS) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga el conjunto de datos dividiendo las nubes grandes en subnubes de tamaño `num_points`.\n",
    "    \"\"\"\n",
    "    x_data, y_data = [], []\n",
    "\n",
    "    for bin_f, label_f in tqdm(zip(bin_files, label_files), total=len(bin_files), desc=\"Cargando datos\"):\n",
    "        subnubes = load_bin_file(bin_f, num_points)\n",
    "\n",
    "        for points, indices in subnubes:\n",
    "            labels = load_label_file(label_f, indices)\n",
    "            x_data.append(points)\n",
    "            y_data.append(labels)\n",
    "\n",
    "    return np.array(x_data, dtype=np.float32), np.array(y_data, dtype=np.uint8)\n",
    "\n",
    "def get_file_paths(data_dir: str) -> List[str]:\n",
    "    \"\"\"Obtiene rutas de archivos en un directorio.\"\"\"\n",
    "    return sorted([str(f) for f in Path(data_dir).glob(\"*.*\")])\n",
    "\n",
    "def load_all_data(x_train_dir: str, y_train_dir: str, x_val_dir: str, y_val_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Carga los datos de entrenamiento y validación en subnubes de tamaño fijo.\"\"\"\n",
    "    x_train_files = get_file_paths(x_train_dir)\n",
    "    y_train_files = get_file_paths(y_train_dir)\n",
    "    x_val_files = get_file_paths(x_val_dir)\n",
    "    y_val_files = get_file_paths(y_val_dir)\n",
    "\n",
    "    assert len(x_train_files) == len(y_train_files), \"Número de archivos x_train y y_train no coincide.\"\n",
    "    assert len(x_val_files) == len(y_val_files), \"Número de archivos x_val y y_val no coincide.\"\n",
    "\n",
    "    print(\"Cargando datos de entrenamiento...\")\n",
    "    x_train, y_train = load_dataset(x_train_files, y_train_files)\n",
    "\n",
    "    print(\"Cargando datos de validación...\")\n",
    "    x_val, y_val = load_dataset(x_val_files, y_val_files)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:48:25.468327Z",
     "iopub.status.busy": "2025-02-19T22:48:25.468011Z",
     "iopub.status.idle": "2025-02-19T22:57:03.863818Z",
     "shell.execute_reply": "2025-02-19T22:57:03.862746Z",
     "shell.execute_reply.started": "2025-02-19T22:48:25.468304Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_path = \"/home/fmartinez/datasets/lidar/train\"\n",
    "y_train_path = \"/home/fmartinez/datasets/labels/train\"\n",
    "x_val_path = \"/home/fmartinez/datasets_val/lidar/val\"\n",
    "y_val_path = \"/home/fmartinez/datasets_val/labels/val\"\n",
    "\n",
    "x_train, y_train, x_val, y_val = load_all_data(x_train_path, y_train_path, x_val_path, y_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:02:41.161169Z",
     "iopub.status.busy": "2025-02-19T23:02:41.160826Z",
     "iopub.status.idle": "2025-02-19T23:02:43.101788Z",
     "shell.execute_reply": "2025-02-19T23:02:43.101090Z",
     "shell.execute_reply.started": "2025-02-19T23:02:41.161144Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_permutados_train = np.random.permutation(x_train.shape[0])\n",
    "indices_permutados_val = np.random.permutation(x_val.shape[0])\n",
    "\n",
    "x_train_shuffle = x_train[indices_permutados_train]\n",
    "y_train_shuffle = y_train[indices_permutados_train]\n",
    "x_val_shuffle = x_val[indices_permutados_val]\n",
    "y_val_shuffle = y_val[indices_permutados_val]\n",
    "\n",
    "x_train_shuffle.shape, y_train_shuffle.shape, x_val_shuffle.shape, y_val_shuffle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train) and len(x_val) == len(y_val)\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_train)} nubes de puntos de {y_train.shape[1]} puntos\")\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_val)} nubes de puntos de {y_val.shape[1]} puntos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:07.718183Z",
     "iopub.status.busy": "2025-02-19T23:04:07.717827Z",
     "iopub.status.idle": "2025-02-19T23:04:07.724041Z",
     "shell.execute_reply": "2025-02-19T23:04:07.723077Z",
     "shell.execute_reply.started": "2025-02-19T23:04:07.718155Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_3D(xyz, labels):\n",
    "    \"\"\"\n",
    "    Visualiza una nube de puntos en 3D con Plotly.\n",
    "    - xyz: (num_points, 3) array con coordenadas (X, Y, Z).\n",
    "    - labels: (num_points,) array con etiquetas semánticas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Definir 9 colores predefinidos en formato RGB\n",
    "    predefined_colors = [\n",
    "        \"rgb(255, 0, 0)\",    # Rojo\n",
    "        \"rgb(0, 255, 0)\",    # Verde\n",
    "        \"rgb(0, 0, 255)\",    # Azul\n",
    "        \"rgb(255, 255, 0)\",  # Amarillo\n",
    "        \"rgb(255, 165, 0)\",  # Naranja\n",
    "        \"rgb(128, 0, 128)\",  # Púrpura\n",
    "        \"rgb(0, 255, 255)\",  # Cian\n",
    "        \"rgb(255, 192, 203)\",# Rosa\n",
    "        \"rgb(128, 128, 128)\" # Gris\n",
    "    ]\n",
    "\n",
    "    # Asignar colores según la etiqueta (se asume que las etiquetas van de 0 a 8)\n",
    "    point_colors = [predefined_colors[label % len(predefined_colors)] for label in labels]\n",
    "\n",
    "    # Crear la figura 3D en Plotly\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=xyz[:, 0], y=xyz[:, 1], z=xyz[:, 2],  # Coordenadas X, Y, Z\n",
    "        mode='markers',\n",
    "        marker=dict(size=1, color=point_colors, opacity=0.8)  # Color basado en etiquetas\n",
    "    ))\n",
    "\n",
    "    # Configurar etiquetas y título\n",
    "    fig.update_layout(\n",
    "        title=\"Nube de Puntos con Etiquetas Semánticas\",\n",
    "        scene=dict(xaxis_title=\"X\", yaxis_title=\"Y\", zaxis_title=\"Z\")\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:24.140632Z",
     "iopub.status.busy": "2025-02-19T23:04:24.140280Z",
     "iopub.status.idle": "2025-02-19T23:04:25.355377Z",
     "shell.execute_reply": "2025-02-19T23:04:25.353411Z",
     "shell.execute_reply.started": "2025-02-19T23:04:24.140600Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_3D(x_train_shuffle[1000], y_train_shuffle[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:08.622104Z",
     "iopub.status.busy": "2025-02-19T23:05:08.621750Z",
     "iopub.status.idle": "2025-02-19T23:05:11.936407Z",
     "shell.execute_reply": "2025-02-19T23:05:11.935406Z",
     "shell.execute_reply.started": "2025-02-19T23:05:08.622075Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print(len(y_train))\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Clase {cls}: {count} puntos\")\n",
    "\n",
    "fig = px.bar(x=unique_classes, y=class_counts, labels={'x': 'Clase', 'y': 'Cantidad de puntos'},\n",
    "             title='Distribución de etiquetas en y_train')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:16.216947Z",
     "iopub.status.busy": "2025-02-19T23:05:16.216572Z",
     "iopub.status.idle": "2025-02-19T23:05:16.246341Z",
     "shell.execute_reply": "2025-02-19T23:05:16.245375Z",
     "shell.execute_reply.started": "2025-02-19T23:05:16.216914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular pesos inversamente proporcionales a la frecuencia de cada clase\n",
    "total_samples = len(y_train.flatten())  # Total de puntos\n",
    "class_weights = {cls: total_samples / (len(unique_classes) * count) for cls, count in zip(unique_classes, class_counts)}\n",
    "\n",
    "print(\"Pesos de las clases:\", class_weights)\n",
    "\n",
    "len(unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:19.384048Z",
     "iopub.status.busy": "2025-02-19T23:05:19.383654Z",
     "iopub.status.idle": "2025-02-19T23:05:19.396156Z",
     "shell.execute_reply": "2025-02-19T23:05:19.395116Z",
     "shell.execute_reply.started": "2025-02-19T23:05:19.384013Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "MAX_POINTS = 32768 # 256, 512, 1024, 2048, 4096, 8192, 16384, 32768\n",
    "\n",
    "def tnet(inputs, num_features):\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(num_features * num_features, kernel_initializer='zeros',\n",
    "                     bias_initializer=tf.keras.initializers.Constant(tf.eye(num_features).numpy().flatten()))(x)\n",
    "    transform_matrix = layers.Reshape((num_features, num_features))(x)\n",
    "\n",
    "    def transform(inputs_and_matrix):\n",
    "        inputs, matrix = inputs_and_matrix\n",
    "        return tf.matmul(inputs, matrix)\n",
    "\n",
    "    transformed_inputs = layers.Lambda(transform)([inputs, transform_matrix])\n",
    "    transformed_inputs = layers.Lambda(lambda t: tf.ensure_shape(t, (None, MAX_POINTS, num_features)))(transformed_inputs)\n",
    "\n",
    "    return transformed_inputs\n",
    "\n",
    "def build_pointnet(num_classes, input_dim=3, max_points=MAX_POINTS):\n",
    "    inputs = tf.keras.Input(shape=(None, input_dim))\n",
    "\n",
    "    x = tnet(inputs, input_dim)\n",
    "\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tnet(x, 64)\n",
    "\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    global_features = layers.GlobalMaxPooling1D()(x)\n",
    "    global_features = layers.Lambda(lambda t: tf.expand_dims(t, axis=1))(global_features)\n",
    "    global_features = layers.Lambda(lambda t: tf.repeat(t, repeats=max_points, axis=1))(global_features)\n",
    "\n",
    "    x = layers.Lambda(lambda t: tf.ensure_shape(t, (None, max_points, 1024)))(x)\n",
    "\n",
    "    x = layers.concatenate([x, global_features], axis=-1)\n",
    "\n",
    "    x = layers.Conv1D(512, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(256, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    outputs = layers.Conv1D(num_classes, 1, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:24.002726Z",
     "iopub.status.busy": "2025-02-19T23:05:24.002339Z",
     "iopub.status.idle": "2025-02-19T23:05:24.009292Z",
     "shell.execute_reply": "2025-02-19T23:05:24.007668Z",
     "shell.execute_reply.started": "2025-02-19T23:05:24.002692Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MeanIoUWrapper(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "        self.metric.update_state(y_true, y_pred_labels)\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n",
    "\n",
    "class MeanIoUWrapper_2(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int32)  # ✅ Asegurar que y_true sea int32\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "\n",
    "        self.metric.update_state(y_true, y_pred_labels, sample_weight)  # ✅ Pasar sample_weight\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:26.333484Z",
     "iopub.status.busy": "2025-02-19T23:05:26.333162Z",
     "iopub.status.idle": "2025-02-19T23:05:27.232814Z",
     "shell.execute_reply": "2025-02-19T23:05:27.231873Z",
     "shell.execute_reply.started": "2025-02-19T23:05:26.333458Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convertir los pesos a tensores\n",
    "class_weight_tensor = tf.constant([class_weights[i] for i in range(len(unique_classes))], dtype=tf.float32)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    sample_weights = tf.gather(class_weight_tensor, y_true)  # Asigna el peso según la clase\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return loss * sample_weights  # Escala la pérdida por el peso de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:46.005650Z",
     "iopub.status.busy": "2025-02-19T23:05:46.005327Z",
     "iopub.status.idle": "2025-02-19T23:05:46.259051Z",
     "shell.execute_reply": "2025-02-19T23:05:46.257938Z",
     "shell.execute_reply.started": "2025-02-19T23:05:46.005625Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 3\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "# Definir el modelo PointNet\n",
    "model = build_pointnet(num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n",
    "mean_iou_wrapper = MeanIoUWrapper(num_classes=NUM_CLASSES)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"accuracy\", mean_iou_wrapper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.259Z",
     "iopub.execute_input": "2025-02-19T23:10:17.372591Z",
     "iopub.status.busy": "2025-02-19T23:10:17.372234Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train_shuffle,  \n",
    "    y_train_shuffle,  \n",
    "    validation_data=(x_val_shuffle, y_val_shuffle), \n",
    "    epochs=30,\n",
    "    batch_size = 8,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"pointnet_goose_16k_3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Supongamos que estos vienen de tu modelo entrenado (Keras history)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss')\n",
    "mean_iou = history.history.get('mean_iou_wrapper')\n",
    "val_mean_iou = history.history.get('val_mean_iou_wrapper')\n",
    "accuracy = history.history.get('accuracy')\n",
    "val_accuracy = history.history.get('val_accuracy')\n",
    "\n",
    "# Crear figura con 1 fila y 3 columnas de subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"Loss Curve\", \"Mean IoU Curve\", \"Accuracy Curve\")\n",
    ")\n",
    "\n",
    "# 1) GRÁFICO DE PÉRDIDA (col=1)\n",
    "epochs_loss = list(range(1, len(loss) + 1))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_loss,\n",
    "        y=loss,\n",
    "        mode='lines+markers',\n",
    "        name='Train Loss'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "if val_loss:\n",
    "    epochs_val_loss = list(range(1, len(val_loss) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_val_loss,\n",
    "            y=val_loss,\n",
    "            mode='lines+markers',\n",
    "            name='Validation Loss'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1, range=[0, 10])  # Rango ajustado\n",
    "\n",
    "# 2) GRÁFICO DE Mean IoU (col=2) -> Limitar valores a [0,1]\n",
    "if mean_iou:\n",
    "    mean_iou = np.clip(mean_iou, 0, 1)\n",
    "    epochs_mean_iou = list(range(1, len(mean_iou) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_mean_iou,\n",
    "            y=mean_iou,\n",
    "            mode='lines+markers',\n",
    "            name='Train Mean IoU'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    if val_mean_iou:\n",
    "        val_mean_iou = np.clip(val_mean_iou, 0, 1)\n",
    "        epochs_val_mean_iou = list(range(1, len(val_mean_iou) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_mean_iou,\n",
    "                y=val_mean_iou,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Mean IoU'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Mean IoU', row=1, col=2, range=[0, 1])\n",
    "\n",
    "# 3) GRÁFICO DE ACCURACY (col=3) -> Limitar valores a [0,1]\n",
    "if accuracy:\n",
    "    accuracy = np.clip(accuracy, 0, 1)\n",
    "    epochs_acc = list(range(1, len(accuracy) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_acc,\n",
    "            y=accuracy,\n",
    "            mode='lines+markers',\n",
    "            name='Train Accuracy'\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "    if val_accuracy:\n",
    "        val_accuracy = np.clip(val_accuracy, 0, 1)\n",
    "        epochs_val_acc = list(range(1, len(val_accuracy) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_acc,\n",
    "                y=val_accuracy,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Accuracy'\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=3)\n",
    "fig.update_yaxes(title_text='Accuracy', row=1, col=3, range=[0, 1])\n",
    "\n",
    "# Ajustes generales de la figura\n",
    "fig.update_layout(\n",
    "    width=1300,\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6693936,
     "sourceId": 10786805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
