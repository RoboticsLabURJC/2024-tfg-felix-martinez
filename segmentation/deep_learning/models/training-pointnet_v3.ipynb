{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPUs detectadas:\", tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:37.429123Z",
     "iopub.status.busy": "2025-02-19T22:45:37.428898Z",
     "iopub.status.idle": "2025-02-19T22:45:42.190720Z",
     "shell.execute_reply": "2025-02-19T22:45:42.189734Z",
     "shell.execute_reply.started": "2025-02-19T22:45:37.429101Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Versión de cuDNN:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])\n",
    "print(\"Versión de CUDA:\", tf.sysconfig.get_build_info()[\"cuda_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Versión de TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOSE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:45:42.192969Z",
     "iopub.status.busy": "2025-02-19T22:45:42.192274Z",
     "iopub.status.idle": "2025-02-19T22:45:42.206732Z",
     "shell.execute_reply": "2025-02-19T22:45:42.205795Z",
     "shell.execute_reply.started": "2025-02-19T22:45:42.192927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_POINTS = 4096  # Tamaño de subnube fijo\n",
    "\n",
    "# Mapeo de categorías\n",
    "category_mapping = {\n",
    "    0: [43, 38, 58, 29, 41, 42, 44, 39, 55],  # Construction\n",
    "    1: [4, 45, 6, 40, 60, 61, 33, 32, 14],  # Object\n",
    "    2: [7, 22, 9, 26, 11, 21],  # Road\n",
    "    3: [48, 47, 1, 19, 46, 10, 25],  # Sign\n",
    "    4: [23, 3, 24, 31, 2],  # Terrain  \n",
    "    5: [51, 50, 5, 18],  # Drivable Vegetation\n",
    "    6: [28, 27, 62, 52, 16, 30, 59, 17],  # Non Drivable Vegetation\n",
    "    7: [13, 15, 12, 36, 57, 49, 20, 35, 37, 34, 63],  # Vehicle\n",
    "    8: [8, 56, 0, 53, 54],  # Void\n",
    "}\n",
    "\n",
    "label_to_category = {label: cat for cat, labels in category_mapping.items() for label in labels}\n",
    "\n",
    "def map_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convierte etiquetas en categorías\"\"\"\n",
    "    return np.array([label_to_category.get(label, 8) for label in labels], dtype=np.uint8)\n",
    "\n",
    "def load_bin_file(bin_path: str, num_points: int = NUM_POINTS, radius: float = 25.0) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Carga una nube de puntos y la divide en subnubes de tamaño fijo `num_points`.\n",
    "\n",
    "    Retorna:\n",
    "        Lista de tuplas (subnube, índices) para cada subnube generada.\n",
    "    \"\"\"\n",
    "    # Cargar la nube de puntos\n",
    "    points = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)[:, :3]\n",
    "\n",
    "    # Filtrar puntos dentro del radio dado\n",
    "    distances = np.linalg.norm(points, axis=1)\n",
    "    mask = distances <= radius\n",
    "    indices = np.arange(len(points))[mask]\n",
    "    points = points[mask]\n",
    "\n",
    "    num_available = points.shape[0]\n",
    "\n",
    "    if num_available < num_points:\n",
    "        return []  # Si no hay suficientes puntos, se descarta la nube\n",
    "\n",
    "    # Ajustar el número de puntos a un múltiplo exacto de `num_points`\n",
    "    num_valid = num_available - (num_available % num_points)\n",
    "\n",
    "    # Seleccionar los primeros `num_valid` puntos\n",
    "    points = points[:num_valid]\n",
    "    indices = indices[:num_valid]\n",
    "\n",
    "    # Dividir en subnubes de `num_points`\n",
    "    num_subnubes = num_valid // num_points\n",
    "    subnubes = [\n",
    "        (points[i * num_points: (i + 1) * num_points], indices[i * num_points: (i + 1) * num_points])\n",
    "        for i in range(num_subnubes)\n",
    "    ]\n",
    "\n",
    "    return subnubes\n",
    "\n",
    "def load_label_file(label_path: str, indices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Carga las etiquetas y las mapea a las categorías correspondientes.\"\"\"\n",
    "    labels = np.fromfile(label_path, dtype=np.uint32) & 0xFFFF\n",
    "    return map_labels(labels[indices])\n",
    "\n",
    "def load_dataset(bin_files: List[str], label_files: List[str], num_points: int = NUM_POINTS) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Carga el conjunto de datos dividiendo las nubes grandes en subnubes de tamaño `num_points`.\n",
    "    \"\"\"\n",
    "    x_data, y_data = [], []\n",
    "\n",
    "    for bin_f, label_f in tqdm(zip(bin_files, label_files), total=len(bin_files), desc=\"Cargando datos\"):\n",
    "        subnubes = load_bin_file(bin_f, num_points)\n",
    "\n",
    "        for points, indices in subnubes:\n",
    "            labels = load_label_file(label_f, indices)\n",
    "            x_data.append(points)\n",
    "            y_data.append(labels)\n",
    "\n",
    "    return np.array(x_data, dtype=np.float32), np.array(y_data, dtype=np.uint8)\n",
    "\n",
    "def get_file_paths(data_dir: str) -> List[str]:\n",
    "    \"\"\"Obtiene rutas de archivos en un directorio.\"\"\"\n",
    "    return sorted([str(f) for f in Path(data_dir).glob(\"*.*\")])\n",
    "\n",
    "def load_all_data(x_train_dir: str, y_train_dir: str, x_val_dir: str, y_val_dir: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Carga los datos de entrenamiento y validación en subnubes de tamaño fijo.\"\"\"\n",
    "    x_train_files = get_file_paths(x_train_dir)\n",
    "    y_train_files = get_file_paths(y_train_dir)\n",
    "    x_val_files = get_file_paths(x_val_dir)\n",
    "    y_val_files = get_file_paths(y_val_dir)\n",
    "\n",
    "    assert len(x_train_files) == len(y_train_files), \"Número de archivos x_train y y_train no coincide.\"\n",
    "    assert len(x_val_files) == len(y_val_files), \"Número de archivos x_val y y_val no coincide.\"\n",
    "\n",
    "    print(\"Cargando datos de entrenamiento...\")\n",
    "    x_train, y_train = load_dataset(x_train_files, y_train_files)\n",
    "\n",
    "    print(\"Cargando datos de validación...\")\n",
    "    x_val, y_val = load_dataset(x_val_files, y_val_files)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:48:25.468327Z",
     "iopub.status.busy": "2025-02-19T22:48:25.468011Z",
     "iopub.status.idle": "2025-02-19T22:57:03.863818Z",
     "shell.execute_reply": "2025-02-19T22:57:03.862746Z",
     "shell.execute_reply.started": "2025-02-19T22:48:25.468304Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_path = \"/home/fmartinez/datasets/lidar/train\"\n",
    "y_train_path = \"/home/fmartinez/datasets/labels/train\"\n",
    "x_val_path = \"/home/fmartinez/datasets_val/lidar/val\"\n",
    "y_val_path = \"/home/fmartinez/datasets_val/labels/val\"\n",
    "\n",
    "x_train, y_train, x_val, y_val = load_all_data(x_train_path, y_train_path, x_val_path, y_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:02:41.161169Z",
     "iopub.status.busy": "2025-02-19T23:02:41.160826Z",
     "iopub.status.idle": "2025-02-19T23:02:43.101788Z",
     "shell.execute_reply": "2025-02-19T23:02:43.101090Z",
     "shell.execute_reply.started": "2025-02-19T23:02:41.161144Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_permutados_train = np.random.permutation(x_train.shape[0])\n",
    "indices_permutados_val = np.random.permutation(x_val.shape[0])\n",
    "\n",
    "x_train_shuffle = x_train[indices_permutados_train]\n",
    "y_train_shuffle = y_train[indices_permutados_train]\n",
    "x_val_shuffle = x_val[indices_permutados_val]\n",
    "y_val_shuffle = y_val[indices_permutados_val]\n",
    "\n",
    "x_train_shuffle.shape, y_train_shuffle.shape, x_val_shuffle.shape, y_val_shuffle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train) and len(x_val) == len(y_val)\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_train)} nubes de puntos de {y_train.shape[1]} puntos\")\n",
    "print(f\"El conjunto de entrenamiento tiene {len(y_val)} nubes de puntos de {y_val.shape[1]} puntos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:07.718183Z",
     "iopub.status.busy": "2025-02-19T23:04:07.717827Z",
     "iopub.status.idle": "2025-02-19T23:04:07.724041Z",
     "shell.execute_reply": "2025-02-19T23:04:07.723077Z",
     "shell.execute_reply.started": "2025-02-19T23:04:07.718155Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_3D(xyz, labels):\n",
    "    \"\"\"\n",
    "    Visualiza una nube de puntos en 3D con Plotly.\n",
    "    - xyz: (num_points, 3) array con coordenadas (X, Y, Z).\n",
    "    - labels: (num_points,) array con etiquetas semánticas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Definir 9 colores predefinidos en formato RGB\n",
    "    predefined_colors = [\n",
    "        \"rgb(255, 0, 0)\",    # Rojo\n",
    "        \"rgb(0, 255, 0)\",    # Verde\n",
    "        \"rgb(0, 0, 255)\",    # Azul\n",
    "        \"rgb(255, 255, 0)\",  # Amarillo\n",
    "        \"rgb(255, 165, 0)\",  # Naranja\n",
    "        \"rgb(128, 0, 128)\",  # Púrpura\n",
    "        \"rgb(0, 255, 255)\",  # Cian\n",
    "        \"rgb(255, 192, 203)\",# Rosa\n",
    "        \"rgb(128, 128, 128)\" # Gris\n",
    "    ]\n",
    "\n",
    "    # Asignar colores según la etiqueta (se asume que las etiquetas van de 0 a 8)\n",
    "    point_colors = [predefined_colors[label % len(predefined_colors)] for label in labels]\n",
    "\n",
    "    # Crear la figura 3D en Plotly\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=xyz[:, 0], y=xyz[:, 1], z=xyz[:, 2],  # Coordenadas X, Y, Z\n",
    "        mode='markers',\n",
    "        marker=dict(size=1, color=point_colors, opacity=0.8)  # Color basado en etiquetas\n",
    "    ))\n",
    "\n",
    "    # Configurar etiquetas y título\n",
    "    fig.update_layout(\n",
    "        title=\"Nube de Puntos con Etiquetas Semánticas\",\n",
    "        scene=dict(xaxis_title=\"X\", yaxis_title=\"Y\", zaxis_title=\"Z\")\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:04:24.140632Z",
     "iopub.status.busy": "2025-02-19T23:04:24.140280Z",
     "iopub.status.idle": "2025-02-19T23:04:25.355377Z",
     "shell.execute_reply": "2025-02-19T23:04:25.353411Z",
     "shell.execute_reply.started": "2025-02-19T23:04:24.140600Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_3D(x_train_shuffle[500], y_train_shuffle[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:08.622104Z",
     "iopub.status.busy": "2025-02-19T23:05:08.621750Z",
     "iopub.status.idle": "2025-02-19T23:05:11.936407Z",
     "shell.execute_reply": "2025-02-19T23:05:11.935406Z",
     "shell.execute_reply.started": "2025-02-19T23:05:08.622075Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print(len(y_train))\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Clase {cls}: {count} puntos\")\n",
    "\n",
    "fig = px.bar(x=unique_classes, y=class_counts, labels={'x': 'Clase', 'y': 'Cantidad de puntos'},\n",
    "             title='Distribución de etiquetas en y_train')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:16.216947Z",
     "iopub.status.busy": "2025-02-19T23:05:16.216572Z",
     "iopub.status.idle": "2025-02-19T23:05:16.246341Z",
     "shell.execute_reply": "2025-02-19T23:05:16.245375Z",
     "shell.execute_reply.started": "2025-02-19T23:05:16.216914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcular pesos inversamente proporcionales a la frecuencia de cada clase\n",
    "total_samples = len(y_train.flatten())  # Total de puntos\n",
    "class_weights = {cls: total_samples / (len(unique_classes) * count) for cls, count in zip(unique_classes, class_counts)}\n",
    "\n",
    "print(\"Pesos de las clases:\", class_weights)\n",
    "\n",
    "len(unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:19.384048Z",
     "iopub.status.busy": "2025-02-19T23:05:19.383654Z",
     "iopub.status.idle": "2025-02-19T23:05:19.396156Z",
     "shell.execute_reply": "2025-02-19T23:05:19.395116Z",
     "shell.execute_reply.started": "2025-02-19T23:05:19.384013Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "MAX_POINTS = 4096 # 256, 512, 1024, 2048, 4096, 8192, 16384, 32768\n",
    "\n",
    "def tnet(inputs, num_features):\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(num_features * num_features, kernel_initializer='zeros',\n",
    "                     bias_initializer=tf.keras.initializers.Constant(tf.eye(num_features).numpy().flatten()))(x)\n",
    "    transform_matrix = layers.Reshape((num_features, num_features))(x)\n",
    "\n",
    "    def transform(inputs_and_matrix):\n",
    "        inputs, matrix = inputs_and_matrix\n",
    "        return tf.matmul(inputs, matrix)\n",
    "\n",
    "    transformed_inputs = layers.Lambda(transform)([inputs, transform_matrix])\n",
    "    transformed_inputs = layers.Lambda(lambda t: tf.ensure_shape(t, (None, MAX_POINTS, num_features)))(transformed_inputs)\n",
    "\n",
    "    return transformed_inputs\n",
    "\n",
    "def build_pointnet(num_classes, input_dim=3, max_points=MAX_POINTS):\n",
    "    inputs = tf.keras.Input(shape=(None, input_dim))\n",
    "\n",
    "    x = tnet(inputs, input_dim)\n",
    "\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(64, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tnet(x, 64)\n",
    "\n",
    "    x = layers.Conv1D(1024, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    global_features = layers.GlobalMaxPooling1D()(x)\n",
    "    global_features = layers.Lambda(lambda t: tf.expand_dims(t, axis=1))(global_features)\n",
    "    global_features = layers.Lambda(lambda t: tf.repeat(t, repeats=max_points, axis=1))(global_features)\n",
    "\n",
    "    x = layers.Lambda(lambda t: tf.ensure_shape(t, (None, max_points, 1024)))(x)\n",
    "\n",
    "    x = layers.concatenate([x, global_features], axis=-1)\n",
    "\n",
    "    x = layers.Conv1D(512, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(256, 1, activation='relu', padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    outputs = layers.Conv1D(num_classes, 1, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:24.002726Z",
     "iopub.status.busy": "2025-02-19T23:05:24.002339Z",
     "iopub.status.idle": "2025-02-19T23:05:24.009292Z",
     "shell.execute_reply": "2025-02-19T23:05:24.007668Z",
     "shell.execute_reply.started": "2025-02-19T23:05:24.002692Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MeanIoUWrapper(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  # Convertir (batch, 16384, 9) -> (batch, 16384)\n",
    "        self.metric.update_state(y_true, y_pred_labels)\n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n",
    "\n",
    "class MeanIoUWrapper_2(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name=\"mean_iou_wrapper\", **kwargs):\n",
    "        super(MeanIoUWrapper, self).__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.metric = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.int32)  \n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)  \n",
    "\n",
    "        self.metric.update_state(y_true, y_pred_labels, sample_weight)  \n",
    "\n",
    "    def result(self):\n",
    "        return self.metric.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.metric.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:26.333484Z",
     "iopub.status.busy": "2025-02-19T23:05:26.333162Z",
     "iopub.status.idle": "2025-02-19T23:05:27.232814Z",
     "shell.execute_reply": "2025-02-19T23:05:27.231873Z",
     "shell.execute_reply.started": "2025-02-19T23:05:26.333458Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Pesos ajustados\n",
    "adjusted_weights = {\n",
    "    0: 0.80,  # Clase frecuente\n",
    "    1: 1.0,   # Clases minoritarias quedan en 1\n",
    "    2: 1.0,\n",
    "    3: 1.0,\n",
    "    4: 0.65,   # Clase frecuente\n",
    "    5: 0.55,  # Clase frecuente\n",
    "    6: 0.3,   # Clase muy frecuente\n",
    "    7: 1.0,\n",
    "    8: 1.0\n",
    "}\n",
    "\n",
    "# Convertir a vector NumPy\n",
    "alpha_vector_adjusted = np.array([adjusted_weights[i] for i in range(len(adjusted_weights))], dtype=np.float32)\n",
    "\n",
    "# Convertir a tensor de TensorFlow\n",
    "alpha_tensor_adjusted = tf.constant(alpha_vector_adjusted, dtype=tf.float32)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    sample_weights = tf.gather(class_weight_tensor, y_true)  # Asigna el peso según la clase\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return loss * sample_weights  # Escala la pérdida por el peso de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "class PointNetSegLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, alpha=None, gamma=0, dice=False):\n",
    "        \"\"\"\n",
    "        Implementación de Focal Loss + Dice Loss para segmentación en TensorFlow.\n",
    "        \n",
    "        Args:\n",
    "            alpha: Tensor con los pesos de clase (influencia de cada clase en la pérdida).\n",
    "            gamma: Factor de Focal Loss. Si gamma > 0, enfatiza ejemplos difíciles.\n",
    "            dice: Si es True, se suma la Dice Loss.\n",
    "        \"\"\"\n",
    "        super(PointNetSegLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.dice = dice\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calcula la pérdida combinada (Focal Loss + Dice Loss si está activada).\n",
    "        \n",
    "        Args:\n",
    "            y_true: Tensor con etiquetas verdaderas (batch_size, num_points).\n",
    "            y_pred: Tensor con logits o probabilidades predichas (batch_size, num_points, num_classes).\n",
    "            \n",
    "        Returns:\n",
    "            Pérdida escalada con los pesos de clase y Focal Loss.\n",
    "        \"\"\"\n",
    "        num_classes = tf.shape(y_pred)[-1]\n",
    "\n",
    "        # Convertir etiquetas a one-hot\n",
    "        y_true = tf.cast(y_true, tf.int32)  # Asegurar tipo int32\n",
    "        y_true_one_hot = tf.one_hot(y_true, depth=num_classes)\n",
    "\n",
    "        # Cross Entropy Loss base\n",
    "        ce_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction=\"none\")(y_true_one_hot, y_pred)\n",
    "\n",
    "        # Aplicar pesos de clase (alpha) si están definidos\n",
    "        if self.alpha is not None:\n",
    "            y_true_flat = tf.reshape(y_true, [-1])  # Convertir a 1D para evitar errores\n",
    "            sample_weights = tf.gather(self.alpha, y_true_flat)  # Extraer pesos\n",
    "\n",
    "            # Ajustar la forma de `sample_weights` para que coincida con `ce_loss`\n",
    "            sample_weights = tf.reshape(sample_weights, tf.shape(ce_loss))\n",
    "\n",
    "            # Escalar la pérdida\n",
    "            ce_loss *= sample_weights\n",
    "\n",
    "        # Aplicar Focal Loss si gamma > 0\n",
    "        if self.gamma > 0:\n",
    "            y_pred_softmax = tf.nn.softmax(y_pred, axis=-1)  # Convertir logits a probabilidades\n",
    "            pt = tf.reduce_sum(y_true_one_hot * y_pred_softmax, axis=-1)  # Probabilidad del target correcto\n",
    "            focal_loss = (1 - pt) ** self.gamma * ce_loss  # Focal Loss\n",
    "        else:\n",
    "            focal_loss = ce_loss\n",
    "\n",
    "        # Promediar la pérdida\n",
    "        loss = tf.reduce_mean(focal_loss)\n",
    "\n",
    "        # Si Dice Loss está activada, se suma a la pérdida total\n",
    "        if self.dice:\n",
    "            dice_loss = self.compute_dice_loss(y_true_one_hot, y_pred_softmax)\n",
    "            loss += dice_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_dice_loss(y_true, y_pred, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Calcula la Dice Loss, que mide la superposición entre predicciones y etiquetas.\n",
    "        \"\"\"\n",
    "        intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])  # Suma sobre puntos y clases\n",
    "        union = tf.reduce_sum(y_true + y_pred, axis=[1, 2])\n",
    "\n",
    "        dice = (2. * intersection + eps) / (union + eps)\n",
    "        return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# Crear la función de pérdida con los pesos ajustados\n",
    "loss_fn = PointNetSegLoss(alpha=alpha_tensor_adjusted, gamma=0.5, dice=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class PointCloudSequence(Sequence):\n",
    "    def __init__(self, x_data, y_data, batch_size=32, shuffle=True, augment_prob=0.25):\n",
    "        \"\"\"\n",
    "        DataLoader con normalización y aumentaciones para segmentación de nubes de puntos.\n",
    "\n",
    "        Args:\n",
    "            x_data: numpy array con shape (m, 4096, 3) (nubes de puntos).\n",
    "            y_data: numpy array con shape (m, 4096) (etiquetas por punto).\n",
    "            batch_size: Número de nubes por batch.\n",
    "            shuffle: Si True, reorganiza los datos en cada epoch.\n",
    "            augment_prob: Probabilidad de aplicar aumentaciones (0.25 = 25%).\n",
    "        \"\"\"\n",
    "        self.x_data = x_data  # Nubes de puntos\n",
    "        self.y_data = y_data  # Etiquetas correspondientes\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment_prob = augment_prob\n",
    "        self.indices = np.arange(len(self.x_data))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Número total de batches por epoch.\"\"\"\n",
    "        return len(self.indices) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Genera un batch de datos con normalización y aumentaciones.\"\"\"\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        x_batch = np.array([self.process_cloud(self.x_data[idx]) for idx in batch_indices])\n",
    "        y_batch = np.array([self.y_data[idx] for idx in batch_indices])  # Etiquetas sin modificar\n",
    "\n",
    "        return x_batch, y_batch  # Devuelve (inputs, etiquetas)\n",
    "\n",
    "    def process_cloud(self, cloud):\n",
    "        \"\"\"Normaliza y aplica aumentaciones con probabilidad augment_prob.\"\"\"\n",
    "        cloud = self.normalize_cloud(cloud)\n",
    "\n",
    "        # Aplicar aumentación con 25% de probabilidad\n",
    "        if np.random.rand() < self.augment_prob:\n",
    "            cloud = self.apply_augmentations(cloud)\n",
    "\n",
    "        return cloud\n",
    "\n",
    "    def normalize_cloud(self, cloud):\n",
    "        \"\"\"Normaliza una nube de puntos individualmente.\"\"\"\n",
    "        return (cloud - np.mean(cloud, axis=0)) / (np.std(cloud, axis=0) + 1e-6)\n",
    "\n",
    "    def apply_augmentations(self, cloud):\n",
    "        \"\"\"Aplica ruido gaussiano y rotaciones aleatorias.\"\"\"\n",
    "        cloud += np.random.normal(0.0, 0.01, cloud.shape)  # Añadir ruido gaussiano\n",
    "\n",
    "        # Rotación aleatoria en ejes X, Y, Z\n",
    "        cloud = np.dot(cloud, self.random_rotation_matrix())\n",
    "\n",
    "        return cloud\n",
    "\n",
    "    def random_rotation_matrix(self):\n",
    "        \"\"\"Genera una matriz de rotación aleatoria en 3D.\"\"\"\n",
    "        theta = np.random.uniform(-np.pi, np.pi)\n",
    "        phi = np.random.uniform(-np.pi, np.pi)\n",
    "        psi = np.random.uniform(-np.pi, np.pi)\n",
    "\n",
    "        rot_x = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, np.cos(theta), -np.sin(theta)],\n",
    "            [0, np.sin(theta), np.cos(theta)]\n",
    "        ])\n",
    "\n",
    "        rot_y = np.array([\n",
    "            [np.cos(phi), 0, np.sin(phi)],\n",
    "            [0, 1, 0],\n",
    "            [-np.sin(phi), 0, np.cos(phi)]\n",
    "        ])\n",
    "\n",
    "        rot_z = np.array([\n",
    "            [np.cos(psi), -np.sin(psi), 0],\n",
    "            [np.sin(psi), np.cos(psi), 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "\n",
    "        return rot_x @ rot_y @ rot_z  # Producto matricial para obtener la rotación final\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Baraja los datos al final de cada epoch si shuffle=True.\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataLoaders\n",
    "train_loader = PointCloudSequence(x_train, y_train, batch_size=32, shuffle=True)\n",
    "val_loader = PointCloudSequence(x_val, y_val, batch_size=32, shuffle=False, augment_prob=0.0)\n",
    "\n",
    "# Verificar un batch de datos\n",
    "x_sample, y_sample = train_loader[0]  # Primer batch\n",
    "print(\"Shape de x_batch:\", x_sample.shape)  # Debe ser (8, 4096, 3)\n",
    "print(\"Shape de y_batch:\", y_sample.shape)  # Debe ser (8, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T23:05:46.005650Z",
     "iopub.status.busy": "2025-02-19T23:05:46.005327Z",
     "iopub.status.idle": "2025-02-19T23:05:46.259051Z",
     "shell.execute_reply": "2025-02-19T23:05:46.257938Z",
     "shell.execute_reply.started": "2025-02-19T23:05:46.005625Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 3\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "# Definir el decay lineal con PolynomialDecay\n",
    "initial_lr = 0.4  # Learning rate inicial\n",
    "final_lr = 0.1  # Learning rate final\n",
    "decay_epochs = 4   # Número de épocas para decaer linealmente\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=decay_epochs,\n",
    "    end_learning_rate=final_lr,\n",
    "    power=1.0,  # Power=1.0 garantiza un decaimiento estrictamente lineal\n",
    "    cycle=False  # No reiniciar después del decaimiento\n",
    ")\n",
    "\n",
    "# Definir el modelo PointNet\n",
    "model = build_pointnet(num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n",
    "mean_iou_wrapper = MeanIoUWrapper(num_classes=NUM_CLASSES)\n",
    "\n",
    "\n",
    "# Definir el optimizador con el scheduler\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=[\"accuracy\", mean_iou_wrapper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "# Liberar memoria de TensorFlow\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Forzar liberación de memoria en Python\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.259Z",
     "iopub.execute_input": "2025-02-19T23:10:17.372591Z",
     "iopub.status.busy": "2025-02-19T23:10:17.372234Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "history = model.fit(\n",
    "    train_loader, \n",
    "    validation_data=val_loader,\n",
    "    epochs=4,\n",
    "    verbose=1,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"pointnet_goose_16k_4.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-20T02:10:23.260Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Supongamos que estos vienen de tu modelo entrenado (Keras history)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history.get('val_loss')\n",
    "mean_iou = history.history.get('mean_iou_wrapper')\n",
    "val_mean_iou = history.history.get('val_mean_iou_wrapper')\n",
    "accuracy = history.history.get('accuracy')\n",
    "val_accuracy = history.history.get('val_accuracy')\n",
    "\n",
    "# Crear figura con 1 fila y 3 columnas de subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"Loss Curve\", \"Mean IoU Curve\", \"Accuracy Curve\")\n",
    ")\n",
    "\n",
    "# 1) GRÁFICO DE PÉRDIDA (col=1)\n",
    "epochs_loss = list(range(1, len(loss) + 1))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=epochs_loss,\n",
    "        y=loss,\n",
    "        mode='lines+markers',\n",
    "        name='Train Loss'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "if val_loss:\n",
    "    epochs_val_loss = list(range(1, len(val_loss) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_val_loss,\n",
    "            y=val_loss,\n",
    "            mode='lines+markers',\n",
    "            name='Validation Loss'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Loss', row=1, col=1, range=[0, 10])  # Rango ajustado\n",
    "\n",
    "# 2) GRÁFICO DE Mean IoU (col=2) -> Limitar valores a [0,1]\n",
    "if mean_iou:\n",
    "    mean_iou = np.clip(mean_iou, 0, 1)\n",
    "    epochs_mean_iou = list(range(1, len(mean_iou) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_mean_iou,\n",
    "            y=mean_iou,\n",
    "            mode='lines+markers',\n",
    "            name='Train Mean IoU'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    if val_mean_iou:\n",
    "        val_mean_iou = np.clip(val_mean_iou, 0, 1)\n",
    "        epochs_val_mean_iou = list(range(1, len(val_mean_iou) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_mean_iou,\n",
    "                y=val_mean_iou,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Mean IoU'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Mean IoU', row=1, col=2, range=[0, 1])\n",
    "\n",
    "# 3) GRÁFICO DE ACCURACY (col=3) -> Limitar valores a [0,1]\n",
    "if accuracy:\n",
    "    accuracy = np.clip(accuracy, 0, 1)\n",
    "    epochs_acc = list(range(1, len(accuracy) + 1))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=epochs_acc,\n",
    "            y=accuracy,\n",
    "            mode='lines+markers',\n",
    "            name='Train Accuracy'\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "\n",
    "    if val_accuracy:\n",
    "        val_accuracy = np.clip(val_accuracy, 0, 1)\n",
    "        epochs_val_acc = list(range(1, len(val_accuracy) + 1))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epochs_val_acc,\n",
    "                y=val_accuracy,\n",
    "                mode='lines+markers',\n",
    "                name='Validation Accuracy'\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text='Epochs', row=1, col=3)\n",
    "fig.update_yaxes(title_text='Accuracy', row=1, col=3, range=[0, 1])\n",
    "\n",
    "# Ajustes generales de la figura\n",
    "fig.update_layout(\n",
    "    width=1300,\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6693936,
     "sourceId": 10786805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
