{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de Datos para conformar un conjunto de entrenamiento y validación\n",
    "\n",
    "En este _notebook_ se procesan los archivos `.bin` y `.label` para formar nubes de puntos de dimensiones _(N, 4)_, siendo _N_ el número de puntos y _4_ las características `x`, `y`, `z` y `remissions`. Las características de todos los puntos serán normalizadas según su naturaleza y respecto a las nubes de puntos que conforman el conjunto de entrenamiento. Finalmente las nubes de puntos normalizadas se guardarán en archivos `.csv` divididas en los directorios correspondientes al entrenamiento y validación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar rutas de los archivos _.bin_ y _.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "\n",
    "# point_clouds_path = Path(r\"goose\\val\\2022-07-22_flight\")\n",
    "# labels_path = Path(r\"goose\\labels\\val\\2022-07-22_flight\")\n",
    "\n",
    "# Linux\n",
    "\n",
    "# point_clouds_path = Path(r\"/home/felix/Escritorio/TFG/datasets/Goose/goose_3d_val/lidar/val/2022-07-22_flight\")\n",
    "# labels_path = Path(r\"/home/felix/Escritorio/TFG/datasets/Goose/goose_3d_val/labels/val/2022-07-22_flight\")\n",
    "\n",
    "# MacOS\n",
    "\n",
    "point_clouds_path = Path(r\"/Users/felixmaral/Desktop/TFG/datasets/goose_3d_val/lidar/val/2023-01-20_aying_mangfall_2\")\n",
    "labels_path = Path(r\"/Users/felixmaral/Desktop/TFG/datasets/goose_3d_val/labels/val/2023-01-20_aying_mangfall_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listar y ordenar archivos _.bin_ y _.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = sorted(os.listdir(point_clouds_path))\n",
    "labels_list = sorted(os.listdir(labels_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer archivos y asignación _X Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "\n",
    "# Reading .label files and adding to Y_DF\n",
    "for file in labels_list:\n",
    "\n",
    "    # reading a .label file\n",
    "    label = np.fromfile(os.path.join(labels_path, file), dtype=np.uint32)\n",
    "    label = label.reshape((-1))\n",
    "\n",
    "    # extract the semantic and instance label IDs\n",
    "    sem_label = label & 0xFFFF  # semantic label in lower half\n",
    "\n",
    "    Y.append(pd.DataFrame(sem_label, columns=[\"sem_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "# Reading .bin files and adding to X_DF\n",
    "for file in files_list:\n",
    "    scan = np.fromfile(os.path.join(point_clouds_path, file), dtype=np.float32)\n",
    "    scan = scan.reshape((-1, 4))\n",
    "\n",
    "    # put in attribute\n",
    "    points = scan[:, 0:3]    # get xyz\n",
    "    remissions = scan[:, 3]  # get remission\n",
    "\n",
    "    df_point_cloud = pd.DataFrame(points, columns=[\"x\",\"y\",\"z\"])\n",
    "    df_point_cloud[\"remissions\"] = remissions\n",
    "    X.append(df_point_cloud)\n",
    "\n",
    "# print(X[0:2])\n",
    "# print(list_labels[0:2])\n",
    "\n",
    "del df_point_cloud # optimizar memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudiar Clases Semánticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_concat = pd.concat(Y)\n",
    "    \n",
    "# df_list_labels\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.hist(Y_concat['sem_label'], \n",
    "         bins=np.arange(Y_concat['sem_label'].min(), Y_concat['sem_label'].max() + 2), \n",
    "         edgecolor='k', \n",
    "         alpha=0.7)\n",
    "\n",
    "# Personalización del gráfico\n",
    "plt.title(\"Histograma de Etiquetas Semánticas\")\n",
    "plt.xlabel(\"Etiqueta Semántica\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Configurar las marcas del eje X en incrementos de 5\n",
    "x_ticks = np.arange(Y_concat['sem_label'].min(), Y_concat['sem_label'].max() + 1, 5)\n",
    "plt.xticks(x_ticks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudiar de _nº puntos_ por barrido LiDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points_X = []\n",
    "\n",
    "# Reading .bin files and adding to DF\n",
    "for file in os.listdir(point_clouds_path):\n",
    "    scan = np.fromfile(os.path.join(point_clouds_path, file), dtype=np.float32)\n",
    "    scan = scan.reshape((-1, 4))\n",
    "\n",
    "    # put in attribute\n",
    "    points = scan[:, 0:3]    # get xyz\n",
    "    remissions = scan[:, 3]  # get remission\n",
    "\n",
    "    n_points_X.append(len(points))\n",
    "\n",
    "n_points_X = np.array(n_points_X)\n",
    "\n",
    "MIN_POINTS_X = n_points_X[n_points_X.argmin()]\n",
    "\n",
    "print(f\"La nube con menos puntos tiene: {MIN_POINTS_X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar Permutación Aleatoria para establecer un _shape_ uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_uniform = []\n",
    "Y_uniform = []\n",
    "    \n",
    "for x, y in zip(X, Y):\n",
    "    if len(x) >= MIN_POINTS_X:\n",
    "        # Seleccionar 'MIN_POINTS_X' puntos aleatorios\n",
    "        sampled_indices = np.random.choice(len(x), size=MIN_POINTS_X, replace=False)\n",
    "        X_uniform.append(x.iloc[sampled_indices].reset_index(drop=True))\n",
    "        Y_uniform.append(y.iloc[sampled_indices].reset_index(drop=True))\n",
    "    else:\n",
    "        raise ValueError(f\"La nube tiene menos puntos ({len(x)}) que 'MIN_POINTS_X' ({MIN_POINTS_X}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir en subconjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_uniform, Y_uniform, train_size=0.8, random_state=42)\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular distancia máxima en _X_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la distancia máxima global\n",
    "d_max = max(\n",
    "    np.sqrt((df[['x', 'y', 'z']] ** 2).sum(axis=1)).max() for df in X_train\n",
    ")\n",
    "\n",
    "# Calcular la media y desviación estándar global de 'remissions'\n",
    "all_remissions = pd.concat([df['remissions'] for df in X_train])\n",
    "mean = all_remissions.mean()\n",
    "std = all_remissions.std()\n",
    "\n",
    "print(f\"d_max: {d_max}, mean: {mean}, std: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar subconjuntos en función a _X_train_\n",
    "\n",
    "- Las remisiones se normalizan en función de la _media_ y _desviación típica_ calculadas en el conjunto de entrenamiento.\n",
    "- Las coordenadas espaciales se normalizan en funcion de la distancia máxima percibida en el sensor LiDAR en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = []\n",
    "\n",
    "for df in X_train:\n",
    "    norm_df = df.copy()\n",
    "    norm_df[['x','y','z']] = (df[['x','y','z']] / d_max)\n",
    "    norm_df['remissions'] = (norm_df['remissions'] - mean) / std\n",
    "    X_train_norm.append(norm_df)\n",
    "\n",
    "X_val_norm = []\n",
    "\n",
    "for df in X_val:\n",
    "    norm_df = df.copy()\n",
    "    norm_df[['x','y','z']] = (df[['x','y','z']] / d_max)\n",
    "    norm_df['remissions'] = (norm_df['remissions'] - mean) / std\n",
    "    X_val_norm.append(norm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar archivos en un directorio para cada subconjunto\n",
    "\n",
    "En el caso de querer guardar los archivos, seleccionar el intérprete de Python en la siguiente celda"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Linux\n",
    "\n",
    "# output_dir_x_train = \"/home/felix/Escritorio/TFG/datasets_norm/goose_norm_test/x_train\"\n",
    "# output_dir_y_train = \"/home/felix/Escritorio/TFG/datasets_norm/goose_norm_test/y_train\"\n",
    "# output_dir_x_val = \"/home/felix/Escritorio/TFG/datasets_norm/goose_norm_test/x_val\"\n",
    "# output_dir_y_val = \"/home/felix/Escritorio/TFG/datasets_norm/goose_norm_test/y_val\"\n",
    "\n",
    "# MacOS\n",
    "\n",
    "output_dir_x_train = \"/Users/felixmaral/Desktop/TFG/dataset_norm/goose_teesting/x_train\"\n",
    "output_dir_y_train = \"/Users/felixmaral/Desktop/TFG/dataset_norm/goose_teesting/y_train\"\n",
    "output_dir_x_val = \"/Users/felixmaral/Desktop/TFG/dataset_norm/goose_teesting/x_val\"\n",
    "output_dir_y_val = \"/Users/felixmaral/Desktop/TFG/dataset_norm/goose_teesting/y_val\"\n",
    "\n",
    "os.makedirs(output_dir_x_train, exist_ok=True)\n",
    "os.makedirs(output_dir_y_train, exist_ok=True)\n",
    "os.makedirs(output_dir_x_val, exist_ok=True)\n",
    "os.makedirs(output_dir_y_val, exist_ok=True)\n",
    "\n",
    "# Guardar cada DataFrame en un archivo\n",
    "for i, df in enumerate(X_train_norm):\n",
    "    # Define el nombre del archivo, por ejemplo: dataframe_0.csv\n",
    "    file_name = f\"dataframe_x_{i}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_x_train, file_name)\n",
    "    \n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "\n",
    "for i, df in enumerate(Y_train):\n",
    "    file_name = f\"dataframe_y_{i}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_y_train, file_name)\n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "\n",
    "# Guardar cada DataFrame en un archivo\n",
    "for i, df in enumerate(X_val_norm):\n",
    "    # Define el nombre del archivo, por ejemplo: dataframe_0.csv\n",
    "    file_name = f\"dataframe_x_{i}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_x_val, file_name)\n",
    "    \n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "\n",
    "for i, df in enumerate(Y_val):\n",
    "    file_name = f\"dataframe_y_{i}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_y_val, file_name)\n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
