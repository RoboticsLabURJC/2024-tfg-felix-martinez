{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de Datos para conformar un conjunto de entrenamiento y validación\n",
    "\n",
    "En este notebook se procesan los archivos `.bin` y `.label` para formar nubes de puntos de dimensiones _(N, 4)_, siendo _N_ el número de puntos y _4_ las características `x`, `y`, `z` y `remissions`. Las características de todos los puntos serán normalizadas según su naturaleza y respecto a las nubes de puntos que conforman el conjunto de entrenamiento. Finalmente las nubes de puntos normalizadas se guardarán en archivos `.csv` divididas en los directorios correspondientes al entrenamiento y validación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "count_x_train = 0\n",
    "count_y_train = 0\n",
    "count_x_val = 0\n",
    "count_y_val = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar rutas de los archivos _.bin_ y _.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecciona la ruta para 'point_clouds_path'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 18:45:32.975 python[29477:11279248] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-04 18:45:33.406 python[29477:11279248] The class 'NSOpenPanel' overrides the method identifier.  This method is implemented by class 'NSWindow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecciona la ruta para 'labels_path'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# Crear una ventana oculta de Tkinter\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Ocultar la ventana principal\n",
    "\n",
    "# Ruta inicial\n",
    "initial_path = Path(r\"/home/felix/Escritorio/TFG/datasets/Goose/goose_3d_train\")\n",
    "\n",
    "try:\n",
    "    # Seleccionar la primera ruta\n",
    "    print(\"Selecciona la ruta para 'point_clouds_path'\")\n",
    "    point_clouds_path = Path(filedialog.askdirectory(\n",
    "        title=\"Selecciona la carpeta para 'point_clouds_path'\",\n",
    "        initialdir=initial_path\n",
    "    ))\n",
    "\n",
    "    # Seleccionar la segunda ruta\n",
    "    print(\"Selecciona la ruta para 'labels_path'\")\n",
    "    labels_path = Path(filedialog.askdirectory(\n",
    "        title=\"Selecciona la carpeta para 'labels_path'\",\n",
    "        initialdir=initial_path\n",
    "    ))\n",
    "\n",
    "    # Mostrar las rutas seleccionadas\n",
    "    print(f\"Ruta seleccionada para point_clouds_path: {point_clouds_path}\")\n",
    "    print(f\"Ruta seleccionada para labels_path: {labels_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al seleccionar rutas: {e}\")\n",
    "\n",
    "finally:\n",
    "    root.destroy()  # Asegúrate de cerrar la ventana de Tkinter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listar y ordenar archivos _.bin_ y _.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = sorted(os.listdir(point_clouds_path))\n",
    "labels_list = sorted(os.listdir(labels_path))\n",
    "\n",
    "print(f\"En este conjunto de datos etiquetado hay {len(files_list)} nubes de puntos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer archivos y asignación _X Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "\n",
    "# Reading .label files and adding to Y_DF\n",
    "for file in labels_list:\n",
    "\n",
    "    # reading a .label file\n",
    "    label = np.fromfile(os.path.join(labels_path, file), dtype=np.uint32)\n",
    "    label = label.reshape((-1))\n",
    "\n",
    "    # extract the semantic and instance label IDs\n",
    "    sem_label = label & 0xFFFF  # semantic label in lower half\n",
    "\n",
    "    Y.append(pd.DataFrame(sem_label, columns=[\"sem_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "# Reading .bin files and adding to X_DF\n",
    "for file in files_list:\n",
    "    scan = np.fromfile(os.path.join(point_clouds_path, file), dtype=np.float32)\n",
    "    scan = scan.reshape((-1, 4))\n",
    "\n",
    "    # put in attribute\n",
    "    points = scan[:, 0:3]    # get xyz\n",
    "    remissions = scan[:, 3]  # get remission\n",
    "\n",
    "    df_point_cloud = pd.DataFrame(points, columns=[\"x\",\"y\",\"z\"])\n",
    "    df_point_cloud[\"remissions\"] = remissions\n",
    "    X.append(df_point_cloud)\n",
    "\n",
    "# print(X[0:2])\n",
    "# print(list_labels[0:2])\n",
    "\n",
    "del df_point_cloud # optimizar memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19.]\n"
     ]
    }
   ],
   "source": [
    "# Aplicar permutación aleatoria fija de 10,000 puntos\n",
    "x = np.linspace(0,19,20)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 10  5  6  3 18 13  2 14  8 17 16 19 12 11  1  0 15  4  9]\n"
     ]
    }
   ],
   "source": [
    "# Aplicar permutación aleatoria fija de 10,000 puntos\n",
    "\n",
    "np.random.seed(10)\n",
    "perm_indices = np.random.choice((len(x)), 20, replace=False, ) \n",
    "print(perm_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 10  5  6  3 18 13  2 14  8]\n"
     ]
    }
   ],
   "source": [
    "# Aplicar permutación aleatoria fija de 10,000 puntos\n",
    "\n",
    "np.random.seed(10)\n",
    "perm_indices = np.random.choice((len(x)), 10, replace=False) \n",
    "print(perm_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Redimensionamiento uniforme de las nubes de puntos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Aplicar permutación aleatoria fija de 10,000 puntos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mX\u001b[49m)):\n\u001b[1;32m      4\u001b[0m     perm_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(X[i]), \u001b[38;5;241m10000\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Aplicar permutación a los puntos y etiquetas\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X_redim = []\n",
    "Y_redim = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    # Permutación aleatoria de los indices de la nube de puntos\n",
    "    perm_indices = np.random.choice(len(X[i]), 10000, replace=False) \n",
    "\n",
    "    # Copia de los df originales para no modificar la información original\n",
    "    df_x = X[i].copy()\n",
    "    df_y = Y[i].copy()\n",
    "\n",
    "    # Aplicar permutación a los puntos y etiquetas\n",
    "    df_x = df_x.iloc[perm_indices].reset_index(drop=True)\n",
    "    df_y = df_y.iloc[perm_indices].reset_index(drop=True)\n",
    "\n",
    "    X_redim.append(df_x)\n",
    "    Y_redim.append(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudiar Clases Semánticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_redim[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_concat = pd.concat(Y_redim)\n",
    "print(len(Y_concat))\n",
    "\n",
    "\n",
    "# df_list_labels\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Crear el histograma\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.style.use('ggplot')\n",
    "plt.hist(Y_concat['sem_label'], \n",
    "         bins=np.arange(Y_concat['sem_label'].min(), Y_concat['sem_label'].max() + 2), \n",
    "         edgecolor='k', \n",
    "         alpha=1, \n",
    "         align='mid')\n",
    "\n",
    "# Personalización del gráfico\n",
    "plt.title(\"Histograma de Etiquetas Semánticas\", pad=20)\n",
    "plt.xlabel(\"Etiqueta Semántica\", labelpad=10)\n",
    "plt.ylabel(\"Frecuencia\", labelpad=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Configurar las marcas del eje X en incrementos de 1\n",
    "x_ticks = np.arange(Y_concat['sem_label'].min(), Y_concat['sem_label'].max() + 2, 1)\n",
    "plt.xticks(x_ticks, fontsize=8)\n",
    "\n",
    "# Ajustar padding de los x_ticks\n",
    "plt.gca().tick_params(axis='x', pad=10)  # Aumenta el espacio entre los ticks y el eje\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudiar de _nº puntos_ por barrido LiDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points_X = []\n",
    "\n",
    "# Reading .bin files and adding to DF\n",
    "for df in Y:\n",
    "    n_points_X.append(len(df))\n",
    "\n",
    "n_points_X = np.array(n_points_X)\n",
    "\n",
    "MIN_POINTS_X = n_points_X[n_points_X.argmin()]\n",
    "nube_min = n_points_X.argmin()\n",
    "\n",
    "MAX_POINTS_X = n_points_X[n_points_X.argmax()]\n",
    "nube_max = n_points_X.argmax()\n",
    "\n",
    "print(f\"La nube con menos puntos tiene: {MIN_POINTS_X}, es la {nube_min}\")\n",
    "print(f\"La nube con más puntos tiene: {MAX_POINTS_X}, es la {nube_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Normalización de X Y Z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar las coordenadas x, y, z de cada nube por su distancia euclidiana máxima\n",
    "for i, df in enumerate(X):\n",
    "    # Calcular la distancia máxima euclidiana para cada nube\n",
    "    d_max = np.sqrt((df[['x', 'y', 'z']] ** 2).sum(axis=1)).max()\n",
    "    # Normalizar x, y, z dividiendo por la distancia máxima\n",
    "    df[['x', 'y', 'z']] = df[['x', 'y', 'z']] / d_max\n",
    "    # Reasignar el dataframe normalizado a la lista X_train\n",
    "    X[i] = df\n",
    "\n",
    "# Calcular la media y desviación estándar global de 'remissions'\n",
    "all_remissions = pd.concat([df['remissions'] for df in X])\n",
    "mean = all_remissions.mean()\n",
    "std = all_remissions.std()\n",
    "\n",
    "print(f\"mean (remissions): {mean}, std (remissions): {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir en subconjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, train_size=0.8, random_state=42)\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar archivos en un directorio para cada subconjunto\n",
    "\n",
    "En el caso de querer guardar los archivos, seleccionar el intérprete de Python en la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "\n",
    "output_dir_x_train = \"/home/felix/Escritorio/TFG/datasets_splits/goose/x_train\"\n",
    "output_dir_y_train = \"/home/felix/Escritorio/TFG/datasets_splits/goose/y_train\"\n",
    "output_dir_x_val = \"/home/felix/Escritorio/TFG/datasets_splits/goose/x_val\"\n",
    "output_dir_y_val = \"/home/felix/Escritorio/TFG/datasets_splits/goose/y_val\"\n",
    "\n",
    "# MacOS\n",
    "\n",
    "#output_dir_x_train = \"/Users/felixmaral/Desktop/TFG/datasets_splits/goose/x_train\"\n",
    "#output_dir_y_train = \"/Users/felixmaral/Desktop/TFG/datasets_splits/goose/y_train\"\n",
    "#output_dir_x_val = \"/Users/felixmaral/Desktop/TFG/v/goose/x_val\"\n",
    "#output_dir_y_val = \"/Users/felixmaral/Desktop/TFG/datasets_splits/goose/y_val\"\n",
    "\n",
    "# Seg\n",
    "\n",
    "#output_dir_x_train = r\"C:\\Users\\fmartinez\\Desktop\\reco\\datasets_slipt\\goose_ex_val\\x_train\"\n",
    "#output_dir_y_train = r\"C:\\Users\\fmartinez\\Desktop\\reco\\datasets_slipt\\goose_ex_val\\y_train\"\n",
    "#output_dir_x_val = r\"C:\\Users\\fmartinez\\Desktop\\reco\\datasets_slipt\\goose_ex_val\\x_val\"\n",
    "#output_dir_y_val = r\"C:\\Users\\fmartinez\\Desktop\\reco\\datasets_slipt\\goose_ex_val\\y_val\"\n",
    "\n",
    "os.makedirs(output_dir_x_train, exist_ok=True)\n",
    "os.makedirs(output_dir_y_train, exist_ok=True)\n",
    "os.makedirs(output_dir_x_val, exist_ok=True)\n",
    "os.makedirs(output_dir_y_val, exist_ok=True)\n",
    "\n",
    "# Guardar cada DataFrame en un archivo\n",
    "for i, df in enumerate(X_train):\n",
    "    # Define el nombre del archivo, por ejemplo: dataframe_0.csv\n",
    "    file_name = f\"dataframe_x_{count_x_train}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_x_train, file_name)\n",
    "    \n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "    count_x_train += 1\n",
    "\n",
    "for i, df in enumerate(Y_train):\n",
    "    file_name = f\"dataframe_y_{count_y_train}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_y_train, file_name)\n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "    count_y_train += 1\n",
    "\n",
    "# Guardar cada DataFrame en un archivo\n",
    "for i, df in enumerate(X_val):\n",
    "    # Define el nombre del archivo, por ejemplo: dataframe_0.csv\n",
    "    file_name = f\"dataframe_x_{count_x_val}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_x_val, file_name)\n",
    "    \n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "    count_x_val += 1\n",
    "\n",
    "for i, df in enumerate(Y_val):\n",
    "    file_name = f\"dataframe_y_{count_y_val}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_y_val, file_name)\n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "    count_y_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
