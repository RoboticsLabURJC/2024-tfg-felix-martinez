{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de Datos para conformar un conjunto de entrenamiento y validación\n",
    "\n",
    "En este notebook se procesan los archivos `.bin` y `.label` para formar nubes de puntos de dimensiones _(N, 4)_, siendo _N_ el número de puntos y _4_ las características `x`, `y`, `z` y `remissions`. Las características de todos los puntos serán normalizadas según su naturaleza y respecto a las nubes de puntos que conforman el conjunto de entrenamiento. Finalmente las nubes de puntos normalizadas se guardarán en archivos `.csv` divididas en los directorios correspondientes al entrenamiento y validación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 17:52:12.685247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738255932.701425   12581 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738255932.706119   12581 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-30 17:52:12.722857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "count_x_train = 0\n",
    "count_y_train = 0\n",
    "count_x_val = 0\n",
    "count_y_val = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar rutas de los archivos _.bin_ y _.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecciona la ruta para 'point_clouds_path'\n",
      "Selecciona la ruta para 'labels_path'\n",
      "Ruta seleccionada para point_clouds_path: .\n",
      "Ruta seleccionada para labels_path: .\n"
     ]
    }
   ],
   "source": [
    "# Crear una ventana oculta de Tkinter\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Ocultar la ventana principal\n",
    "\n",
    "# Ruta inicial\n",
    "initial_path = Path(r\"/home/felix/Escritorio/TFG/datasets/Goose/goose_3d_train\")\n",
    "\n",
    "try:\n",
    "    # Seleccionar la primera ruta\n",
    "    print(\"Selecciona la ruta para 'point_clouds_path'\")\n",
    "    point_clouds_path = Path(filedialog.askdirectory(\n",
    "        title=\"Selecciona la carpeta para 'point_clouds_path'\",\n",
    "        initialdir=initial_path\n",
    "    ))\n",
    "\n",
    "    # Seleccionar la segunda ruta\n",
    "    print(\"Selecciona la ruta para 'labels_path'\")\n",
    "    labels_path = Path(filedialog.askdirectory(\n",
    "        title=\"Selecciona la carpeta para 'labels_path'\",\n",
    "        initialdir=initial_path\n",
    "    ))\n",
    "\n",
    "    # Mostrar las rutas seleccionadas\n",
    "    print(f\"Ruta seleccionada para point_clouds_path: {point_clouds_path}\")\n",
    "    print(f\"Ruta seleccionada para labels_path: {labels_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al seleccionar rutas: {e}\")\n",
    "\n",
    "finally:\n",
    "    root.destroy()  # Asegúrate de cerrar la ventana de Tkinter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listar y ordenar archivos _.bin_ y _.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = sorted(os.listdir(point_clouds_path))\n",
    "labels_list = sorted(os.listdir(labels_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer archivos y asignación _X Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "\n",
    "# Reading .label files and adding to Y_DF\n",
    "for file in labels_list:\n",
    "\n",
    "    # reading a .label file\n",
    "    label = np.fromfile(os.path.join(labels_path, file), dtype=np.uint32)\n",
    "    label = label.reshape((-1))\n",
    "\n",
    "    # extract the semantic and instance label IDs\n",
    "    sem_label = label & 0xFFFF  # semantic label in lower half\n",
    "\n",
    "    Y.append(pd.DataFrame(sem_label, columns=[\"sem_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "# Reading .bin files and adding to X_DF\n",
    "for file in files_list:\n",
    "    scan = np.fromfile(os.path.join(point_clouds_path, file), dtype=np.float32)\n",
    "    scan = scan.reshape((-1, 4))\n",
    "\n",
    "    # put in attribute\n",
    "    points = scan[:, 0:3]    # get xyz\n",
    "    remissions = scan[:, 3]  # get remission\n",
    "\n",
    "    df_point_cloud = pd.DataFrame(points, columns=[\"x\",\"y\",\"z\"])\n",
    "    df_point_cloud[\"remissions\"] = remissions\n",
    "    X.append(df_point_cloud)\n",
    "\n",
    "# print(X[0:2])\n",
    "# print(list_labels[0:2])\n",
    "\n",
    "del df_point_cloud # optimizar memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudiar Clases Semánticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_concat = pd.concat(Y)\n",
    "    \n",
    "# df_list_labels\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Crear el histograma\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.style.use('ggplot')\n",
    "plt.hist(Y_concat['sem_label'], \n",
    "         bins=np.arange(Y_concat['sem_label'].min(), Y_concat['sem_label'].max() + 2), \n",
    "         edgecolor='k', \n",
    "         alpha=1, \n",
    "         align='mid')\n",
    "\n",
    "# Personalización del gráfico\n",
    "plt.title(\"Histograma de Etiquetas Semánticas\", pad=20)\n",
    "plt.xlabel(\"Etiqueta Semántica\", labelpad=10)\n",
    "plt.ylabel(\"Frecuencia\", labelpad=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Configurar las marcas del eje X en incrementos de 1\n",
    "x_ticks = np.arange(Y_concat['sem_label'].min(), Y_concat['sem_label'].max() + 2, 1)\n",
    "plt.xticks(x_ticks, fontsize=8)\n",
    "\n",
    "# Ajustar padding de los x_ticks\n",
    "plt.gca().tick_params(axis='x', pad=10)  # Aumenta el espacio entre los ticks y el eje\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudiar de _nº puntos_ por barrido LiDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points_X = []\n",
    "\n",
    "# Reading .bin files and adding to DF\n",
    "for file in os.listdir(point_clouds_path):\n",
    "    scan = np.fromfile(os.path.join(point_clouds_path, file), dtype=np.float32)\n",
    "    scan = scan.reshape((-1, 4))\n",
    "\n",
    "    # put in attribute\n",
    "    points = scan[:, 0:3]    # get xyz\n",
    "    remissions = scan[:, 3]  # get remissions\n",
    "\n",
    "    n_points_X.append(len(points))\n",
    "\n",
    "n_points_X = np.array(n_points_X)\n",
    "\n",
    "MIN_POINTS_X = n_points_X[n_points_X.argmin()]\n",
    "nube = n_points_X.argmin()\n",
    "\n",
    "print(f\"La nube con menos puntos tiene: {MIN_POINTS_X}, es la {nube}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Normalización de X Y Z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar las coordenadas x, y, z de cada nube por su distancia euclidiana máxima\n",
    "for i, df in enumerate(X):\n",
    "    # Calcular la distancia máxima euclidiana para cada nube\n",
    "    d_max = np.sqrt((df[['x', 'y', 'z']] ** 2).sum(axis=1)).max()\n",
    "    # Normalizar x, y, z dividiendo por la distancia máxima\n",
    "    df[['x', 'y', 'z']] = df[['x', 'y', 'z']] / d_max\n",
    "    # Reasignar el dataframe normalizado a la lista X_train\n",
    "    X[i] = df\n",
    "\n",
    "# Calcular la media y desviación estándar global de 'remissions'\n",
    "all_remissions = pd.concat([df['remissions'] for df in X])\n",
    "mean = all_remissions.mean()\n",
    "std = all_remissions.std()\n",
    "\n",
    "print(f\"mean (remissions): {mean}, std (remissions): {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir en subconjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, train_size=0.8, random_state=42)\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar archivos en un directorio para cada subconjunto\n",
    "\n",
    "En el caso de querer guardar los archivos, seleccionar el intérprete de Python en la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "\n",
    "output_dir_x_train = \"/home/felix/Escritorio/TFG/datasets_splits/goose/x_train\"\n",
    "output_dir_y_train = \"/home/felix/Escritorio/TFG/datasets_splits/goose/y_train\"\n",
    "output_dir_x_val = \"/home/felix/Escritorio/TFG/datasets_splits/goose/x_val\"\n",
    "output_dir_y_val = \"/home/felix/Escritorio/TFG/datasets_splits/goose/y_val\"\n",
    "\n",
    "# MacOS\n",
    "\n",
    "#output_dir_x_train = \"/Users/felixmaral/Desktop/TFG/datasets_splits/goose/x_train\"\n",
    "#output_dir_y_train = \"/Users/felixmaral/Desktop/TFG/datasets_splits/goose/y_train\"\n",
    "#output_dir_x_val = \"/Users/felixmaral/Desktop/TFG/v/goose/x_val\"\n",
    "#output_dir_y_val = \"/Users/felixmaral/Desktop/TFG/datasets_splits/goose/y_val\"\n",
    "\n",
    "# Seg\n",
    "\n",
    "#output_dir_x_train = r\"C:\\Users\\fmartinez\\Desktop\\reco\\datasets_slipt\\goose_ex_val\\x_train\"\n",
    "#output_dir_y_train = r\"C:\\Users\\fmartinez\\Desktop\\reco\\datasets_slipt\\goose_ex_val\\y_train\"\n",
    "#output_dir_x_val = r\"C:\\Users\\fmartinez\\Desktop\\reco\\datasets_slipt\\goose_ex_val\\x_val\"\n",
    "#output_dir_y_val = r\"C:\\Users\\fmartinez\\Desktop\\reco\\datasets_slipt\\goose_ex_val\\y_val\"\n",
    "\n",
    "os.makedirs(output_dir_x_train, exist_ok=True)\n",
    "os.makedirs(output_dir_y_train, exist_ok=True)\n",
    "os.makedirs(output_dir_x_val, exist_ok=True)\n",
    "os.makedirs(output_dir_y_val, exist_ok=True)\n",
    "\n",
    "# Guardar cada DataFrame en un archivo\n",
    "for i, df in enumerate(X_train):\n",
    "    # Define el nombre del archivo, por ejemplo: dataframe_0.csv\n",
    "    file_name = f\"dataframe_x_{count_x_train}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_x_train, file_name)\n",
    "    \n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "    count_x_train += 1\n",
    "\n",
    "for i, df in enumerate(Y_train):\n",
    "    file_name = f\"dataframe_y_{count_y_train}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_y_train, file_name)\n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "    count_y_train += 1\n",
    "\n",
    "# Guardar cada DataFrame en un archivo\n",
    "for i, df in enumerate(X_val):\n",
    "    # Define el nombre del archivo, por ejemplo: dataframe_0.csv\n",
    "    file_name = f\"dataframe_x_{count_x_val}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_x_val, file_name)\n",
    "    \n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "    count_x_val += 1\n",
    "\n",
    "for i, df in enumerate(Y_val):\n",
    "    file_name = f\"dataframe_y_{count_y_val}.csv\"  # Cambia a .parquet si prefieres parquet\n",
    "    file_path = os.path.join(output_dir_y_val, file_name)\n",
    "    # Guardar el DataFrame como CSV\n",
    "    df.to_csv(file_path, index=False)  # Usa index=False para omitir el índice\n",
    "    print(f\"Guardado: {file_path}\")\n",
    "    count_y_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
